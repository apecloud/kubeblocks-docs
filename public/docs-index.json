[
  {
    "id": "docs_en_preview_kubeblocks-for-elasticsearch_01-overview",
    "title": "Overview of KubeBlocks Elasticsearch Addon",
    "content": "\n# Overview of KubeBlocks Elasticsearch Addon\n\n## Overview\n\nElasticsearch is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads. Each Elasticsearch cluster consists of one or more nodes, with each node assuming specific roles.\n\n### Node Roles\n\n| Role | Description |\n|------|-------------|\n| **master** | Manages cluster state and coordinates operations |\n| **data** | Stores data and handles data-related operations |\n| **data_content** | Stores document data |\n| **data_hot** | Handles recent, frequently accessed data |\n| **data_warm** | Stores less frequently accessed data |\n| **data_cold** | Handles rarely accessed data |\n| **data_frozen** | Manages archived data |\n| **ingest** | Processes documents before indexing |\n| **ml** | Runs machine learning jobs |\n| **remote_cluster_client** | Connects to remote clusters |\n| **transform** | Handles data transformations |\n\n[See Elasticsearch Node Roles documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html)\n\n## Key features\n\n### Lifecycle Management\n\nKubeBlocks simplifies Elasticsearch operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |\n| **Vertical Scaling**         | Adjust CPU/memory resources for Elasticsearch instances                       |\n| **Volume Expansion**         | Dynamically increase storage capacity without downtime                     |\n| **Restart Operations**       | Controlled cluster restarts with minimal disruption                        |\n| **Start/Stop**               | Temporarily suspend/resume cluster operations                              |\n| **Custom Services**          | Expos",
    "path": "docs/preview/kubeblocks-for-elasticsearch/01-overview",
    "description": " # Overview of KubeBlocks Elasticsearch Addon  ## Overview  Elasticsearch is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads. Each Elasticsearch cluster consists of one or more nodes, with each node assuming specific roles.  ### Node Roles  | Role"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-elasticsearch_02-quickstart",
    "title": "Elasticsearch Quickstart",
    "content": "\n\n# Elasticsearch Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing Elasticsearch ReplicaSet  Clusters using the **KubeBlocks Elasticsearch Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify Elasticsearch Add-on\n\nThe Elasticsearch Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep elasticsearch\n```\n\n\nExample Output:\n\n```bash\nNAME                       NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-elasticsearch     kb-system   1           2025-05-21                  deployed    elasticsearch-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/elasticsearch --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-elasticsearch kubeblocks-addons/elasticsearch --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n ",
    "path": "docs/preview/kubeblocks-for-elasticsearch/02-quickstart",
    "description": "  # Elasticsearch Quickstart  This guide provides a comprehensive walkabout for deploying and managing Elasticsearch ReplicaSet  Clusters using the **KubeBlocks Elasticsearch Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational managem"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-kafka_01-overview",
    "title": "Overview of KubeBlocks Kafka Addon",
    "content": "\n# Overview of KubeBlocks Kafka Addon\n\nApache Kafka is a distributed streaming platform designed to build real-time pipelines and can be used as a message broker or as a replacement for a log aggregation solution for big data applications.\n\n- A broker is a Kafka server that stores data and handles requests from producers and consumers. Kafka clusters consist of multiple brokers, each identified by a unique ID. Brokers work together to distribute and replicate data across the cluster.\n- KRaft was introduced in Kafka 3.3.1 in October 2022 as an alternative to Zookeeper. A subset of brokers are designated as controllers, and these controllers provide the consensus services that used to be provided by Zookeeper.\n\n## Key features\n\n### Lifecycle Management\n\nKubeBlocks simplifies Kafka operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |\n| **Vertical Scaling**         | Adjust CPU/memory resources for Kafka instances                       |\n| **Volume Expansion**         | Dynamically increase storage capacity without downtime                     |\n| **Restart Operations**       | Controlled cluster restarts with minimal disruption                        |\n| **Start/Stop**               | Temporarily suspend/resume cluster operations                              |\n| **Custom Services**          | Expose specialized database endpoints                                      |\n| **Replica Management**       | Safely decommission or rebuild specific replicas                          |\n| **Version Upgrades**         | Perform minor version upgrades seamlessly                                  |\n| **Advanced Scheduling**      | Customize pod placement and ",
    "path": "docs/preview/kubeblocks-for-kafka/01-overview",
    "description": " # Overview of KubeBlocks Kafka Addon  Apache Kafka is a distributed streaming platform designed to build real-time pipelines and can be used as a message broker or as a replacement for a log aggregation solution for big data applications.  - A broker is a Kafka server that stores data and handles r"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-kafka_02-quickstart",
    "title": "Kafka Quickstart",
    "content": "\n\n# Kafka Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing Kafka ReplicaSet  Clusters using the **KubeBlocks Kafka Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify Kafka Add-on\n\nThe Kafka Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep kafka\n```\n\n\nExample Output:\n\n```bash\nNAME               NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-kafka     kb-system   1           2025-05-21                  deployed    kafka-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/kafka --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-kafka kubeblocks-addons/kafka --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n  # Update the index\n  kbcli addon index update kubeblocks\n  # Update all indexes\n  kbcli addon i",
    "path": "docs/preview/kubeblocks-for-kafka/02-quickstart",
    "description": "  # Kafka Quickstart  This guide provides a comprehensive walkabout for deploying and managing Kafka ReplicaSet  Clusters using the **KubeBlocks Kafka Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational management including start/stop"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-milvus_01-overview",
    "title": "Overview of KubeBlocks Milvus Addon",
    "content": "\n# Overview of KubeBlocks Milvus Addon\n\nMilvus is an open source (Apache-2.0 licensed) vector database built to power embedding similarity search and AI applications. Milvus's architecture is designed to handle large-scale vector datasets and includes various deployment modes:  Milvus Standalone, and Milvus Distributed, to accommodate different data scale needs.\n\n## Key Features\n\n### Supported Topologies\n\nMilvus supports two deployment modes to accommodate different scale requirements:\n\n#### Standalone Mode\n\nA lightweight deployment suitable for development and testing:\n\n- **Milvus Core**: Provides vector search and database functionality\n- **Metadata Storage (ETCD)**: Stores cluster metadata and configuration\n- **Object Storage (MinIO/S3)**: Persists vector data and indexes\n\n#### Cluster Mode\n\nA distributed deployment for production workloads with multiple specialized components:\n\n**Access Layer**\n\n- Stateless proxies that handle client connections and request routing\n\n**Compute Layer**\n\n- Query Nodes: Execute search operations\n- Data Nodes: Handle data ingestion and compaction\n- Index Nodes: Build and maintain vector indexes\n\n**Coordination Layer**\n\n- Root Coordinator: Manages global metadata\n- Query Coordinator: Orchestrates query execution\n- Data Coordinator: Manages data distribution\n- Index Coordinator: Oversees index building\n\n**Storage Layer**\n\n- Metadata Storage (ETCD): Cluster metadata and configuration\n- Object Storage (MinIO/S3): Persistent vector data storage\n- Log Storage (Pulsar): Message queue for change data capture\n\n\n### Lifecycle Management\n\nKubeBlocks simplifies Milvus operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |",
    "path": "docs/preview/kubeblocks-for-milvus/01-overview",
    "description": " # Overview of KubeBlocks Milvus Addon  Milvus is an open source (Apache-2.0 licensed) vector database built to power embedding similarity search and AI applications. Milvus's architecture is designed to handle large-scale vector datasets and includes various deployment modes:  Milvus Standalone, an"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-milvus_02-quickstart",
    "title": "Milvus Quickstart",
    "content": "\n\n# Milvus Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing Milvus ReplicaSet  Clusters using the **KubeBlocks Milvus Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify Milvus Add-on\n\nThe Milvus Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep milvus\n```\n\n\nExample Output:\n\n```bash\nNAME                  NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-milvus       kb-system   1           2025-05-21                  deployed    milvus-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/milvus --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-milvus kubeblocks-addons/milvus --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n  # Update the index\n  kbcli addon index update kubeblocks\n  # Update all indexes",
    "path": "docs/preview/kubeblocks-for-milvus/02-quickstart",
    "description": "  # Milvus Quickstart  This guide provides a comprehensive walkabout for deploying and managing Milvus ReplicaSet  Clusters using the **KubeBlocks Milvus Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational management including start/s"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_01-overview",
    "title": "Overview of KubeBlocks MongoDB Addon",
    "content": "\n# Overview of KubeBlocks MongoDB Addon\n\nMongoDB is a NoSQL document database designed for flexibility, scalability, and high performance. Unlike traditional relational databases (SQL), MongoDB stores data in JSON-like documents (BSON format), making it ideal for unstructured or semi-structured data.\n\n### Supported Topologies\n\n**replicaset**\n\nA MongoDB replica set is a group of MongoDB servers that maintain the same dataset, providing high availability and data redundancy. Replica sets are the foundation of MongoDB's fault tolerance and data reliability. By replicating data across multiple nodes, MongoDB ensures that if one server fails, another can take over seamlessly without affecting the application's availability.\n\nIn a replica set, there are typically three types of nodes:\n\n- **Primary Node**: Handles all write operations and serves read requests by default.\n- **Secondary Nodes**: Maintain copies of the primary's data and can optionally serve read requests.\n- **Arbiter Node**: Participates in elections but does not store data. It is used to maintain an odd number of voting members in the replica set.\n\nAnd it is recommended to create a cluster with at least **three** nodes to ensure high availability, one primary and two secondary nodes.\n\n### Lifecycle Management\n\nKubeBlocks simplifies MongoDB operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |\n| **Vertical Scaling**         | Adjust CPU/memory resources for MongoDB instances                       |\n| **Volume Expansion**         | Dynamically increase storage capacity without downtime                     |\n| **Restart Operations**       | Controlled cluster restarts with minimal disr",
    "path": "docs/preview/kubeblocks-for-mongodb/01-overview",
    "description": " # Overview of KubeBlocks MongoDB Addon  MongoDB is a NoSQL document database designed for flexibility, scalability, and high performance. Unlike traditional relational databases (SQL), MongoDB stores data in JSON-like documents (BSON format), making it ideal for unstructured or semi-structured data"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_02-quickstart",
    "title": "MongoDB Quickstart",
    "content": "\n\n# MongoDB Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing MongoDB ReplicaSet  Clusters using the **KubeBlocks MongoDB Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify MongoDB Add-on\n\nThe MongoDB Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep mongodb\n```\n\n\nExample Output:\n\n```bash\nNAME               NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-mongodb     kb-system   1           2025-05-21                  deployed    mongodb-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/mongodb --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-mongodb kubeblocks-addons/mongodb --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n  # Update the index\n  kbcli addon index update kubeblocks\n  # Update all i",
    "path": "docs/preview/kubeblocks-for-mongodb/02-quickstart",
    "description": "  # MongoDB Quickstart  This guide provides a comprehensive walkabout for deploying and managing MongoDB ReplicaSet  Clusters using the **KubeBlocks MongoDB Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational management including star"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_01-overview",
    "title": "Overview of KubeBlocks MySQL Addon",
    "content": "\n# Overview of KubeBlocks MySQL Addon\n\nThe **KubeBlocks MySQL Addon** offers a comprehensive solution for deploying and managing MySQL clusters in Kubernetes. This document provides an overview of its features, including deployment topologies, lifecycle management options, backup and restore functionality, and supported MySQL versions.\n\n## Features\n\n### Topologies\nThe **KubeBlocks Operator** supports deploying MySQL in three different topologies, tailored to meet varying requirements for performance, consistency, and high availability:\n\n\n| Features                      | Description\n|-------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| SemiSync                      | Leverages MySQL’s semi-synchronous replication mechanism to achieve near-real-time data consistency.  • Requires at least one replica to acknowledge receipt of the transaction before the primary commits.  • Balances performance and consistency by reducing the chance of data loss in case of a primary failure.                                                    |\n| MySQL Group Replication (MGR) | Creates a distributed, multi-primary MySQL cluster using MySQL’s native Group Replication.  • Ensures fault-tolerant operations and automatic data synchronization across all nodes.  • Provides built-in conflict detection and resolution for continuous database availability.                  |\n| Orchestrator Integration      | Integrates an external Orchestrator for high-availability (HA) management.  • Adds automated monitoring and failover capabilities, including replica promotion.  • Allows dynamic handling of node failures or degradations, reducing downtime. |\n\nWith these options, you can tailor your MySQL deployment to your specific requirements for performance, consistency, and availability.\n\n### Lifecycle Management\n\nKubeBlocks provides robust lifecycle mana",
    "path": "docs/preview/kubeblocks-for-mysql/01-overview",
    "description": " # Overview of KubeBlocks MySQL Addon  The **KubeBlocks MySQL Addon** offers a comprehensive solution for deploying and managing MySQL clusters in Kubernetes. This document provides an overview of its features, including deployment topologies, lifecycle management options, backup and restore functio"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_02-quickstart",
    "title": "Quickstart",
    "content": "\n\n# Quickstart\n\nThis guide walks you through the process of getting started with the **KubeBlocks MySQL Add-on**, including prerequisites, enabling the add-on, creating a MySQL cluster, and managing the cluster with ease.\n\n\n## Prerequisites\n\nThis tutorial assumes that you have a Kubernetes cluster installed and running, and that you have installed the `kubectl` command line tool and `helm` somewhere in your path. Please see the [getting started](https://kubernetes.io/docs/setup/)  and [Installing Helm](https://helm.sh/docs/intro/install/) for installation instructions for your platform.\n\nAlso, this example requires KubeBlocks installed and running. Please see the [Install KubeBlocks](../user_docs/overview/install-kubeblocks) to install KubeBlocks.\n\n\n### Enable MySQL Add-on\n\nVerify whether MySQL Addon is installed. By default, the MySQL Addon is installed along with the KubeBlocks Helm chart.\n```bash\nhelm list -A\nNAME                        \tNAMESPACE  \tREVISION\tUPDATED                                \tSTATUS  \tCHART                       \tAPP VERSION\n...\nkb-addon-mysql              \tkb-system  \t1       \t2024-12-16 00:28:52.78819557 +0000 UTC \tdeployed\tmysql-1.0.0                 \t5.7.44\n```\n\nIf MySQL Addon is not enabled, you can enable it by following the steps below.\n\n```bash\n# Add Helm repo\nhelm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n# For users in Mainland China, if github is not accessible or very slow for you, please use following repo instead\n#helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n# Update helm repo\nhelm repo update\n# Search versions of the Addon\nhelm search repo kubeblocks/mysql --versions\n# Install the version you want (replace $version with the one you need)\nhelm upgrade -i mysql kubeblocks-addons/mysql --version $version -n kb-system\n```\n\n## Create A MySQL Cluster\n\n```bash\nkubectl apply -f https://raw.githubusercontent.com/apecloud/kubeblocks-addons/refs/heads/main/exampl",
    "path": "docs/preview/kubeblocks-for-mysql/02-quickstart",
    "description": "  # Quickstart  This guide walks you through the process of getting started with the **KubeBlocks MySQL Add-on**, including prerequisites, enabling the add-on, creating a MySQL cluster, and managing the cluster with ease.   ## Prerequisites  This tutorial assumes that you have a Kubernetes cluster i"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_01-overview",
    "title": "Overview of KubeBlocks PostgreSQL Addon",
    "content": "\n# Overview of KubeBlocks PostgreSQL Addon\n\nPostgreSQL is a powerful, open-source relational database management system known for its:\n- Scalability for handling large datasets\n- Robust security features\n- Extensive customization options\n- Support for diverse applications\n\nThe KubeBlocks PostgreSQL Addon enhances PostgreSQL with additional capabilities and extensions:\n\n## Supported Extensions\n\n| Extension      | Description                                                                 |\n|----------------|-----------------------------------------------------------------------------|\n| **pgvector**   | Enables vector operations for AI/ML applications and similarity searches    |\n\nThe **KubeBlocks PostgreSQL Addon** provides a complete solution for deploying and managing PostgreSQL clusters in Kubernetes environments. Key features include:\n\n- Flexible deployment topologies\n- Comprehensive lifecycle management\n- Reliable backup and restore operations\n- Support for multiple PostgreSQL versions\n\n## Key Features\n\n### Lifecycle Management\n\nKubeBlocks simplifies PostgreSQL operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |\n| **Vertical Scaling**         | Adjust CPU/memory resources for PostgreSQL instances                       |\n| **Volume Expansion**         | Dynamically increase storage capacity without downtime                     |\n| **Restart Operations**       | Controlled cluster restarts with minimal disruption                        |\n| **Start/Stop**               | Temporarily suspend/resume cluster operations                              |\n| **Password Management**      | Ability to set and manage custom root password for the Post",
    "path": "docs/preview/kubeblocks-for-postgresql/01-overview",
    "description": " # Overview of KubeBlocks PostgreSQL Addon  PostgreSQL is a powerful, open-source relational database management system known for its: - Scalability for handling large datasets - Robust security features - Extensive customization options - Support for diverse applications  The KubeBlocks PostgreSQL "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_02-quickstart",
    "title": "PostgreSQL Quickstart",
    "content": "\n\n# PostgreSQL Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing PostgreSQL clusters using the **KubeBlocks PostgreSQL Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify PostgreSQL Add-on\n\nThe PostgreSQL Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep postgresql\n```\n\n\nExample Output:\n\n```bash\nNAME                    NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-postgresql     kb-system   1           2025-05-21                  deployed    postgresql-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/postgresql --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-postgresql kubeblocks-addons/postgresql --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n  # Update the index\n  kbcli addon index update k",
    "path": "docs/preview/kubeblocks-for-postgresql/02-quickstart",
    "description": "  # PostgreSQL Quickstart  This guide provides a comprehensive walkabout for deploying and managing PostgreSQL clusters using the **KubeBlocks PostgreSQL Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational management including start/s"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant_01-overview",
    "title": "Overview of KubeBlocks Qdrant Addon",
    "content": "\n# Overview of KubeBlocks Qdrant Addon\n\nQdrant is an open-source vector search engine and vector database designed for efficient similarity search and storage of high-dimensional vectors. It is optimized for AI-driven applications, such as semantic search, recommendation systems, and retrieval-augmented generation (RAG) in large language models (LLMs).\n\n## Key Features\n\n### Lifecycle Management\n\nKubeBlocks simplifies Qdrant operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |\n| **Vertical Scaling**         | Adjust CPU/memory resources for Qdrant instances                       |\n| **Volume Expansion**         | Dynamically increase storage capacity without downtime                     |\n| **Restart Operations**       | Controlled cluster restarts with minimal disruption                        |\n| **Start/Stop**               | Temporarily suspend/resume cluster operations                              |\n| **Password Management**      | Ability to set and manage custom root password for the Qdrant cluster during creation |\n| **Custom Services**          | Expose specialized database endpoints                                      |\n| **Replica Management**       | Safely decommission or rebuild specific replicas                          |\n| **Version Upgrades**         | Perform minor version upgrades seamlessly                                  |\n| **Advanced Scheduling**      | Customize pod placement and resource allocation                           |\n| **Monitoring**               | Integrated Prometheus metrics collection                                  |\n| **Logging**                  | Centralized logs via Loki Stack                   ",
    "path": "docs/preview/kubeblocks-for-qdrant/01-overview",
    "description": " # Overview of KubeBlocks Qdrant Addon  Qdrant is an open-source vector search engine and vector database designed for efficient similarity search and storage of high-dimensional vectors. It is optimized for AI-driven applications, such as semantic search, recommendation systems, and retrieval-augme"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant_02-quickstart",
    "title": "Qdrant Quickstart",
    "content": "\n\n# Qdrant Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing Qdrant ReplicaSet  Clusters using the **KubeBlocks Qdrant Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify Qdrant Add-on\n\nThe Qdrant Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep qdrant\n```\n\n\nExample Output:\n\n```bash\nNAME                NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-qdrant     kb-system   1           2025-05-21                  deployed    qdrant-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/qdrant --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-qdrant kubeblocks-addons/qdrant --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n  # Update the index\n  kbcli addon index update kubeblocks\n  # Update all indexes\n  k",
    "path": "docs/preview/kubeblocks-for-qdrant/02-quickstart",
    "description": "  # Qdrant Quickstart  This guide provides a comprehensive walkabout for deploying and managing Qdrant ReplicaSet  Clusters using the **KubeBlocks Qdrant Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational management including start/s"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-rabbitmq_01-overview",
    "title": "Overview of KubeBlocks RabbitMQ Addon",
    "content": "\n# Overview of KubeBlocks RabbitMQ Addon\n\nRabbitMQ is an open-source and lightweight message broker which supports multiple messaging protocols.\n\n## Key features\n\n### Lifecycle Management\n\nKubeBlocks simplifies RabbitMQ operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |\n| **Vertical Scaling**         | Adjust CPU/memory resources for RabbitMQ instances                       |\n| **Volume Expansion**         | Dynamically increase storage capacity without downtime                     |\n| **Restart Operations**       | Controlled cluster restarts with minimal disruption                        |\n| **Start/Stop**               | Temporarily suspend/resume cluster operations                              |\n| **Password Management**      | Ability to set and manage custom root password for the RabbitMQ cluster during creation |\n| **Custom Services**          | Expose specialized database endpoints                                      |\n| **Replica Management**       | Safely decommission or rebuild specific replicas                          |\n| **Version Upgrades**         | Perform minor version upgrades seamlessly                                  |\n| **Advanced Scheduling**      | Customize pod placement and resource allocation                           |\n| **Monitoring**               | Integrated Prometheus metrics collection                                  |\n| **Logging**                  | Centralized logs via Loki Stack                                           |\n\n### Supported Versions\n\nKubeBlocks RabbitMQ Addon supports these RabbitMQ versions:\n\n| Major Version | Supported Minor Versions       |\n|---------------|------------------------",
    "path": "docs/preview/kubeblocks-for-rabbitmq/01-overview",
    "description": " # Overview of KubeBlocks RabbitMQ Addon  RabbitMQ is an open-source and lightweight message broker which supports multiple messaging protocols.  ## Key features  ### Lifecycle Management  KubeBlocks simplifies RabbitMQ operations with comprehensive lifecycle management:  | Feature                  "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-rabbitmq_02-quickstart",
    "title": "RabbitMQ Quickstart",
    "content": "\n\n# RabbitMQ Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing RabbitMQ ReplicaSet  Clusters using the **KubeBlocks RabbitMQ Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify RabbitMQ Add-on\n\nThe RabbitMQ Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep rabbitmq\n```\n\n\nExample Output:\n\n```bash\nNAME                  NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-rabbitmq     kb-system   1           2025-05-21                  deployed    rabbitmq-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/rabbitmq --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-rabbitmq kubeblocks-addons/rabbitmq --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n  # Update the index\n  kbcli addon index update kubeblocks\n  ",
    "path": "docs/preview/kubeblocks-for-rabbitmq/02-quickstart",
    "description": "  # RabbitMQ Quickstart  This guide provides a comprehensive walkabout for deploying and managing RabbitMQ ReplicaSet  Clusters using the **KubeBlocks RabbitMQ Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational management including s"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_01-overview",
    "title": "Overview of KubeBlocks Redis Addon",
    "content": "\n# Overview of KubeBlocks Redis Addon\n\nRedis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. This example shows how it can be managed in Kubernetes with KubeBlocks.\n\n\n## Key Features\n\n### Supported Topologies\n\n| Topology      | Data Distribution | Scalability | High Availability | Use Cases                     |\n|---------------|-------------------|-------------|--------------------|-------------------------------|\n| **Standalone**| Single node       | No          | No                 | Development/testing, small datasets |\n| **Replication** with sentinel     | Primary-Secondary replication | Read scaling | Yes | Read-heavy workloads, data redundancy needed |\n| **Cluster**   | Sharded storage   | Read/write scaling | Yes | Large datasets, high-concurrency production environments |\n\n### Lifecycle Management\n\nKubeBlocks simplifies Redis operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |\n| **Vertical Scaling**         | Adjust CPU/memory resources for Redis instances                       |\n| **Volume Expansion**         | Dynamically increase storage capacity without downtime                     |\n| **Restart Operations**       | Controlled cluster restarts with minimal disruption                        |\n| **Start/Stop**               | Temporarily suspend/resume cluster operations                              |\n| **Password Management**      | Ability to set and manage custom root password for the Redis cluster during creation |\n| **Dynamic Configuration**    | Modify Redis parameters without restarting                            |\n| **Custom Services**          | Expose spec",
    "path": "docs/preview/kubeblocks-for-redis/01-overview",
    "description": " # Overview of KubeBlocks Redis Addon  Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. This example shows how it can be managed in Kubernetes with KubeBlocks.   ## Key Features  ### Supported Topologies  | Topology      | Data Dis"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_02-quickstart",
    "title": "Redis Quickstart",
    "content": "\n\n# Redis Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing Redis Replication Clusters using the **KubeBlocks Redis Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify Redis Add-on\n\nThe Redis Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep redis\n```\n\n\nExample Output:\n\n```bash\nNAME               NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-redis     kb-system   1           2025-05-21                  deployed    redis-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/redis --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-redis kubeblocks-addons/redis --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n  # Update the index\n  kbcli addon index update kubeblocks\n  # Update all indexes\n  kbcli addon i",
    "path": "docs/preview/kubeblocks-for-redis/02-quickstart",
    "description": "  # Redis Quickstart  This guide provides a comprehensive walkabout for deploying and managing Redis Replication Clusters using the **KubeBlocks Redis Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational management including start/stop"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-starrocks_01-introduction",
    "title": "Starrocks",
    "content": "\n\n# StarRocks\n\nStarRocks is a Linux Foundation project, it is the next-generation data platform designed to make data-intensive real-time analytics fast and easy.\n\nStarRocks supports **shared-nothing** (Each BE has a portion of the data on its local storage) and **shared-data** (all data on object storage or HDFS and each CN has only cache on local storage).\n\n- FrontEnds (FE) are responsible for metadata management, client connection management, query planning, and query scheduling. Each FE stores and maintains a complete copy of the metadata in its memory, which guarantees indiscriminate services among the FEs.\n- BackEnds (BE) are responsible for data storage, data processing, and query execution. Each BE stores a portion of the data and processes the queries in parallel.\n\nKubeBlocks supports creating a **shared-nothing** StarRocks cluster.\n\n## Supported Features\n\n### Lifecycle Management\n\n|   Topology       | Horizontalscaling | Vertical scaling | Expandvolume | Restart   | Stop/Start | Configure | Expose | Switchover |\n|------------------|------------------------|-----------------------|-------------------|-----------|------------|-----------|--------|------------|\n| shared-nothing     | Yes                  | Yes                   | Yes               | Yes       | Yes        | No        | Yes    | N/A      |\n\n### Versions\n\n| Major Versions | Description |\n|----------------|-------------|\n| 3.3.x          | 3.3.0, 3.3.2|",
    "path": "docs/preview/kubeblocks-for-starrocks/01-introduction",
    "description": "  # StarRocks  StarRocks is a Linux Foundation project, it is the next-generation data platform designed to make data-intensive real-time analytics fast and easy.  StarRocks supports **shared-nothing** (Each BE has a portion of the data on its local storage) and **shared-data** (all data on object s"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-starrocks_02-provision",
    "title": "Provision",
    "content": "\n\n## Before you start\n\n- [Install kbcli](./../installation/install-kbcli.md) if you want to manage the StarRocks cluster with `kbcli`.\n- [Install KubeBlocks](./../installation/install-kubeblocks.md).\n- [Install and enable the starrocks Addon](./../installation/install-addons.md).\n- To keep things isolated, create a separate namespace called `demo` throughout this tutorial.\n\n  ```bash\n  kubectl create namespace demo\n  ```\n\n## Create a cluster\n\nKubeBlocks implements a `Cluster` CRD to define a cluster. Here is an example of creating a StarRocks cluster. If you only have one node for deploying a cluster with multiple replicas, configure the cluster affinity by setting `spec.schedulingPolicy` or `spec.componentSpecs.schedulingPolicy`. For details, you can refer to the [API docs](../user_docs/references/api-reference/cluster#apps.kubeblocks.io/v1.SchedulingPolicy). But for a production environment, it is not recommended to deploy all replicas on one node, which may decrease the cluster availability.\n\n```yaml\ncat <<EOF | kubectl apply -f -\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: mycluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  componentSpecs:\n  - name: fe\n    componentDef: starrocks-ce-fe\n    serviceAccountName: kb-starrocks-cluster\n    replicas: 1\n    resources:\n      limits:\n        cpu: '1'\n        memory: 1Gi\n      requests:\n        cpu: '1'\n        memory: 1Gi\n  - name: be\n    componentDef: starrocks-ce-be\n    replicas: 2\n    resources:\n      limits:\n        cpu: '1'\n        memory: 1Gi\n      requests:\n        cpu: '1'\n        memory: 1Gi\n    volumeClaimTemplates:\n    - name: data\n      spec:\n        accessModes:\n        - ReadWriteOnce\n        resources:\n          requests:\n            storage: 20Gi\nEOF\n```\n\n| Field                                 | Definition  |\n|---------------------------------------|--------------------------------------|\n| `spec.terminationPolicy`              | It is the policy of cluster termination. V",
    "path": "docs/preview/kubeblocks-for-starrocks/02-provision",
    "description": "  ## Before you start  - [Install kbcli](./../installation/install-kbcli.md) if you want to manage the StarRocks cluster with `kbcli`. - [Install KubeBlocks](./../installation/install-kubeblocks.md). - [Install and enable the starrocks Addon](./../installation/install-addons.md). - To keep things is"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-starrocks_03-scale",
    "title": "Scale",
    "content": "\n\n## Scale\n\n### Scale vertically\n\n#### Before you start\n\nCheck whether the cluster status is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n>\nNAME        CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    AGE\nmycluster   starrocks            starrocks-3.1.1   Delete               Running   4m29s\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS        CREATED-TIME\nmycluster   demo        starrocks            starrocks-3.1.1   Delete               Running       Jul 17,2024 19:06 UTC+0800\n```\n\n\n\n\n\n#### Steps\n\n\n\n1. Apply an OpsRequest to the specified cluster. Configure the parameters according to your needs.\n\n   ```bash\n   kubectl apply -f - \n   NAMESPACE   NAME                         TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        mycluster-vertical-scaling   VerticalScaling   mycluster   Succeed   3/3        6m\n   ```\n\n   If an error occurs, you can troubleshoot with `kubectl describe ops -n demo` command to view the events of this operation.\n\n3. Check whether the corresponding resources change.\n\n    ```bash\n    kubectl describe cluster mycluster -n demo\n    ```\n\n\n\n\n1. Change the configuration of `spec.componentSpecs.resources` in the YAML file. `spec.componentSpecs.resources` controls the requirement and limit of resources and changing them triggers a vertical scaling.\n\n   ```bash\n   kubectl edit cluster mycluster -n demo\n   ```\n\n   Edit the values of `spec.componentSpecs.resources`.\n\n   ```yaml\n   ...\n   spec:\n     clusterDefinitionRef: starrocks-ce\n     clusterVersionRef: starrocks-ce-3.1.1\n     componentSpecs:\n     - name: fe\n       componentDefRef: fe\n       replicas: 2\n       resources: # Change the values of resources\n         requests:\n           memory: \"2Gi\"\n           cpu: \"1\"\n         limits:\n           memory: \"4Gi\"\n           cpu: \"2\"\n   ...\n   ```\n\n2. Check ",
    "path": "docs/preview/kubeblocks-for-starrocks/03-scale",
    "description": "  ## Scale  ### Scale vertically  #### Before you start  Check whether the cluster status is `Running`. Otherwise, the following operations may fail.    ```bash kubectl get cluster mycluster -n demo > NAME        CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    AGE mycluster   s"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-starrocks_04-stop-and-start",
    "title": "Stop/Start",
    "content": "\n\n## Stop/Start a cluster\n\nYou can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. You can start this cluster again by snapshots if you want to restore the cluster resources.\n\n### Stop a cluster\n\n1. Configure the name of your cluster and run the command below to stop this cluster.\n\n\n\n    Apply an OpsRequest to restart a cluster.\n\n    ```bash\n    kubectl apply -f - \n\n\n    ```bash\n    kubectl edit cluster mycluster -n demo\n    ```\n\n    Configure replicas as 0 to delete pods.\n\n    ```yaml\n    ...\n    spec:\n      clusterDefinitionRef: starrocks-ce\n      clusterVersionRef: starrocks-ce-3.1.1\n      terminationPolicy: Delete\n      affinity:\n        podAntiAffinity: Preferred\n        topologyKeys:\n        - kubernetes.io/hostname\n      tolerations:\n        - key: kb-data\n          operator: Equal\n          value: 'true'\n          effect: NoSchedule\n      componentSpecs:\n      - name: fe\n        componentDefRef: fe\n        serviceAccountName: kb-starrocks-cluster\n        replicas: 0 # Change this value\n      - name: be\n        componentDefRef: be\n        replicas: 0 # Change this value\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster stop mycluster -n demo\n    ```\n\n    \n\n    \n\n2. Check the status of the cluster to see whether it is stopped.\n\n\n\n    ```bash\n    kubectl get cluster mycluster -n demo\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster list mycluster -n demo\n    ```\n\n    \n\n    \n\n### Start a cluster\n\n1. Configure the name of your cluster and run the command below to start this cluster.\n\n\n\n   Apply an OpsRequest to start a cluster.\n\n   ```bash\n   kubectl apply -f - \n\n\n   ```bash\n   kubectl edit cluster mycluster -n demo\n   ```\n\n   Change replicas back to the original amount to start this cluster again.\n\n   ```yaml\n   spec:\n     clusterDefinitionRef: starrocks-ce\n     clusterVersionRef: starrocks-ce-3.1.1\n",
    "path": "docs/preview/kubeblocks-for-starrocks/04-stop-and-start",
    "description": "  ## Stop/Start a cluster  You can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. You can start this cluster again by snapshots"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-starrocks_05-restart",
    "title": "Restart",
    "content": "\n\n## Restart\n\n\n\n1. Restart a cluster.\n\n   ```bash\n   kubectl apply -f - \n\n\n1. Configure the values of `components` and `ttlSecondsAfterSucceed` and run the command below to restart a specified cluster.\n\n   ```bash\n   kbcli cluster restart mycluster -n demo --components=\"starrocks\" --ttlSecondsAfterSucceed=30\n   ```\n\n   - `components` describes the component name that needs to be restarted.\n   - `ttlSecondsAfterSucceed` describes the time to live of an OpsRequest job after the restarting succeeds.\n\n2. Validate the restarting.\n\n   Run the command below to check the cluster status to check the restarting status.\n\n   ```bash\n   kbcli cluster list mycluster -n demo\n   >\n   NAME        NAMESPACE   CLUSTER-DEFINITION     VERSION             TERMINATION-POLICY   STATUS    CREATED-TIME\n   mycluster   demo        starrocks               starrocks-3.1.1    Delete               Running   Jul 17,2024 19:06 UTC+0800\n   ```\n\n   * STATUS=Updating: it means the cluster restart is in progress.\n   * STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n",
    "path": "docs/preview/kubeblocks-for-starrocks/05-restart",
    "description": "  ## Restart    1. Restart a cluster.     ```bash    kubectl apply -f -    1. Configure the values of `components` and `ttlSecondsAfterSucceed` and run the command below to restart a specified cluster.     ```bash    kbcli cluster restart mycluster -n demo --components=\"starrocks\" --ttlSecondsAfterS"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-starrocks_06-expand-volume",
    "title": "Expand Volume",
    "content": "\n\n## Volume expansion\n\n### Before you start\n\nCheck whether the cluster status is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n>\nNAME        CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    AGE\nmycluster   starrocks            starrocks-3.1.1   Delete               Running   4m29s\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS        CREATED-TIME\nmycluster   demo        starrocks            starrocks-3.1.1   Delete               Running       Jul 17,2024 19:06 UTC+0800\n```\n\n\n\n\n\n### Steps\n\n\n\n1. Change the value of storage according to your need and run the command below to expand the volume of a cluster.\n\n    ```yaml\n    kubectl apply -f - \n    NAMESPACE   NAME                         TYPE              CLUSTER     STATUS    PROGRESS   AGE\n    demo        mycluster-volume-expansion   VolumeExpansion   mycluster   Succeed   3/3        6m\n    ```\n\n    If an error occurs, you can troubleshoot with `kubectl describe ops -n demo` command to view the events of this operation.\n\n3. Check whether the corresponding cluster resources change.\n\n    ```bash\n    kubectl describe cluster mycluster -n demo\n    ```\n\n\n\n\n1. Change the value of `spec.componentSpecs.volumeClaimTemplates.spec.resources` in the cluster YAML file.\n\n   `spec.componentSpecs.volumeClaimTemplates.spec.resources` is the storage resource information of the pod and changing this value triggers the volume expansion of a cluster.\n\n   ```bash\n   kubectl edit cluster mycluster -n demo\n   ```\n\n   Edit the values of `spec.componentSpecs.volumeClaimTemplates.spec.resources`.\n\n   ```yaml\n   ...\n   spec:\n     clusterDefinitionRef: starrocks-ce\n     clusterVersionRef: starrocks-ce-3.1.1\n     componentSpecs:\n     - name: be\n       componentDefRef: be\n       volumeClaimTemplates:\n       - name: be-storage\n         spec:\n           accessModes:\n       ",
    "path": "docs/preview/kubeblocks-for-starrocks/06-expand-volume",
    "description": "  ## Volume expansion  ### Before you start  Check whether the cluster status is `Running`. Otherwise, the following operations may fail.    ```bash kubectl get cluster mycluster -n demo > NAME        CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    AGE mycluster   starrocks    "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-starrocks_10-delete",
    "title": "Delete",
    "content": "\n\n## Delete a cluster\n\n### Termination policy\n\n:::note\n\nThe termination policy determines how a cluster is deleted.\n\n:::\n\n| **terminationPolicy** | **Deleting Operation**                           |\n|:----------------------|:-------------------------------------------------|\n| `DoNotTerminate`      | `DoNotTerminate` prevents deletion of the Cluster. This policy ensures that all resources remain intact.       |\n| `Delete`              | `Delete` deletes Cluster resources like Pods, Services, and Persistent Volume Claims (PVCs), leading to a thorough cleanup while removing all persistent data.   |\n| `WipeOut`             | `WipeOut` is an aggressive policy that deletes all Cluster resources, including volume snapshots and backups in external storage. This results in complete data removal and should be used cautiously, primarily in non-production environments to avoid irreversible data loss.  |\n\nTo check the termination policy, execute the following command.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n>\nNAME        CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    AGE\nmycluster   starrocks            starrocks-3.1.1   Delete               Running   34m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION     VERSION         TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo                                               Delete               Running   Sep 30,2024 13:03 UTC+0800\n```\n\n\n\n\n\n### Steps\n\nRun the command below to delete a specified cluster.\n\n\n\nIf you want to delete a cluster and its all related resources, you can modify the termination policy to `WipeOut`, then delete the cluster.\n\n```bash\nkubectl patch -n demo cluster mycluster -p '}' --type=\"merge\"\n\nkubectl delete -n demo cluster mycluster\n```\n\n\n\n\n```bash\nkbcli cluster delete mycluster -n demo\n```\n\n\n\n\n",
    "path": "docs/preview/kubeblocks-for-starrocks/10-delete",
    "description": "  ## Delete a cluster  ### Termination policy  :::note  The termination policy determines how a cluster is deleted.  :::  | **terminationPolicy** | **Deleting Operation**                           | |:----------------------|:-------------------------------------------------| | `DoNotTerminate`      "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-apecloud-mysql_kubeblocks-for-apecloud-mysql",
    "title": "KubeBlocks for ApeCloud MySQL",
    "content": "\n# KubeBlocks for ApeCloud MySQL\n\nThis tutorial illustrates how to create and manage an ApeCloud MySQL cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/apecloud-mysql).\n\n* [Introduction](./apecloud-mysql-intro/apecloud-mysql-intro.md)\n* [Cluster Management](./cluster-management/create-and-connect-an-apecloud-mysql-cluster.md)\n* [Configuration](./configuration/configuration.md)\n* [High Availability](./high-availability/high-availability.md)\n* [Proxy](./proxy/introduction.md)",
    "path": "docs/release-0_9/kubeblocks-for-apecloud-mysql/kubeblocks-for-apecloud-mysql",
    "description": " # KubeBlocks for ApeCloud MySQL  This tutorial illustrates how to create and manage an ApeCloud MySQL cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/apecloud-mysql).  * ["
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-elasticsearch_01-overview",
    "title": "Manage Elasticsearch with KubeBlocks",
    "content": "\n\n# Manage Elasticsearch with KubeBlocks\n\nElasticsearch is a distributed, RESTful search and analytics engine that is capable of solving an ever-growing number of use cases. As the heart of the Elastic Stack, Elasticsearch stores your data centrally, allowing you to search it quickly, tune relevancy, perform sophisticated analytics, and easily scale.\n\nKubeBlocks supports the management of Elasticsearch. This tutorial illustrates how to create and manage an Elasticsearch cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples and guides in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/elasticsearch).\n\n## Before you start\n\n- [Install kbcli](./../user_docs/installation/install-kbcli) if you want to manage your Elasticsearch cluster with `kbcli`.\n- [Install KubeBlocks](./../user_docs/installation/install-kubeblocks).\n- [Install and enable the elasticsearch Addon](./../user_docs/installation/install-addons).\n- To keep things isolated, create a separate namespace called `demo` throughout this tutorial.\n\n  ```bash\n  kubectl create namespace demo\n  ```\n\n## Create a cluster\n\n\n\nKubeBlocks implements a `Cluster` CRD to define a cluster. Here is an example of creating an Elasticsearch cluster.\n\n```yaml\ncat \n\n\n***Steps***\n\n1. Execute the following command to create an Elasticsearch cluster.\n\n   ```bash\n   kbcli cluster create elasticsearch mycluster -n demo\n   ```\n\n:::note\n\nIf you want to customize your cluster specifications, kbcli provides various options, such as setting cluster version, termination policy, CPU, and memory. You can view these options by adding `--help` or `-h` flag.\n\n```bash\nkbcli cluster create elasticsearch --help\n\nkbcli cluster create elasticsearch -h\n```\n\n:::\n\n2. Check whether the cluster is created.\n\n   ```bash\n   kbcli cluster list\n   >\n   NAME        NAMESPACE   CLUSTER-DEFINITION   VERSION   TERMINATION-POLICY   STATUS     CREATED-TIME\n   mycluster   demo                           ",
    "path": "docs/release-0_9/kubeblocks-for-elasticsearch/01-overview",
    "description": "  # Manage Elasticsearch with KubeBlocks  Elasticsearch is a distributed, RESTful search and analytics engine that is capable of solving an ever-growing number of use cases. As the heart of the Elastic Stack, Elasticsearch stores your data centrally, allowing you to search it quickly, tune relevancy"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-kafka_01-overview",
    "title": "KubeBlocks for Kafka",
    "content": "\n# KubeBlocks for Kafka\n\nThis tutorial illustrates how to create and manage a Kafka cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/kafka).\n\n* [Cluster Management](./cluster-management/create-a-kafka-cluster.md)\n* [Configuration](./configuration/configuration.md)\n* [Resource Description](./configuration-recommendations-for-production-environments.md)\n",
    "path": "docs/release-0_9/kubeblocks-for-kafka/01-overview",
    "description": " # KubeBlocks for Kafka  This tutorial illustrates how to create and manage a Kafka cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/kafka).  * [Cluster Management](./cluste"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-kafka_configuration-recommendations-for-production-environments",
    "title": "Resource description",
    "content": "\n# Resource Description\n\n## Java Heap\n\nKafka Server's JVM Heap configuration, production environment can refer to the [official recommended configuration](https://kafka.apache.org/33/documentation.html#java):\n\n```bash\n-Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:G1HeapRegionSize=16M -XX:MinMetaspaceFreeRatio=50 -XX:MaxMetaspaceFreeRatio=80 -XX:+ExplicitGCInvokesConcurrent\n```\n\n- Combined mode\n    When creating a Kafka Cluster, specify the `--broker-heap` parameter.\n- Separated mode\n    When creating a Kafka Cluster, specify the component parameters with the `--broker-heap`; specify controller with the `--controller-heap` parameter.\n\n:::note\n\nWhen modifying the Java Heap configuration, attention should be paid to the resources allocated to the Cluster at the same time. For example, `--memory=1Gi`, but `-Xms` in `--broker-heap` is specified as `6g`, the broker service will not start normally due to insufficient memory.\n\n:::\n\n## Hardware resources\n\nIt is recommended to use the following hardware resource in a production environment:\n\n- `-cpu` >= 16 cores\n- `-memory` >= 64Gi\n\nSince Kafka uses a large amount of Page Cache to improve read and write speed, memory resources outside the heap memory allocated can improve overall performance.\n\n- `-storage` configure it according to the specific situations\n\nThe default compression algorithm for Kafka Cluster is `compression.type=producer`, which is specified by the Producer end. Refer to the average message size of the Topic, compression ratio, number of Topic replicas, and data retention time to perform a comprehensive evaluation.",
    "path": "docs/release-0_9/kubeblocks-for-kafka/configuration-recommendations-for-production-environments",
    "description": " # Resource Description  ## Java Heap  Kafka Server's JVM Heap configuration, production environment can refer to the [official recommended configuration](https://kafka.apache.org/33/documentation.html#java):  ```bash -Xmx6g -Xms6g -XX:MetaspaceSize=96m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:Initi"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-milvus_01-overview",
    "title": "Manage Milvus with KubeBlocks",
    "content": "\n\n# Manage Milvus with KubeBlocks\n\nThe popularity of generative AI (Generative AI) has aroused widespread attention and completely ignited the vector database (Vector Database) market.\n\nMilvus is a highly flexible, reliable, and blazing-fast cloud-native, open-source vector database. It powers embedding similarity search and AI applications and strives to make vector databases accessible to every organization. Milvus can store, index, and manage a billion+ embedding vectors generated by deep neural networks and other machine learning (ML) models.\n\nThis tutorial illustrates how to create and manage a Milvus cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples and guides in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/milvus).\n\n## Before you start\n\n- [Install kbcli](./../user_docs/installation/install-kbcli) if you want to manage the Milvus cluster with `kbcli`.\n- [Install KubeBlocks](./../user_docs/installation/install-kubeblocks).\n- [Install and enable the milvus Addon](./../user_docs/installation/install-addons).\n- To keep things isolated, create a separate namespace called `demo` throughout this tutorial.\n\n  ```bash\n  kubectl create namespace demo\n  ```\n\n## Create a cluster\n\n\n\nKubeBlocks implements a `Cluster` CRD to define a cluster. Here is an example of creating a Milvus cluster.\n\n```yaml\ncat \n\n\n***Steps***\n\n1. Execute the following command to create a Milvus cluster.\n\n   ```bash\n   kbcli cluster create mycluster --cluster-definition=milvus-2.3.2 -n demo\n   ```\n\n:::note\n\nIf you want to customize your cluster specifications, `kbcli` provides various options, such as setting cluster version, termination policy, CPU, and memory. You can view these options by adding `--help` or `-h` flag.\n\n```bash\nkbcli cluster create milvus --help\n\nkbcli cluster create milvus -h\n```\n\n:::\n\n2. Check whether the cluster is created successfully.\n\n   ```bash\n   kbcli cluster list -n demo\n   >\n   NAME        NAME",
    "path": "docs/release-0_9/kubeblocks-for-milvus/01-overview",
    "description": "  # Manage Milvus with KubeBlocks  The popularity of generative AI (Generative AI) has aroused widespread attention and completely ignited the vector database (Vector Database) market.  Milvus is a highly flexible, reliable, and blazing-fast cloud-native, open-source vector database. It powers embed"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mongodb_01-overview",
    "title": "KubeBlocks for MongoDB",
    "content": "\n# KubeBlocks for MongoDB\n\nThis tutorial illustrates how to create and manage a MongoDB cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/mongodb).\n\n* [Cluster Management](./cluster-management/create-and-connect-to-a-mongodb-cluster.md)\n* [Configuration](./configuration/configuration.md)\n",
    "path": "docs/release-0_9/kubeblocks-for-mongodb/01-overview",
    "description": " # KubeBlocks for MongoDB  This tutorial illustrates how to create and manage a MongoDB cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/mongodb).  * [Cluster Management](./"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mysql_01-overview",
    "title": "KubeBlocks for MySQL",
    "content": "\n# KubeBlocks for MySQL\n\nThis tutorial illustrates how to create and manage a MySQL cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/mysql).\n\n* [Cluster Management](./cluster-management/create-and-connect-a-mysql-cluster.md)\n* [Configuration](./configuration/configuration.md)\n* [High Availability](./high-availability/high-availability.md)\n",
    "path": "docs/release-0_9/kubeblocks-for-mysql/01-overview",
    "description": " # KubeBlocks for MySQL  This tutorial illustrates how to create and manage a MySQL cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/mysql).  * [Cluster Management](./cluste"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-postgresql_01-overview",
    "title": "KubeBlocks for PostgreSQL",
    "content": "\n# KubeBlocks for PostgreSQL\n\nThis tutorial illustrates how to create and manage a PostgreSQL cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/postgresql).\n\n* [Introduction](./introduction/introduction.md)\n* [Cluster Management](./cluster-management/create-and-connect-a-postgresql-cluster.md)\n* [Configuration](./configuration/configuration.md)\n* [High Availability](./high-availability/high-availability.md)\n* [PostgreSQL Connection Pool](./postgresql-connection-pool/postgresql-connection-pool.md)\n",
    "path": "docs/release-0_9/kubeblocks-for-postgresql/01-overview",
    "description": " # KubeBlocks for PostgreSQL  This tutorial illustrates how to create and manage a PostgreSQL cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/postgresql).  * [Introduction]"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-pulsar_01-overview",
    "title": "KubeBlocks for Pulsar",
    "content": "\n# KubeBlocks for Pulsar\n\nThis tutorial illustrates how to create and manage a Pulsar cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/pulsar).\n\n* [Cluster Management](./cluster-management/create-pulsar-cluster-on-kubeblocks.md)\n* [Configuration](./configuration/configuration.md)\n",
    "path": "docs/release-0_9/kubeblocks-for-pulsar/01-overview",
    "description": " # KubeBlocks for Pulsar  This tutorial illustrates how to create and manage a Pulsar cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/pulsar).  * [Cluster Management](./clu"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-qdrant_01-overview",
    "title": "Manage Qdrant with KubeBlocks",
    "content": "\n\n# Manage Qdrant with KubeBlocks\n\nThe popularity of generative AI (Generative AI) has aroused widespread attention and completely ignited the vector database (Vector Database) market. Qdrant (read: quadrant) is a vector similarity search engine and vector database. It provides a production-ready service with a convenient API to store, search, and manage points—vectors with an additional payload Qdrant is tailored to extended filtering support. It makes it useful for all sorts of neural-network or semantic-based matching, faceted search, and other applications.\n\nKubeBlocks supports the management of Qdrant. This tutorial illustrates how to create and manage a Qdrant cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/qdrant).\n\n## Before you start\n\n- [Install kbcli](./../user_docs/installation/install-kbcli) if you want to manage the Milvus cluster with `kbcli`.\n- [Install KubeBlocks](./../user_docs/installation/install-kubeblocks).\n- [Install and enable the Qdrant Addon](./../user_docs/installation/install-addons).\n- To keep things isolated, create a separate namespace called `demo` throughout this tutorial.\n\n   ```bash\n   kubectl create namespace demo\n   ```\n\n## Create a cluster\n\n***Steps***\n\n\n\nKubeBlocks implements a `Cluster` CRD to define a cluster. Here is an example of creating a Qdrant Replication cluster. Primary and Secondary are distributed on different nodes by default. But if you only have one node for deploying a Replication Cluster, set `spec.affinity.topologyKeys` as `null`.\n\n```yaml\ncat \n\n\n1. Execute the following command to create a Qdrant cluster.\n\n   ```bash\n   kbcli cluster create qdrant mycluster -n demo\n   ```\n\n   If you want to customize your cluster specifications, kbcli provides various options, such as setting cluster version, termination policy, CPU, and memory. You can view these options by adding `--help` or `-h` fl",
    "path": "docs/release-0_9/kubeblocks-for-qdrant/01-overview",
    "description": "  # Manage Qdrant with KubeBlocks  The popularity of generative AI (Generative AI) has aroused widespread attention and completely ignited the vector database (Vector Database) market. Qdrant (read: quadrant) is a vector similarity search engine and vector database. It provides a production-ready se"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-rabbitmq_01-overview",
    "title": "Manage RabbitMQ with KubeBlocks",
    "content": "\n\n# Manage RabbitMQ with KubeBlocks\n\nRabbitMQ is a reliable and mature messaging and streaming broker, which is easy to deploy on cloud environments, on-premises, and on your local machine.\n\nKubeBlocks supports the management of RabbitMQ.\n\n:::note\n\nCurrently, KubeBlocks only supports managing RabbitMQ by `kubectl`.\n\n:::\n\n## Before you start\n\n- [Install KubeBlocks](./../user_docs/installation/install-kubeblocks).\n- [Install and enable the rabbitmq addon](./../user_docs/installation/install-addons).\n\n## Create a cluster\n\nKubeBlocks implements a Cluster CRD to define a cluster. Here is an example of creating a RabbitMQ cluster with three replicas. Pods are distributed on different nodes by default. But if you only have one node for a cluster with three replicas, set `spec.affinity.topologyKeys` as `null`.\n\n```yaml\ncat \nNAME        CLUSTER-DEFINITION    VERSION        TERMINATION-POLICY     STATUS    AGE\nmycluster                                        Delete                 Running   47m\n```\n\n#### Steps\n\n\n\n1. Apply an OpsRequest to the specified cluster. Configure the parameters according to your needs.\n\n   ```bash\n   kubectl apply -f - \n   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   ops-vertical-scaling   VerticalScaling   mycluster   Succeed   3/3        6m\n   ```\n\n   If an error occurs, you can troubleshoot it with `kubectl describe ops -n demo` command to view the events of this operation.\n\n3. Check whether the corresponding resources change.\n\n    ```bash\n    kubectl describe cluster mycluster -n demo\n    ```\n\n\n\n\n1. Change the configuration of `spec.componentSpecs.resources` in the YAML file. `spec.componentSpecs.resources` controls the requirement and limit of resources and changing them triggers a vertical scaling.\n\n   ```yaml\n   apiVersion: apps.kubeblocks.io/v1alpha1\n   kind: Cluster\n   metadata:\n     name: mycluster\n     namespace: demo\n   spec:\n     componentSpecs:\n     - name: rabbitmq\n       componentDefRef: rabbitmq\n   ",
    "path": "docs/release-0_9/kubeblocks-for-rabbitmq/01-overview",
    "description": "  # Manage RabbitMQ with KubeBlocks  RabbitMQ is a reliable and mature messaging and streaming broker, which is easy to deploy on cloud environments, on-premises, and on your local machine.  KubeBlocks supports the management of RabbitMQ.  :::note  Currently, KubeBlocks only supports managing Rabbit"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-redis_01-overview",
    "title": "KubeBlocks for Redis",
    "content": "\n# KubeBlocks for Redis\n\nThis tutorial illustrates how to create and manage a Redis cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/redis).\n\n* [Cluster Management](./cluster-management/create-and-connect-a-redis-cluster.md)\n* [Configuration](./configuration/configuration.md)\n* [High Availability](./high-availability/high-availability.md)\n",
    "path": "docs/release-0_9/kubeblocks-for-redis/01-overview",
    "description": " # KubeBlocks for Redis  This tutorial illustrates how to create and manage a Redis cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/redis).  * [Cluster Management](./cluste"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-redis_redis-cluster-mode",
    "title": "Redis Cluster Mode",
    "content": "\n\n# Redis Cluster Mode\n\nWhile Redis Sentinel clusters provide excellent failover support, they do not inherently provide data sharding. All data remains on a single Redis instance, limited by its memory and performance capacity. Therefore, it may impact horizontal scalability when dealing with large datasets and high read/write operations.\n\nKubeBlocks now supports Redis Cluster Mode, which not only allows for greater memory distribution but also parallel processing, significantly improving performance for data-intensive operations. A brief overview of Redis Cluster Mode and its basic operations will be elaborated in this documentation.\n\n## What is Redis Cluster Mode\n\nRedis Cluster Mode is a distributed deployment mode of the Redis database used to horizontally scale data storage and improve system availability across multiple nodes.\n\nIn Redis Cluster Mode, the cluster manages data through a sharding mechanism and provides capabilities for data replication, failover, and traffic routing among shards. This sharding mechanism enables distributing a large amount of data across different nodes, achieving horizontal scalability and load balancing.\n\nRedis Cluster Mode ensures high availability through master-slave replication. Each master node can have one or more slave nodes that replicate the data from the master and provide read services. In case of a master node failure, a slave node can automatically take over the master's role and continue serving, ensuring failover and fault tolerance.\n\nRedis Cluster Mode also provides communication and data migration mechanisms among cluster nodes. When there are changes in the cluster, such as adding nodes, node failures, or node removal, Redis Cluster automatically performs data migration and resharding to maintain data balance and availability.\n\n## Basic Ops\n\nBelow is a brief introduction to the basic operations of Redis Cluster Mode.\n\n### Before you start\n\n* [Install KubeBlocks](./../user_docs/installation/install-kubeblocks)\n ",
    "path": "docs/release-0_9/kubeblocks-for-redis/redis-cluster-mode",
    "description": "  # Redis Cluster Mode  While Redis Sentinel clusters provide excellent failover support, they do not inherently provide data sharding. All data remains on a single Redis instance, limited by its memory and performance capacity. Therefore, it may impact horizontal scalability when dealing with large"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-starrocks_01-overview",
    "title": "Manage StarRocks with KubeBlocks",
    "content": "\n\n# Manage StarRocks with KubeBlocks\n\nStarRocks is a next-gen, high-performance analytical data warehouse that enables real-time, multi-dimensional, and highly concurrent data analysis.\n\nThis tutorial illustrates how to create and manage a StarRocks cluster by `kbcli`, `kubectl` or a YAML file. You can find the YAML examples and guides in [the GitHub repository](https://github.com/apecloud/kubeblocks-addons/tree/release-0.9/examples/starrocks).\n\n## Before you start\n\n- [Install kbcli](./../installation/install-kbcli.md) if you want to manage the StarRocks cluster with `kbcli`.\n- [Install KubeBlocks](./../installation/install-kubeblocks.md).\n- [Install and enable the starrocks Addon](./../installation/install-addons.md).\n- To keep things isolated, create a separate namespace called `demo` throughout this tutorial.\n\n  ```bash\n  kubectl create namespace demo\n  ```\n\n## Create a cluster\n\n\n\nKubeBlocks implements a `Cluster` CRD to define a cluster. Here is an example of creating a StarRocks cluster.\n\n```yaml\ncat \n\n\n***Steps***\n\n1. Execute the following command to create a StarRocks cluster.\n\n   ```bash\n   kbcli cluster create mycluster --cluster-definition=starrocks -n demo\n   ```\n\n   You can also create a cluster with specified CPU, memory and storage values.\n\n   ```bash\n   kbcli cluster create mycluster --cluster-definition=starrocks --set cpu=1,memory=2Gi,storage=10Gi -n demo\n   ```\n\n:::note\n\nIf you want to customize your cluster specifications, `kbcli` provides various options, such as setting cluster version, termination policy, CPU, and memory. You can view these options by adding `--help` or `-h` flag.\n\n```bash\nkbcli cluster create --help\nkbcli cluster create -h\n```\n\n:::\n\n2. Check whether the cluster is created successfully.\n\n   ```bash\n   kbcli cluster list -n demo\n   >\n   NAME        NAMESPACE   CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS     CREATED-TIME\n   mycluster   demo        starrocks            starrocks-3.1.1   Delete               ",
    "path": "docs/release-0_9/kubeblocks-for-starrocks/01-overview",
    "description": "  # Manage StarRocks with KubeBlocks  StarRocks is a next-gen, high-performance analytical data warehouse that enables real-time, multi-dimensional, and highly concurrent data analysis.  This tutorial illustrates how to create and manage a StarRocks cluster by `kbcli`, `kubectl` or a YAML file. You "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-elasticsearch_01-overview",
    "title": "Overview of KubeBlocks Elasticsearch Addon",
    "content": "\n# Overview of KubeBlocks Elasticsearch Addon\n\n## Overview\n\nElasticsearch is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads. Each Elasticsearch cluster consists of one or more nodes, with each node assuming specific roles.\n\n### Node Roles\n\n| Role | Description |\n|------|-------------|\n| **master** | Manages cluster state and coordinates operations |\n| **data** | Stores data and handles data-related operations |\n| **data_content** | Stores document data |\n| **data_hot** | Handles recent, frequently accessed data |\n| **data_warm** | Stores less frequently accessed data |\n| **data_cold** | Handles rarely accessed data |\n| **data_frozen** | Manages archived data |\n| **ingest** | Processes documents before indexing |\n| **ml** | Runs machine learning jobs |\n| **remote_cluster_client** | Connects to remote clusters |\n| **transform** | Handles data transformations |\n\n[See Elasticsearch Node Roles documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html)\n\n## Key features\n\n### Lifecycle Management\n\nKubeBlocks simplifies Elasticsearch operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |\n| **Vertical Scaling**         | Adjust CPU/memory resources for Elasticsearch instances                       |\n| **Volume Expansion**         | Dynamically increase storage capacity without downtime                     |\n| **Restart Operations**       | Controlled cluster restarts with minimal disruption                        |\n| **Start/Stop**               | Temporarily suspend/resume cluster operations                              |\n| **Custom Services**          | Expos",
    "path": "docs/release-1_0/kubeblocks-for-elasticsearch/01-overview",
    "description": " # Overview of KubeBlocks Elasticsearch Addon  ## Overview  Elasticsearch is a distributed, RESTful search engine optimized for speed and relevance on production-scale workloads. Each Elasticsearch cluster consists of one or more nodes, with each node assuming specific roles.  ### Node Roles  | Role"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-elasticsearch_02-quickstart",
    "title": "Elasticsearch Quickstart",
    "content": "\n\n# Elasticsearch Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing Elasticsearch ReplicaSet  Clusters using the **KubeBlocks Elasticsearch Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify Elasticsearch Add-on\n\nThe Elasticsearch Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep elasticsearch\n```\n\n\nExample Output:\n\n```bash\nNAME                       NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-elasticsearch     kb-system   1           2025-05-21                  deployed    elasticsearch-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/elasticsearch --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-elasticsearch kubeblocks-addons/elasticsearch --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n ",
    "path": "docs/release-1_0/kubeblocks-for-elasticsearch/02-quickstart",
    "description": "  # Elasticsearch Quickstart  This guide provides a comprehensive walkabout for deploying and managing Elasticsearch ReplicaSet  Clusters using the **KubeBlocks Elasticsearch Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational managem"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-kafka_01-overview",
    "title": "Overview of KubeBlocks Kafka Addon",
    "content": "\n# Overview of KubeBlocks Kafka Addon\n\nApache Kafka is a distributed streaming platform designed to build real-time pipelines and can be used as a message broker or as a replacement for a log aggregation solution for big data applications.\n\n- A broker is a Kafka server that stores data and handles requests from producers and consumers. Kafka clusters consist of multiple brokers, each identified by a unique ID. Brokers work together to distribute and replicate data across the cluster.\n- KRaft was introduced in Kafka 3.3.1 in October 2022 as an alternative to Zookeeper. A subset of brokers are designated as controllers, and these controllers provide the consensus services that used to be provided by Zookeeper.\n\n## Key features\n\n### Lifecycle Management\n\nKubeBlocks simplifies Kafka operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |\n| **Vertical Scaling**         | Adjust CPU/memory resources for Kafka instances                       |\n| **Volume Expansion**         | Dynamically increase storage capacity without downtime                     |\n| **Restart Operations**       | Controlled cluster restarts with minimal disruption                        |\n| **Start/Stop**               | Temporarily suspend/resume cluster operations                              |\n| **Custom Services**          | Expose specialized database endpoints                                      |\n| **Replica Management**       | Safely decommission or rebuild specific replicas                          |\n| **Version Upgrades**         | Perform minor version upgrades seamlessly                                  |\n| **Advanced Scheduling**      | Customize pod placement and ",
    "path": "docs/release-1_0/kubeblocks-for-kafka/01-overview",
    "description": " # Overview of KubeBlocks Kafka Addon  Apache Kafka is a distributed streaming platform designed to build real-time pipelines and can be used as a message broker or as a replacement for a log aggregation solution for big data applications.  - A broker is a Kafka server that stores data and handles r"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-kafka_02-quickstart",
    "title": "Kafka Quickstart",
    "content": "\n\n# Kafka Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing Kafka ReplicaSet  Clusters using the **KubeBlocks Kafka Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify Kafka Add-on\n\nThe Kafka Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep kafka\n```\n\n\nExample Output:\n\n```bash\nNAME               NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-kafka     kb-system   1           2025-05-21                  deployed    kafka-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/kafka --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-kafka kubeblocks-addons/kafka --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n  # Update the index\n  kbcli addon index update kubeblocks\n  # Update all indexes\n  kbcli addon i",
    "path": "docs/release-1_0/kubeblocks-for-kafka/02-quickstart",
    "description": "  # Kafka Quickstart  This guide provides a comprehensive walkabout for deploying and managing Kafka ReplicaSet  Clusters using the **KubeBlocks Kafka Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational management including start/stop"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-milvus_01-overview",
    "title": "Overview of KubeBlocks Milvus Addon",
    "content": "\n# Overview of KubeBlocks Milvus Addon\n\nMilvus is an open source (Apache-2.0 licensed) vector database built to power embedding similarity search and AI applications. Milvus's architecture is designed to handle large-scale vector datasets and includes various deployment modes:  Milvus Standalone, and Milvus Distributed, to accommodate different data scale needs.\n\n## Key Features\n\n### Supported Topologies\n\nMilvus supports two deployment modes to accommodate different scale requirements:\n\n#### Standalone Mode\n\nA lightweight deployment suitable for development and testing:\n\n- **Milvus Core**: Provides vector search and database functionality\n- **Metadata Storage (ETCD)**: Stores cluster metadata and configuration\n- **Object Storage (MinIO/S3)**: Persists vector data and indexes\n\n#### Cluster Mode\n\nA distributed deployment for production workloads with multiple specialized components:\n\n**Access Layer**\n\n- Stateless proxies that handle client connections and request routing\n\n**Compute Layer**\n\n- Query Nodes: Execute search operations\n- Data Nodes: Handle data ingestion and compaction\n- Index Nodes: Build and maintain vector indexes\n\n**Coordination Layer**\n\n- Root Coordinator: Manages global metadata\n- Query Coordinator: Orchestrates query execution\n- Data Coordinator: Manages data distribution\n- Index Coordinator: Oversees index building\n\n**Storage Layer**\n\n- Metadata Storage (ETCD): Cluster metadata and configuration\n- Object Storage (MinIO/S3): Persistent vector data storage\n- Log Storage (Pulsar): Message queue for change data capture\n\n\n### Lifecycle Management\n\nKubeBlocks simplifies Milvus operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |",
    "path": "docs/release-1_0/kubeblocks-for-milvus/01-overview",
    "description": " # Overview of KubeBlocks Milvus Addon  Milvus is an open source (Apache-2.0 licensed) vector database built to power embedding similarity search and AI applications. Milvus's architecture is designed to handle large-scale vector datasets and includes various deployment modes:  Milvus Standalone, an"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-milvus_02-quickstart",
    "title": "Milvus Quickstart",
    "content": "\n\n# Milvus Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing Milvus ReplicaSet  Clusters using the **KubeBlocks Milvus Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify Milvus Add-on\n\nThe Milvus Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep milvus\n```\n\n\nExample Output:\n\n```bash\nNAME                  NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-milvus       kb-system   1           2025-05-21                  deployed    milvus-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/milvus --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-milvus kubeblocks-addons/milvus --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n  # Update the index\n  kbcli addon index update kubeblocks\n  # Update all indexes",
    "path": "docs/release-1_0/kubeblocks-for-milvus/02-quickstart",
    "description": "  # Milvus Quickstart  This guide provides a comprehensive walkabout for deploying and managing Milvus ReplicaSet  Clusters using the **KubeBlocks Milvus Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational management including start/s"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_01-overview",
    "title": "Overview of KubeBlocks MongoDB Addon",
    "content": "\n# Overview of KubeBlocks MongoDB Addon\n\nMongoDB is a NoSQL document database designed for flexibility, scalability, and high performance. Unlike traditional relational databases (SQL), MongoDB stores data in JSON-like documents (BSON format), making it ideal for unstructured or semi-structured data.\n\n### Supported Topologies\n\n**replicaset**\n\nA MongoDB replica set is a group of MongoDB servers that maintain the same dataset, providing high availability and data redundancy. Replica sets are the foundation of MongoDB's fault tolerance and data reliability. By replicating data across multiple nodes, MongoDB ensures that if one server fails, another can take over seamlessly without affecting the application's availability.\n\nIn a replica set, there are typically three types of nodes:\n\n- Primary Node: Handles all write operations and serves read requests by default.\n- Secondary Nodes: Maintain copies of the primary's data and can optionally serve read requests.\n- Arbiter Node: Participates in elections but does not store data. It is used to maintain an odd number of voting members in the replica set.\n\nAnd it is recommended to create a cluster with at least **three** nodes to ensure high availability, one primary and two secondary nodes.\n\n### Lifecycle Management\n\nKubeBlocks simplifies MongoDB operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |\n| **Vertical Scaling**         | Adjust CPU/memory resources for MongoDB instances                       |\n| **Volume Expansion**         | Dynamically increase storage capacity without downtime                     |\n| **Restart Operations**       | Controlled cluster restarts with minimal disruption      ",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/01-overview",
    "description": " # Overview of KubeBlocks MongoDB Addon  MongoDB is a NoSQL document database designed for flexibility, scalability, and high performance. Unlike traditional relational databases (SQL), MongoDB stores data in JSON-like documents (BSON format), making it ideal for unstructured or semi-structured data"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_02-quickstart",
    "title": "MongoDB Quickstart",
    "content": "\n\n# MongoDB Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing MongoDB ReplicaSet  Clusters using the **KubeBlocks MongoDB Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify MongoDB Add-on\n\nThe MongoDB Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep mongodb\n```\n\n\nExample Output:\n\n```bash\nNAME               NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-mongodb     kb-system   1           2025-05-21                  deployed    mongodb-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/mongodb --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-mongodb kubeblocks-addons/mongodb --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n  # Update the index\n  kbcli addon index update kubeblocks\n  # Update all i",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/02-quickstart",
    "description": "  # MongoDB Quickstart  This guide provides a comprehensive walkabout for deploying and managing MongoDB ReplicaSet  Clusters using the **KubeBlocks MongoDB Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational management including star"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_01-overview",
    "title": "Overview of KubeBlocks MySQL Addon",
    "content": "\n# Overview of KubeBlocks MySQL Addon\n\nThe **KubeBlocks MySQL Addon** offers a comprehensive solution for deploying and managing MySQL clusters in Kubernetes. This document provides an overview of its features, including deployment topologies, lifecycle management options, backup and restore functionality, and supported MySQL versions.\n\n## Features\n\n### Topologies\nThe **KubeBlocks Operator** supports deploying MySQL in three different topologies, tailored to meet varying requirements for performance, consistency, and high availability:\n\n\n| Features                      | Description\n|-------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| SemiSync                      | Leverages MySQL’s semi-synchronous replication mechanism to achieve near-real-time data consistency.  • Requires at least one replica to acknowledge receipt of the transaction before the primary commits.  • Balances performance and consistency by reducing the chance of data loss in case of a primary failure.                                                    |\n| MySQL Group Replication (MGR) | Creates a distributed, multi-primary MySQL cluster using MySQL’s native Group Replication.  • Ensures fault-tolerant operations and automatic data synchronization across all nodes.  • Provides built-in conflict detection and resolution for continuous database availability.                  |\n| Orchestrator Integration      | Integrates an external Orchestrator for high-availability (HA) management.  • Adds automated monitoring and failover capabilities, including replica promotion.  • Allows dynamic handling of node failures or degradations, reducing downtime. |\n\nWith these options, you can tailor your MySQL deployment to your specific requirements for performance, consistency, and availability.\n\n### Lifecycle Management\n\nKubeBlocks provides robust lifecycle mana",
    "path": "docs/release-1_0/kubeblocks-for-mysql/01-overview",
    "description": " # Overview of KubeBlocks MySQL Addon  The **KubeBlocks MySQL Addon** offers a comprehensive solution for deploying and managing MySQL clusters in Kubernetes. This document provides an overview of its features, including deployment topologies, lifecycle management options, backup and restore functio"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_02-quickstart",
    "title": "Quickstart",
    "content": "\n\n# Quickstart\n\nThis guide walks you through the process of getting started with the **KubeBlocks MySQL Add-on**, including prerequisites, enabling the add-on, creating a MySQL cluster, and managing the cluster with ease.\n\n\n## Prerequisites\n\nThis tutorial assumes that you have a Kubernetes cluster installed and running, and that you have installed the `kubectl` command line tool and `helm` somewhere in your path. Please see the [getting started](https://kubernetes.io/docs/setup/)  and [Installing Helm](https://helm.sh/docs/intro/install/) for installation instructions for your platform.\n\nAlso, this example requires KubeBlocks installed and running. Please see the [Install KubeBlocks](../user_docs/overview/install-kubeblocks) to install KubeBlocks.\n\n\n### Enable MySQL Add-on\n\nVerify whether MySQL Addon is installed. By default, the MySQL Addon is installed along with the KubeBlocks Helm chart.\n```bash\nhelm list -A\nNAME                        \tNAMESPACE  \tREVISION\tUPDATED                                \tSTATUS  \tCHART                       \tAPP VERSION\n...\nkb-addon-mysql              \tkb-system  \t1       \t2024-12-16 00:28:52.78819557 +0000 UTC \tdeployed\tmysql-1.0.0                 \t5.7.44\n```\n\nIf MySQL Addon is not enabled, you can enable it by following the steps below.\n\n```bash\n# Add Helm repo\nhelm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n# For users in Mainland China, if github is not accessible or very slow for you, please use following repo instead\n#helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n# Update helm repo\nhelm repo update\n# Search versions of the Addon\nhelm search repo kubeblocks/mysql --versions\n# Install the version you want (replace $version with the one you need)\nhelm upgrade -i mysql kubeblocks-addons/mysql --version $version -n kb-system\n```\n\n## Create A MySQL Cluster\n\n```bash\nkubectl apply -f https://raw.githubusercontent.com/apecloud/kubeblocks-addons/refs/heads/main/exampl",
    "path": "docs/release-1_0/kubeblocks-for-mysql/02-quickstart",
    "description": "  # Quickstart  This guide walks you through the process of getting started with the **KubeBlocks MySQL Add-on**, including prerequisites, enabling the add-on, creating a MySQL cluster, and managing the cluster with ease.   ## Prerequisites  This tutorial assumes that you have a Kubernetes cluster i"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_01-overview",
    "title": "Overview of KubeBlocks PostgreSQL Addon",
    "content": "\n# Overview of KubeBlocks PostgreSQL Addon\n\nPostgreSQL is a powerful, open-source relational database management system known for its:\n- Scalability for handling large datasets\n- Robust security features\n- Extensive customization options\n- Support for diverse applications\n\nThe KubeBlocks PostgreSQL Addon enhances PostgreSQL with additional capabilities and extensions:\n\n## Supported Extensions\n\n| Extension      | Description                                                                 |\n|----------------|-----------------------------------------------------------------------------|\n| **pgvector**   | Enables vector operations for AI/ML applications and similarity searches    |\n\nThe **KubeBlocks PostgreSQL Addon** provides a complete solution for deploying and managing PostgreSQL clusters in Kubernetes environments. Key features include:\n\n- Flexible deployment topologies\n- Comprehensive lifecycle management\n- Reliable backup and restore operations\n- Support for multiple PostgreSQL versions\n\n## Key Features\n\n### Lifecycle Management\n\nKubeBlocks simplifies PostgreSQL operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |\n| **Vertical Scaling**         | Adjust CPU/memory resources for PostgreSQL instances                       |\n| **Volume Expansion**         | Dynamically increase storage capacity without downtime                     |\n| **Restart Operations**       | Controlled cluster restarts with minimal disruption                        |\n| **Start/Stop**               | Temporarily suspend/resume cluster operations                              |\n| **Password Management**      | Ability to set and manage custom root password for the Post",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/01-overview",
    "description": " # Overview of KubeBlocks PostgreSQL Addon  PostgreSQL is a powerful, open-source relational database management system known for its: - Scalability for handling large datasets - Robust security features - Extensive customization options - Support for diverse applications  The KubeBlocks PostgreSQL "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_02-quickstart",
    "title": "PostgreSQL Quickstart",
    "content": "\n\n# PostgreSQL Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing PostgreSQL clusters using the **KubeBlocks PostgreSQL Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify PostgreSQL Add-on\n\nThe PostgreSQL Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep postgresql\n```\n\n\nExample Output:\n\n```bash\nNAME                    NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-postgresql     kb-system   1           2025-05-21                  deployed    postgresql-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/postgresql --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-postgresql kubeblocks-addons/postgresql --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n  # Update the index\n  kbcli addon index update k",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/02-quickstart",
    "description": "  # PostgreSQL Quickstart  This guide provides a comprehensive walkabout for deploying and managing PostgreSQL clusters using the **KubeBlocks PostgreSQL Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational management including start/s"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant_01-overview",
    "title": "Overview of KubeBlocks Qdrant Addon",
    "content": "\n# Overview of KubeBlocks Qdrant Addon\n\nQdrant is an open-source vector search engine and vector database designed for efficient similarity search and storage of high-dimensional vectors. It is optimized for AI-driven applications, such as semantic search, recommendation systems, and retrieval-augmented generation (RAG) in large language models (LLMs).\n\n## Key Features\n\n### Lifecycle Management\n\nKubeBlocks simplifies Qdrant operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |\n| **Vertical Scaling**         | Adjust CPU/memory resources for Qdrant instances                       |\n| **Volume Expansion**         | Dynamically increase storage capacity without downtime                     |\n| **Restart Operations**       | Controlled cluster restarts with minimal disruption                        |\n| **Start/Stop**               | Temporarily suspend/resume cluster operations                              |\n| **Password Management**      | Ability to set and manage custom root password for the Qdrant cluster during creation |\n| **Custom Services**          | Expose specialized database endpoints                                      |\n| **Replica Management**       | Safely decommission or rebuild specific replicas                          |\n| **Version Upgrades**         | Perform minor version upgrades seamlessly                                  |\n| **Advanced Scheduling**      | Customize pod placement and resource allocation                           |\n| **Monitoring**               | Integrated Prometheus metrics collection                                  |\n| **Logging**                  | Centralized logs via Loki Stack                   ",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/01-overview",
    "description": " # Overview of KubeBlocks Qdrant Addon  Qdrant is an open-source vector search engine and vector database designed for efficient similarity search and storage of high-dimensional vectors. It is optimized for AI-driven applications, such as semantic search, recommendation systems, and retrieval-augme"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant_02-quickstart",
    "title": "Qdrant Quickstart",
    "content": "\n\n# Qdrant Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing Qdrant ReplicaSet  Clusters using the **KubeBlocks Qdrant Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify Qdrant Add-on\n\nThe Qdrant Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep qdrant\n```\n\n\nExample Output:\n\n```bash\nNAME                NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-qdrant     kb-system   1           2025-05-21                  deployed    qdrant-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/qdrant --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-qdrant kubeblocks-addons/qdrant --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n  # Update the index\n  kbcli addon index update kubeblocks\n  # Update all indexes\n  k",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/02-quickstart",
    "description": "  # Qdrant Quickstart  This guide provides a comprehensive walkabout for deploying and managing Qdrant ReplicaSet  Clusters using the **KubeBlocks Qdrant Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational management including start/s"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-rabbitmq_01-overview",
    "title": "Overview of KubeBlocks RabbitMQ Addon",
    "content": "\n# Overview of KubeBlocks RabbitMQ Addon\n\nRabbitMQ is an open-source and lightweight message broker which supports multiple messaging protocols.\n\n## Key features\n\n### Lifecycle Management\n\nKubeBlocks simplifies RabbitMQ operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |\n| **Vertical Scaling**         | Adjust CPU/memory resources for RabbitMQ instances                       |\n| **Volume Expansion**         | Dynamically increase storage capacity without downtime                     |\n| **Restart Operations**       | Controlled cluster restarts with minimal disruption                        |\n| **Start/Stop**               | Temporarily suspend/resume cluster operations                              |\n| **Password Management**      | Ability to set and manage custom root password for the RabbitMQ cluster during creation |\n| **Custom Services**          | Expose specialized database endpoints                                      |\n| **Replica Management**       | Safely decommission or rebuild specific replicas                          |\n| **Version Upgrades**         | Perform minor version upgrades seamlessly                                  |\n| **Advanced Scheduling**      | Customize pod placement and resource allocation                           |\n| **Monitoring**               | Integrated Prometheus metrics collection                                  |\n| **Logging**                  | Centralized logs via Loki Stack                                           |\n\n### Supported Versions\n\nKubeBlocks RabbitMQ Addon supports these RabbitMQ versions:\n\n| Major Version | Supported Minor Versions       |\n|---------------|------------------------",
    "path": "docs/release-1_0/kubeblocks-for-rabbitmq/01-overview",
    "description": " # Overview of KubeBlocks RabbitMQ Addon  RabbitMQ is an open-source and lightweight message broker which supports multiple messaging protocols.  ## Key features  ### Lifecycle Management  KubeBlocks simplifies RabbitMQ operations with comprehensive lifecycle management:  | Feature                  "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-rabbitmq_02-quickstart",
    "title": "RabbitMQ Quickstart",
    "content": "\n\n# RabbitMQ Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing RabbitMQ ReplicaSet  Clusters using the **KubeBlocks RabbitMQ Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify RabbitMQ Add-on\n\nThe RabbitMQ Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep rabbitmq\n```\n\n\nExample Output:\n\n```bash\nNAME                  NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-rabbitmq     kb-system   1           2025-05-21                  deployed    rabbitmq-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/rabbitmq --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-rabbitmq kubeblocks-addons/rabbitmq --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n  # Update the index\n  kbcli addon index update kubeblocks\n  ",
    "path": "docs/release-1_0/kubeblocks-for-rabbitmq/02-quickstart",
    "description": "  # RabbitMQ Quickstart  This guide provides a comprehensive walkabout for deploying and managing RabbitMQ ReplicaSet  Clusters using the **KubeBlocks RabbitMQ Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational management including s"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_01-overview",
    "title": "Overview of KubeBlocks Redis Addon",
    "content": "\n# Overview of KubeBlocks Redis Addon\n\nRedis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. This example shows how it can be managed in Kubernetes with KubeBlocks.\n\n\n## Key Features\n\n### Supported Topologies\n\n| Topology      | Data Distribution | Scalability | High Availability | Use Cases                     |\n|---------------|-------------------|-------------|--------------------|-------------------------------|\n| **Standalone**| Single node       | No          | No                 | Development/testing, small datasets |\n| **Replication** with sentinel     | Primary-Secondary replication | Read scaling | Yes | Read-heavy workloads, data redundancy needed |\n| **Cluster**   | Sharded storage   | Read/write scaling | Yes | Large datasets, high-concurrency production environments |\n\n### Lifecycle Management\n\nKubeBlocks simplifies Redis operations with comprehensive lifecycle management:\n\n| Feature                      | Description                                                                 |\n|------------------------------|-----------------------------------------------------------------------------|\n| **Horizontal Scaling**       | Scale replicas in/out to adjust capacity                                   |\n| **Vertical Scaling**         | Adjust CPU/memory resources for Redis instances                       |\n| **Volume Expansion**         | Dynamically increase storage capacity without downtime                     |\n| **Restart Operations**       | Controlled cluster restarts with minimal disruption                        |\n| **Start/Stop**               | Temporarily suspend/resume cluster operations                              |\n| **Password Management**      | Ability to set and manage custom root password for the Redis cluster during creation |\n| **Dynamic Configuration**    | Modify Redis parameters without restarting                            |\n| **Custom Services**          | Expose spec",
    "path": "docs/release-1_0/kubeblocks-for-redis/01-overview",
    "description": " # Overview of KubeBlocks Redis Addon  Redis is an open source (BSD licensed), in-memory data structure store, used as a database, cache and message broker. This example shows how it can be managed in Kubernetes with KubeBlocks.   ## Key Features  ### Supported Topologies  | Topology      | Data Dis"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_02-quickstart",
    "title": "Redis Quickstart",
    "content": "\n\n# Redis Quickstart\n\nThis guide provides a comprehensive walkabout for deploying and managing Redis Replication Clusters using the **KubeBlocks Redis Add-on**, covering:\n- System prerequisites and add-on installation\n- Cluster creation and configuration\n- Operational management including start/stop procedures\n- Connection methods and cluster monitoring\n\n## Prerequisites\n\n### System Requirements\n\nBefore proceeding, verify your environment meets these requirements:\n\n- A functional Kubernetes cluster (v1.21+ recommended)\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks))\n\n### Verify Redis Add-on\n\nThe Redis Add-on is included with KubeBlocks by default. Check its status:\n\n```bash\nhelm list -n kb-system | grep redis\n```\n\n\nExample Output:\n\n```bash\nNAME               NAMESPACE   REVISION    UPDATED                     STATUS      CHART\nkb-addon-redis     kb-system   1           2025-05-21                  deployed    redis-1.0.0\n```\n\n\nIf the add-on isn't enabled, choose an installation method:\n\n\n  ```bash\n  # Add Helm repo\n  helm repo add kubeblocks-addons https://apecloud.github.io/helm-charts\n  # For users in Mainland China, if GitHub is inaccessible or slow, use this alternative repo:\n  #helm repo add kubeblocks-addons https://jihulab.com/api/v4/projects/150246/packages/helm/stable\n\n  # Update helm repo\n  helm repo update\n  # Search available Add-on versions\n  helm search repo kubeblocks/redis --versions\n  # Install your desired version (replace  with your chosen version)\n  helm upgrade -i kb-addon-redis kubeblocks-addons/redis --version  -n kb-system\n  ```\n  \n\n\n  ```bash\n  # Add an index (kubeblocks is added by default)\n  kbcli addon index add kubeblocks https://github.com/apecloud/block-index.git\n  # Update the index\n  kbcli addon index update kubeblocks\n  # Update all indexes\n  kbcli addon i",
    "path": "docs/release-1_0/kubeblocks-for-redis/02-quickstart",
    "description": "  # Redis Quickstart  This guide provides a comprehensive walkabout for deploying and managing Redis Replication Clusters using the **KubeBlocks Redis Add-on**, covering: - System prerequisites and add-on installation - Cluster creation and configuration - Operational management including start/stop"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-starrocks_01-introduction",
    "title": "Starrocks",
    "content": "\n\n# StarRocks\n\nStarRocks is a Linux Foundation project, it is the next-generation data platform designed to make data-intensive real-time analytics fast and easy.\n\nStarRocks supports **shared-nothing** (Each BE has a portion of the data on its local storage) and **shared-data** (all data on object storage or HDFS and each CN has only cache on local storage).\n\n- FrontEnds (FE) are responsible for metadata management, client connection management, query planning, and query scheduling. Each FE stores and maintains a complete copy of the metadata in its memory, which guarantees indiscriminate services among the FEs.\n- BackEnds (BE) are responsible for data storage, data processing, and query execution. Each BE stores a portion of the data and processes the queries in parallel.\n\nKubeBlocks supports creating a **shared-nothing** StarRocks cluster.\n\n## Supported Features\n\n### Lifecycle Management\n\n|   Topology       | Horizontalscaling | Vertical scaling | Expandvolume | Restart   | Stop/Start | Configure | Expose | Switchover |\n|------------------|------------------------|-----------------------|-------------------|-----------|------------|-----------|--------|------------|\n| shared-nothing     | Yes                  | Yes                   | Yes               | Yes       | Yes        | No        | Yes    | N/A      |\n\n### Versions\n\n| Major Versions | Description |\n|----------------|-------------|\n| 3.3.x          | 3.3.0, 3.3.2|",
    "path": "docs/release-1_0/kubeblocks-for-starrocks/01-introduction",
    "description": "  # StarRocks  StarRocks is a Linux Foundation project, it is the next-generation data platform designed to make data-intensive real-time analytics fast and easy.  StarRocks supports **shared-nothing** (Each BE has a portion of the data on its local storage) and **shared-data** (all data on object s"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-starrocks_02-provision",
    "title": "Provision",
    "content": "\n\n## Before you start\n\n- [Install kbcli](./../installation/install-kbcli.md) if you want to manage the StarRocks cluster with `kbcli`.\n- [Install KubeBlocks](./../installation/install-kubeblocks.md).\n- [Install and enable the starrocks Addon](./../installation/install-addons.md).\n- To keep things isolated, create a separate namespace called `demo` throughout this tutorial.\n\n  ```bash\n  kubectl create namespace demo\n  ```\n\n## Create a cluster\n\nKubeBlocks implements a `Cluster` CRD to define a cluster. Here is an example of creating a StarRocks cluster. If you only have one node for deploying a cluster with multiple replicas, configure the cluster affinity by setting `spec.schedulingPolicy` or `spec.componentSpecs.schedulingPolicy`. For details, you can refer to the [API docs](../user_docs/references/api-reference/cluster#apps.kubeblocks.io/v1.SchedulingPolicy). But for a production environment, it is not recommended to deploy all replicas on one node, which may decrease the cluster availability.\n\n```yaml\ncat <<EOF | kubectl apply -f -\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: mycluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  componentSpecs:\n  - name: fe\n    componentDef: starrocks-ce-fe\n    serviceAccountName: kb-starrocks-cluster\n    replicas: 1\n    resources:\n      limits:\n        cpu: '1'\n        memory: 1Gi\n      requests:\n        cpu: '1'\n        memory: 1Gi\n  - name: be\n    componentDef: starrocks-ce-be\n    replicas: 2\n    resources:\n      limits:\n        cpu: '1'\n        memory: 1Gi\n      requests:\n        cpu: '1'\n        memory: 1Gi\n    volumeClaimTemplates:\n    - name: data\n      spec:\n        accessModes:\n        - ReadWriteOnce\n        resources:\n          requests:\n            storage: 20Gi\nEOF\n```\n\n| Field                                 | Definition  |\n|---------------------------------------|--------------------------------------|\n| `spec.terminationPolicy`              | It is the policy of cluster termination. V",
    "path": "docs/release-1_0/kubeblocks-for-starrocks/02-provision",
    "description": "  ## Before you start  - [Install kbcli](./../installation/install-kbcli.md) if you want to manage the StarRocks cluster with `kbcli`. - [Install KubeBlocks](./../installation/install-kubeblocks.md). - [Install and enable the starrocks Addon](./../installation/install-addons.md). - To keep things is"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-starrocks_03-scale",
    "title": "Scale",
    "content": "\n\n## Scale\n\n### Scale vertically\n\n#### Before you start\n\nCheck whether the cluster status is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n>\nNAME        CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    AGE\nmycluster   starrocks            starrocks-3.1.1   Delete               Running   4m29s\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS        CREATED-TIME\nmycluster   demo        starrocks            starrocks-3.1.1   Delete               Running       Jul 17,2024 19:06 UTC+0800\n```\n\n\n\n\n\n#### Steps\n\n\n\n1. Apply an OpsRequest to the specified cluster. Configure the parameters according to your needs.\n\n   ```bash\n   kubectl apply -f - \n   NAMESPACE   NAME                         TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        mycluster-vertical-scaling   VerticalScaling   mycluster   Succeed   3/3        6m\n   ```\n\n   If an error occurs, you can troubleshoot with `kubectl describe ops -n demo` command to view the events of this operation.\n\n3. Check whether the corresponding resources change.\n\n    ```bash\n    kubectl describe cluster mycluster -n demo\n    ```\n\n\n\n\n1. Change the configuration of `spec.componentSpecs.resources` in the YAML file. `spec.componentSpecs.resources` controls the requirement and limit of resources and changing them triggers a vertical scaling.\n\n   ```bash\n   kubectl edit cluster mycluster -n demo\n   ```\n\n   Edit the values of `spec.componentSpecs.resources`.\n\n   ```yaml\n   ...\n   spec:\n     clusterDefinitionRef: starrocks-ce\n     clusterVersionRef: starrocks-ce-3.1.1\n     componentSpecs:\n     - name: fe\n       componentDefRef: fe\n       replicas: 2\n       resources: # Change the values of resources\n         requests:\n           memory: \"2Gi\"\n           cpu: \"1\"\n         limits:\n           memory: \"4Gi\"\n           cpu: \"2\"\n   ...\n   ```\n\n2. Check ",
    "path": "docs/release-1_0/kubeblocks-for-starrocks/03-scale",
    "description": "  ## Scale  ### Scale vertically  #### Before you start  Check whether the cluster status is `Running`. Otherwise, the following operations may fail.    ```bash kubectl get cluster mycluster -n demo > NAME        CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    AGE mycluster   s"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-starrocks_04-stop-and-start",
    "title": "Stop/Start",
    "content": "\n\n## Stop/Start a cluster\n\nYou can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. You can start this cluster again by snapshots if you want to restore the cluster resources.\n\n### Stop a cluster\n\n1. Configure the name of your cluster and run the command below to stop this cluster.\n\n\n\n    Apply an OpsRequest to restart a cluster.\n\n    ```bash\n    kubectl apply -f - \n\n\n    ```bash\n    kubectl edit cluster mycluster -n demo\n    ```\n\n    Configure replicas as 0 to delete pods.\n\n    ```yaml\n    ...\n    spec:\n      clusterDefinitionRef: starrocks-ce\n      clusterVersionRef: starrocks-ce-3.1.1\n      terminationPolicy: Delete\n      affinity:\n        podAntiAffinity: Preferred\n        topologyKeys:\n        - kubernetes.io/hostname\n      tolerations:\n        - key: kb-data\n          operator: Equal\n          value: 'true'\n          effect: NoSchedule\n      componentSpecs:\n      - name: fe\n        componentDefRef: fe\n        serviceAccountName: kb-starrocks-cluster\n        replicas: 0 # Change this value\n      - name: be\n        componentDefRef: be\n        replicas: 0 # Change this value\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster stop mycluster -n demo\n    ```\n\n    \n\n    \n\n2. Check the status of the cluster to see whether it is stopped.\n\n\n\n    ```bash\n    kubectl get cluster mycluster -n demo\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster list mycluster -n demo\n    ```\n\n    \n\n    \n\n### Start a cluster\n\n1. Configure the name of your cluster and run the command below to start this cluster.\n\n\n\n   Apply an OpsRequest to start a cluster.\n\n   ```bash\n   kubectl apply -f - \n\n\n   ```bash\n   kubectl edit cluster mycluster -n demo\n   ```\n\n   Change replicas back to the original amount to start this cluster again.\n\n   ```yaml\n   spec:\n     clusterDefinitionRef: starrocks-ce\n     clusterVersionRef: starrocks-ce-3.1.1\n",
    "path": "docs/release-1_0/kubeblocks-for-starrocks/04-stop-and-start",
    "description": "  ## Stop/Start a cluster  You can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. You can start this cluster again by snapshots"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-starrocks_05-restart",
    "title": "Restart",
    "content": "\n\n## Restart\n\n\n\n1. Restart a cluster.\n\n   ```bash\n   kubectl apply -f - \n\n\n1. Configure the values of `components` and `ttlSecondsAfterSucceed` and run the command below to restart a specified cluster.\n\n   ```bash\n   kbcli cluster restart mycluster -n demo --components=\"starrocks\" --ttlSecondsAfterSucceed=30\n   ```\n\n   - `components` describes the component name that needs to be restarted.\n   - `ttlSecondsAfterSucceed` describes the time to live of an OpsRequest job after the restarting succeeds.\n\n2. Validate the restarting.\n\n   Run the command below to check the cluster status to check the restarting status.\n\n   ```bash\n   kbcli cluster list mycluster -n demo\n   >\n   NAME        NAMESPACE   CLUSTER-DEFINITION     VERSION             TERMINATION-POLICY   STATUS    CREATED-TIME\n   mycluster   demo        starrocks               starrocks-3.1.1    Delete               Running   Jul 17,2024 19:06 UTC+0800\n   ```\n\n   * STATUS=Updating: it means the cluster restart is in progress.\n   * STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n",
    "path": "docs/release-1_0/kubeblocks-for-starrocks/05-restart",
    "description": "  ## Restart    1. Restart a cluster.     ```bash    kubectl apply -f -    1. Configure the values of `components` and `ttlSecondsAfterSucceed` and run the command below to restart a specified cluster.     ```bash    kbcli cluster restart mycluster -n demo --components=\"starrocks\" --ttlSecondsAfterS"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-starrocks_06-expand-volume",
    "title": "Expand Volume",
    "content": "\n\n## Volume expansion\n\n### Before you start\n\nCheck whether the cluster status is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n>\nNAME        CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    AGE\nmycluster   starrocks            starrocks-3.1.1   Delete               Running   4m29s\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS        CREATED-TIME\nmycluster   demo        starrocks            starrocks-3.1.1   Delete               Running       Jul 17,2024 19:06 UTC+0800\n```\n\n\n\n\n\n### Steps\n\n\n\n1. Change the value of storage according to your need and run the command below to expand the volume of a cluster.\n\n    ```yaml\n    kubectl apply -f - \n    NAMESPACE   NAME                         TYPE              CLUSTER     STATUS    PROGRESS   AGE\n    demo        mycluster-volume-expansion   VolumeExpansion   mycluster   Succeed   3/3        6m\n    ```\n\n    If an error occurs, you can troubleshoot with `kubectl describe ops -n demo` command to view the events of this operation.\n\n3. Check whether the corresponding cluster resources change.\n\n    ```bash\n    kubectl describe cluster mycluster -n demo\n    ```\n\n\n\n\n1. Change the value of `spec.componentSpecs.volumeClaimTemplates.spec.resources` in the cluster YAML file.\n\n   `spec.componentSpecs.volumeClaimTemplates.spec.resources` is the storage resource information of the pod and changing this value triggers the volume expansion of a cluster.\n\n   ```bash\n   kubectl edit cluster mycluster -n demo\n   ```\n\n   Edit the values of `spec.componentSpecs.volumeClaimTemplates.spec.resources`.\n\n   ```yaml\n   ...\n   spec:\n     clusterDefinitionRef: starrocks-ce\n     clusterVersionRef: starrocks-ce-3.1.1\n     componentSpecs:\n     - name: be\n       componentDefRef: be\n       volumeClaimTemplates:\n       - name: be-storage\n         spec:\n           accessModes:\n       ",
    "path": "docs/release-1_0/kubeblocks-for-starrocks/06-expand-volume",
    "description": "  ## Volume expansion  ### Before you start  Check whether the cluster status is `Running`. Otherwise, the following operations may fail.    ```bash kubectl get cluster mycluster -n demo > NAME        CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    AGE mycluster   starrocks    "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-starrocks_10-delete",
    "title": "Delete",
    "content": "\n\n## Delete a cluster\n\n### Termination policy\n\n:::note\n\nThe termination policy determines how a cluster is deleted.\n\n:::\n\n| **terminationPolicy** | **Deleting Operation**                           |\n|:----------------------|:-------------------------------------------------|\n| `DoNotTerminate`      | `DoNotTerminate` prevents deletion of the Cluster. This policy ensures that all resources remain intact.       |\n| `Delete`              | `Delete` deletes Cluster resources like Pods, Services, and Persistent Volume Claims (PVCs), leading to a thorough cleanup while removing all persistent data.   |\n| `WipeOut`             | `WipeOut` is an aggressive policy that deletes all Cluster resources, including volume snapshots and backups in external storage. This results in complete data removal and should be used cautiously, primarily in non-production environments to avoid irreversible data loss.  |\n\nTo check the termination policy, execute the following command.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n>\nNAME        CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    AGE\nmycluster   starrocks            starrocks-3.1.1   Delete               Running   34m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION     VERSION         TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo                                               Delete               Running   Sep 30,2024 13:03 UTC+0800\n```\n\n\n\n\n\n### Steps\n\nRun the command below to delete a specified cluster.\n\n\n\nIf you want to delete a cluster and its all related resources, you can modify the termination policy to `WipeOut`, then delete the cluster.\n\n```bash\nkubectl patch -n demo cluster mycluster -p '}' --type=\"merge\"\n\nkubectl delete -n demo cluster mycluster\n```\n\n\n\n\n```bash\nkbcli cluster delete mycluster -n demo\n```\n\n\n\n\n",
    "path": "docs/release-1_0/kubeblocks-for-starrocks/10-delete",
    "description": "  ## Delete a cluster  ### Termination policy  :::note  The termination policy determines how a cluster is deleted.  :::  | **terminationPolicy** | **Deleting Operation**                           | |:----------------------|:-------------------------------------------------| | `DoNotTerminate`      "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-elasticsearch_04-operations_01-stop-start-restart",
    "title": "Elasticsearch  Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# Elasticsearch  Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a Elasticsearch  Cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a Elasticsearch  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a Elasticsearch  Cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Retains persistent storage (PVCs)\n3. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\n  Option 1: OpsRequest API\n\n  Create a Stop operation request:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: es-multinode-stop-ops\n    namespace: demo\n  spec:\n    clusterName: es-multinode\n    type: Stop\n  ```\n  \n\n\n  Option 2: Cluster API Patch\n\n  Modify the cluster spec directly by patching the stop field:\n\n  ```bash\n  kubectl patch cluster es-multinode -n demo --type='json' -p='[\n  ,\n  \n  ]'\n  ```\n\n  \n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster es-multinode -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME           CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\n    es-multinode                        Delete               Stopping   8m6s\n    es-multinode                        Del",
    "path": "docs/preview/kubeblocks-for-elasticsearch/04-operations/01-stop-start-restart",
    "description": "  # Elasticsearch  Cluster Lifecycle Management  This guide demonstrates how to manage a Elasticsearch  Cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help optimize"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-elasticsearch_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a Elasticsearch  Cluster",
    "content": "\n\n\n\n# Vertical Scaling for Elasticsearch  Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a Elasticsearch  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for Elasticsearch instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks ensures minimal impact during scaling operations by following a controlled, role-aware update strategy:\n**Role-Aware Replicas (Primary/Secondary Replicas)**\n- Secondary replicas update first – Non-leader pods are upgraded to minimize disruption.\n- Primary updates last – Only after all secondaries are healthy does the primary pod restart.\n- Cluster state progresses from Updating → Running once all replicas are stable.\n\n**Role-Unaware Replicas (Ordinal-Based Scaling)**\nIf replicas have no defined roles, updates follow Kubernetes pod ordinal order:\n- Highest ordinal first (e.g., pod-2 → pod-1 → pod-0) to ensure deterministic rollouts.\n\n## Prerequisites\n\n\n\n## Deploy a Elasticsearch  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Pods are updated in pod ordinal order, from highest to lowest, (e.g., pod-2 → pod-1 → pod-0)\n1. Cluster status transitions from `Updating` to `Running`\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the elasticsearch-broker component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: es-multinode-vscale-ops\n    namespace: demo\n  spec:\n    clusterName: es-multinode\n    type: VerticalScaling\n    verticalScaling:\n    - componentName: dit\n      requests:\n        cpu: '1'\n        memory: 1Gi\n      limits:\n       ",
    "path": "docs/preview/kubeblocks-for-elasticsearch/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for Elasticsearch  Clusters with KubeBlocks  This guide demonstrates how to vertically scale a Elasticsearch  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies compute resourc"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-elasticsearch_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of Elasticsearch Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for Elasticsearch Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a Elasticsearch cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a Elasticsearch  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running`.\n2. Cluster status changes from `Updating` to `Running`\n\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the Elasticsearch cluster by adding 1 replica to elasticsearch component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: es-multinode-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: es-multinode\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: dit\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops es-multinode-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME                         TYPE                CLUSTER        STATUS    PROGRESS   AGE\n  es-multinode-scale-out-ops   HorizontalScaling   es-multinode   Running   0/1        9s\n  es-multinode-scale-out-ops   HorizontalScaling   es-multinode   Running   1/1        16s\n  es-multinode-scale-out-ops   HorizontalScaling   es-multinode   Succeed   1/1        16s\n  ```\n  \n\n\n  Option 2: Direct Cluster API Update\n\n  Alternatively, you can perform a direct update to the `replicas` field in the Cluster resource:\n\n  ```yaml\n  apiVersion: apps.kubeblocks.io/v1\n  kind: Cluster\n  spec:\n    componentSpecs:\n      - name: dit\n        replicas: 4 # increase replicas",
    "path": "docs/preview/kubeblocks-for-elasticsearch/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for Elasticsearch Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a Elasticsearch cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Pr"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-elasticsearch_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a Elasticsearch Cluster",
    "content": "\n\n\n\n# Expanding Volume in a Elasticsearch Cluster\n\nThis guide explains how to expand Persistent Volume Claims (PVCs) in a Elasticsearch cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the underlying storage class, this operation can be performed without downtime.\n\nVolume expansion allows you to increase the size of a Persistent Volume Claim (PVC) after it has been created. This feature was introduced in Kubernetes v1.11 and became generally available (GA) in Kubernetes v1.24.\n\n## Prerequisites\n\n\n\n### Check the Storage Class for Volume Expansion Support\n\nList all available storage classes and verify if volume expansion is supported by checking the `ALLOWVOLUMEEXPANSION` field:\n```bash\nkubectl get storageclass\n```\n\nExample Output:\n```bash\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4d10h\nkb-default-sc       ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   3d7h\nsc-s3-repo-2qsxfh   ru.yandex.s3.csi        Retain          Immediate              false                  3d7h\n```\nEnsure the storage class you are using has `ALLOWVOLUMEEXPANSION` set to true. If it is false, the storage class does not support volume expansion.\n\n## Deploy a Elasticsearch  Cluster with StorageClass\n\nKubeBlocks uses a declarative approach to manage Elasticsearch clusters. Below is an example configuration for deploying a Elasticsearch cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: es-multinode\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  componentSpecs:\n    - name: dit\n      componentDef: elasticsearch-8\n      serviceVersion: 8.8.2\n      configs:\n",
    "path": "docs/preview/kubeblocks-for-elasticsearch/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a Elasticsearch Cluster  This guide explains how to expand Persistent Volume Claims (PVCs) in a Elasticsearch cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When suppo"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-elasticsearch_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy Elasticsearch Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage Elasticsearch Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing Elasticsearch services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n## Deploy a Elasticsearch  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## View Network Services\nList the Services created for the Elasticsearch cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=es-multinode -n demo\n```\n\nExample Services:\n```bash\nNAME                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nes-multinode-dit-http      ClusterIP   10.96.224.72           9200/TCP   56m\nes-multinode-master-http   ClusterIP   10.96.153.35           9200/TCP   56m\n```\n\n## Expose Elasticsearch Service\n\nExternal service addresses enable public internet access to Elasticsearch, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the Elasticsearch service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: es-multinode-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: es-multinode\n    expose:\n    - componentName: master\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options are 'ClusterIP', 'NodePort', and 'LoadBalancer'.\n        serviceTyp",
    "path": "docs/preview/kubeblocks-for-elasticsearch/04-operations/05-manage-loadbalancer",
    "description": "    # Manage Elasticsearch Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing Elasticsearch services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer se"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-elasticsearch_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed Elasticsearch Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed Elasticsearch Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in Elasticsearch clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a Elasticsearch Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\nTo decommission a specific Pod (e.g., 'es-multinode-dit-1'), you can use one of the following methods:\n\n\n\n  Option 1: Using OpsRequest\n\n  Create an OpsRequest to mark the Pod as offline:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: es-multinode-decommission-ops\n    namespace: demo\n  spec:\n    clusterName: es-multinode\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: dit\n      scaleIn:\n        onlineInstancesToOffline:\n          - 'es-multinode-dit-1'  # Specifies the instance names that need to be taken offline\n  ```\n\n  #### Monitor the Decommiss",
    "path": "docs/preview/kubeblocks-for-elasticsearch/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed Elasticsearch Clusters  This guide explains how to decommission (take offline) specific Pods in Elasticsearch clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use th"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-elasticsearch_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for Elasticsearch Clusters with the Prometheus Operator",
    "content": "\n\n# Elasticsearch Monitoring with Prometheus Operator\n\nThis guide demonstrates how to configure comprehensive monitoring for Elasticsearch clusters in KubeBlocks using:\n\n1. Prometheus Operator for metrics collection\n2. Built-in Elasticsearch exporter for metrics exposure\n3. Grafana for visualization\n\n## Prerequisites\n\n\n\n## Install Monitoring Stack\n\n### 1. Install Prometheus Operator\nDeploy the kube-prometheus-stack using Helm:\n\n```bash\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  -n monitoring \\\n  --create-namespace\n```\n\n### 2. Verify Installation\nCheck all components are running:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-jmxg8                1/1     Running   0          2m1s\n```\n\n\n## Deploy a Elasticsearch Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Configure Metrics Collection\n\n### 1. Verify Exporter Endpoint\n\n```bash\nkubectl -n demo exec -it pods/es-multinode-dit-0 -- \\\n  curl -s http://127.0.0.1:9114/metrics | head -n 50\n\nkubectl -n demo exec -it pods/es-multinode-master-0 -- \\\n  curl -s http:",
    "path": "docs/preview/kubeblocks-for-elasticsearch/08-monitoring/01-integrate-with-prometheus-operator",
    "description": "  # Elasticsearch Monitoring with Prometheus Operator  This guide demonstrates how to configure comprehensive monitoring for Elasticsearch clusters in KubeBlocks using:  1. Prometheus Operator for metrics collection 2. Built-in Elasticsearch exporter for metrics exposure 3. Grafana for visualization"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-elasticsearch__tpl__create-cluster",
    "title": "_create-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing Elasticsearch  Clusters.\nBelow is an example configuration for deploying a Elasticsearch Cluster with\ncreate a cluster with replicas for different roles.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: es-multinode\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  componentSpecs:\n    - name: dit\n      componentDef: elasticsearch-8\n      serviceVersion: 8.8.2\n      configs:\n        - name: es-cm\n          variables:\n            # use key `roles` to specify roles this component assume\n            roles: data,ingest,transform\n      replicas: 3\n      disableExporter: false\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: \"2Gi\"\n        requests:\n          cpu: \"1\"\n          memory: \"2Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n    - name: master\n      componentDef: elasticsearch-8\n      serviceVersion: 8.8.2\n      configs:\n        - name: es-cm\n          variables:\n            # use key `roles` to specify roles this component assume\n            roles: master\n      replicas: 3\n      disableExporter: false\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: \"2Gi\"\n        requests:\n          cpu: \"1\"\n          memory: \"2Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```",
    "path": "docs/preview/kubeblocks-for-elasticsearch/_tpl/_create-cluster",
    "description": "KubeBlocks uses a declarative approach for managing Elasticsearch  Clusters. Below is an example configuration for deploying a Elasticsearch Cluster with create a cluster with replicas for different roles.  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps.kubeb"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-elasticsearch__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/preview/kubeblocks-for-elasticsearch/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-elasticsearch__tpl__verify-cluster",
    "title": "_verify-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster es-multinode -n demo -w\n```\n\nExpected Output:\n\n```bash\nNAME           CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nes-multinode                        Delete               Creating   10s\nes-multinode                        Delete               Updating   41s\nes-multinode                        Delete               Running    42s\n```\n\nCheck the pod status and roles:\n```bash\nkubectl get pods -l app.kubernetes.io/instance=es-multinode -n demo\n```\n\nExpected Output:\n```bash\nNAME                    READY   STATUS    RESTARTS   AGE\nes-multinode-dit-0      3/3     Running   0          6m21s\nes-multinode-dit-1      3/3     Running   0          6m21s\nes-multinode-dit-2      3/3     Running   0          6m21s\nes-multinode-master-0   3/3     Running   0          6m21s\nes-multinode-master-1   3/3     Running   0          6m21s\nes-multinode-master-2   3/3     Running   0          6m21s\n```\n\nOnce the cluster status becomes Running, your Elasticsearch cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::\n",
    "path": "docs/preview/kubeblocks-for-elasticsearch/_tpl/_verify-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster es-multinode -n demo -w ```  Expected Output:  ```bash NAME           CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE es-multinode                        Delete               Creating   10s es-"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-kafka_04-operations_01-stop-start-restart",
    "title": "Kafka  Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# Kafka  Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a Kafka  Cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a Kafka  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a Kafka  Cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Retains persistent storage (PVCs)\n3. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\n  Option 1: OpsRequest API\n\n  Create a Stop operation request:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: kafka-separated-cluster-stop-ops\n    namespace: demo\n  spec:\n    clusterName: kafka-separated-cluster\n    type: Stop\n  ```\n  \n\n\n  Option 2: Cluster API Patch\n\n  Modify the cluster spec directly by patching the stop field:\n\n  ```bash\n  kubectl patch cluster kafka-separated-cluster -n demo --type='json' -p='[\n  ,\n  ,\n  \n  ]'\n  ```\n\n  \n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster kafka-separated-cluster -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME                      CLUSTER-DEFINITION    TERMINATION-POLICY   STATUS     AGE\n    kafka-separated-cluster   kafka                 Delete               Stopping   16m3s\n  ",
    "path": "docs/preview/kubeblocks-for-kafka/04-operations/01-stop-start-restart",
    "description": "  # Kafka  Cluster Lifecycle Management  This guide demonstrates how to manage a Kafka  Cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help optimize resource usage "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-kafka_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a Kafka  Cluster",
    "content": "\n\n\n\n# Vertical Scaling for Kafka  Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a Kafka  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for Kafka instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks ensures minimal impact during scaling operations by following a controlled, role-aware update strategy:\n**Role-Aware Replicas (Primary/Secondary Replicas)**\n- Secondary replicas update first – Non-leader pods are upgraded to minimize disruption.\n- Primary updates last – Only after all secondaries are healthy does the primary pod restart.\n- Cluster state progresses from Updating → Running once all replicas are stable.\n\n**Role-Unaware Replicas (Ordinal-Based Scaling)**\nIf replicas have no defined roles, updates follow Kubernetes pod ordinal order:\n- Highest ordinal first (e.g., pod-2 → pod-1 → pod-0) to ensure deterministic rollouts.\n\n## Prerequisites\n\n\n\n## Deploy a Kafka  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Pods are updated in pod ordinal order, from highest to lowest, (e.g., pod-2 → pod-1 → pod-0)\n1. Cluster status transitions from `Updating` to `Running`\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the kafka-broker component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: kafka-separated-cluster-vscale-ops\n    namespace: demo\n  spec:\n    clusterName: kafka-separated-cluster\n    type: VerticalScaling\n    verticalScaling:\n    - componentName: kafka-broker\n      requests:\n        cpu: '1'\n        memory: 1Gi\n      limits:\n        cpu: '1'",
    "path": "docs/preview/kubeblocks-for-kafka/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for Kafka  Clusters with KubeBlocks  This guide demonstrates how to vertically scale a Kafka  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies compute resources (CPU and memo"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-kafka_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of Kafka Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for Kafka Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a Kafka cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a Kafka  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running`.\n2. Cluster status changes from `Updating` to `Running`\n\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the Kafka cluster by adding 1 replica to kafka component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: kafka-separated-cluster-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: kafka-separated-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: kafka-broker\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops kafka-separated-cluster-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME                                    TYPE                CLUSTER                   STATUS    PROGRESS   AGE\n  kafka-separated-cluster-scale-out-ops   HorizontalScaling   kafka-separated-cluster   Running   0/1        9s\n  kafka-separated-cluster-scale-out-ops   HorizontalScaling   kafka-separated-cluster   Running   1/1        16s\n  kafka-separated-cluster-scale-out-ops   HorizontalScaling   kafka-separated-cluster   Succeed   1/1        16s\n  ```\n  \n\n\n  Option 2: Direct Cluster API Update\n\n  Alternatively, you can perform a direct update to the `replicas` field in the Cluster resource:\n\n  ```yaml\n  apiVersion: apps.kubeblocks.io/v1\n  kind: Clu",
    "path": "docs/preview/kubeblocks-for-kafka/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for Kafka Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a Kafka cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequisites    #"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-kafka_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a Kafka Cluster",
    "content": "\n\n\n\n# Expanding Volume in a Kafka Cluster\n\nThis guide explains how to expand Persistent Volume Claims (PVCs) in a Kafka cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the underlying storage class, this operation can be performed without downtime.\n\nVolume expansion allows you to increase the size of a Persistent Volume Claim (PVC) after it has been created. This feature was introduced in Kubernetes v1.11 and became generally available (GA) in Kubernetes v1.24.\n\n## Prerequisites\n\n\n\n### Check the Storage Class for Volume Expansion Support\n\nList all available storage classes and verify if volume expansion is supported by checking the `ALLOWVOLUMEEXPANSION` field:\n```bash\nkubectl get storageclass\n```\n\nExample Output:\n```bash\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4d10h\nkb-default-sc       ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   3d7h\nsc-s3-repo-2qsxfh   ru.yandex.s3.csi        Retain          Immediate              false                  3d7h\n```\nEnsure the storage class you are using has `ALLOWVOLUMEEXPANSION` set to true. If it is false, the storage class does not support volume expansion.\n\n## Deploy a Kafka  Cluster with StorageClass\n\nKubeBlocks uses a declarative approach to manage Kafka clusters. Below is an example configuration for deploying a Kafka cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: kafka-separated-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: kafka\n  topology: separated_monitor\n  componentSpecs:\n    - name: kafka-broker\n      replicas: 3\n      resources:\n        limits",
    "path": "docs/preview/kubeblocks-for-kafka/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a Kafka Cluster  This guide explains how to expand Persistent Volume Claims (PVCs) in a Kafka cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the unde"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-kafka_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy Kafka Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage Kafka Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing Kafka services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n## Deploy a Kafka  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## View Network Services\nList the Services created for the Kafka cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=kafka-separated-cluster -n demo\n```\n\nExample Services:\n```bash\nNAME                                                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nkafka-separated-cluster-kafka-broker-advertised-listener-0   ClusterIP   10.96.101.247           9092/TCP   19m\n```\n\n## Expose Kafka Service\n\nExternal service addresses enable public internet access to Kafka, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the Kafka service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: kafka-separated-cluster-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: kafka-separated-cluster\n    expose:\n    - componentName: kafka-broker\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options are 'ClusterIP', 'NodePort', and 'LoadBalancer'.\n        serviceType: LoadBalancer\n        ",
    "path": "docs/preview/kubeblocks-for-kafka/04-operations/05-manage-loadbalancer",
    "description": "    # Manage Kafka Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing Kafka services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage i"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-kafka_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed Kafka Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed Kafka Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in Kafka clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a Kafka Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\n\nBefore decommissioning a specific pod from a component, make sure this component has more than one replicas.\nIf not, please scale out the component ahead.\n\nE.g. you can patch the cluster CR with command, to declare there are 3 replicas in component querynode.\n\n```bash\nkubectl patch cluster kafka-separated-cluster -n demo --type='json' -p='[\n  \n]'\n```\n\nWait till all pods are running\n```bash\nkubectl get pods -n demo -l app.kubernetes.io/instance=kafka-separated-cluster,apps.kubeblocks.io/component-name=kafka-broker\n```\nExpected Output:\n```\nNAME                                     READY   STATUS    RESTARTS   AGE\nkafka-separated-cluster-kafka-broke",
    "path": "docs/preview/kubeblocks-for-kafka/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed Kafka Clusters  This guide explains how to decommission (take offline) specific Pods in Kafka clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-kafka_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for Kafka Clusters with the Prometheus Operator",
    "content": "\n\n# Kafka Monitoring with Prometheus Operator\n\nThis guide demonstrates how to configure comprehensive monitoring for Kafka clusters in KubeBlocks using:\n\n1. Prometheus Operator for metrics collection\n2. Built-in Kafka exporter for metrics exposure\n3. Grafana for visualization\n\n## Prerequisites\n\n\n\n## Install Monitoring Stack\n\n### 1. Install Prometheus Operator\nDeploy the kube-prometheus-stack using Helm:\n\n```bash\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  -n monitoring \\\n  --create-namespace\n```\n\n### 2. Verify Installation\nCheck all components are running:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-jmxg8                1/1     Running   0          2m1s\n```\n\n\n## Deploy a Kafka Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Configure Metrics Collection\n\n### 1. Get Exporter details\n\n```bash\nkubectl get po -n demo kafka-separated-cluster-kafka-broker-0 -oyaml | yq '.spec.containers[] | select(.name==\"jmx-exporter\") | .ports'\n```\n\n\nExample Output:\n\n```text\n- containerPort: 5556\n  name: metrics\n  protocol:",
    "path": "docs/preview/kubeblocks-for-kafka/08-monitoring/01-integrate-with-prometheus-operator",
    "description": "  # Kafka Monitoring with Prometheus Operator  This guide demonstrates how to configure comprehensive monitoring for Kafka clusters in KubeBlocks using:  1. Prometheus Operator for metrics collection 2. Built-in Kafka exporter for metrics exposure 3. Grafana for visualization  ## Prerequisites    ##"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-kafka__tpl__create-cluster",
    "title": "_create-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing Kafka  Clusters.\nBelow is an example configuration for deploying a Kafka Cluster with 3 components\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: kafka-separated-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: kafka\n  topology: separated_monitor\n  componentSpecs:\n    - name: kafka-broker\n      replicas: 1\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      env:\n        - name: KB_KAFKA_BROKER_HEAP\n          value: \"-XshowSettings:vm -XX:MaxRAMPercentage=100 -Ddepth=64\"\n        - name: KB_KAFKA_CONTROLLER_HEAP\n          value: \"-XshowSettings:vm -XX:MaxRAMPercentage=100 -Ddepth=64\"\n        - name: KB_BROKER_DIRECT_POD_ACCESS\n          value: \"true\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n        - name: metadata\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 1Gi\n    - name: kafka-controller\n      replicas: 1\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: metadata\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 1Gi\n    - name: kafka-exporter\n      replicas: 1\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"1Gi\"\n        requests:\n          cpu: \"0.1\"\n          memory: \"0.2Gi\"\n```\n\n\n:::note\n\nThese three c",
    "path": "docs/preview/kubeblocks-for-kafka/_tpl/_create-cluster",
    "description": "KubeBlocks uses a declarative approach for managing Kafka  Clusters. Below is an example configuration for deploying a Kafka Cluster with 3 components  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps.kubeblocks.io/v1 kind: Cluster metadata:   name: kafka-separ"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-kafka__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/preview/kubeblocks-for-kafka/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-kafka__tpl__verify-cluster",
    "title": "_verify-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster kafka-separated-cluster -n demo -w\n```\n\nExpected Output:\n\n```bash\nkubectl get cluster kafka-separated-cluster -n demo\nNAME                      CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nkafka-separated-cluster   kafka                Delete               Creating   13s\nkafka-separated-cluster   kafka                Delete               Running    63s\n```\n\nCheck the pod status and roles:\n```bash\nkubectl get pods -l app.kubernetes.io/instance=kafka-separated-cluster -n demo\n```\n\nExpected Output:\n```bash\nNAME                                         READY   STATUS    RESTARTS   AGE\nkafka-separated-cluster-kafka-broker-0       2/2     Running   0          13m\nkafka-separated-cluster-kafka-controller-0   2/2     Running   0          13m\nkafka-separated-cluster-kafka-exporter-0     1/1     Running   0          12m\n```\n\nOnce the cluster status becomes Running, your Kafka cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::\n",
    "path": "docs/preview/kubeblocks-for-kafka/_tpl/_verify-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster kafka-separated-cluster -n demo -w ```  Expected Output:  ```bash kubectl get cluster kafka-separated-cluster -n demo NAME                      CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE k"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-milvus_03-topologies_01-standlone",
    "title": "Deploying a Milvus Standalone Cluster with KubeBlocks",
    "content": "\n# Deploying a Milvus Standalone Cluster with KubeBlocks\n\nStandalone is a lightweight deployment suitable for development and testing with following components:\n\n- **Milvus Core**: Provides vector search and database functionality\n- **Metadata Storage (ETCD)**: Stores cluster metadata and configuration\n- **Object Storage (MinIO/S3)**: Persists vector data and indexes\n\n## Prerequisites\n\n\n\n## Deploying the Milvus Standalone Cluster\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: milvus-standalone\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  # The value must be `milvus` to create a Milvus Cluster\n  clusterDef: milvus\n  # Valid options are: [standalone,cluster]\n  topology: standalone\n  componentSpecs:\n    - name: etcd\n      replicas: 1\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n    - name: minio\n      replicas: 1\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n    - name: milvus\n      replicas: 1\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n**Key Configuration",
    "path": "docs/preview/kubeblocks-for-milvus/03-topologies/01-standlone",
    "description": " # Deploying a Milvus Standalone Cluster with KubeBlocks  Standalone is a lightweight deployment suitable for development and testing with following components:  - **Milvus Core**: Provides vector search and database functionality - **Metadata Storage (ETCD)**: Stores cluster metadata and configurat"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-milvus_03-topologies_02-cluster",
    "title": "Deploying a Milvus Cluster with KubeBlocks",
    "content": "\n# Deploying a Milvus Cluster with KubeBlocks\n\nMilvus Cluster is a distributed deployment for production workloads with multiple specialized components:\n\n**Access Layer**\n\n- Stateless proxies that handle client connections and request routing\n\n**Compute Layer**\n\n- Query Nodes: Execute search operations\n- Data Nodes: Handle data ingestion and compaction\n- Index Nodes: Build and maintain vector indexes\n\n**Coordination Layer**\n\n- Root Coordinator: Manages global metadata\n- Query Coordinator: Orchestrates query execution\n- Data Coordinator: Manages data distribution\n- Index Coordinator: Oversees index building\n\n**Storage Layer**\n\n- Metadata Storage (ETCD): Cluster metadata and configuration\n- Object Storage (MinIO/S3): Persistent vector data storage\n- Log Storage (Pulsar): Message queue for change data capture\n\n## Prerequisites\n\n\n\n## Deploying the Milvus Cluster\n\n### Step 1. Deploy an ETCD Cluster\n\nETCD cluster is for metadata storage\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: etcdm-cluster\n  namespace: demo\nspec:\n  terminationPolicy: WipeOut\n  componentSpecs:\n    - name: etcd\n      componentDef: etcd-3-1.0.0\n      serviceVersion: 3.5.6\n      replicas: 1\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n\n### Step 2. Deploy a minio Cluster\n\nMinio is for object storage\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: miniom-cluster\n  namespace: demo\nspec:\n  terminationPolicy: WipeOut\n  componentSpecs:\n    - name: minio\n      componentDef: milvus-minio-1.0.0\n      replicas: 1\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n       ",
    "path": "docs/preview/kubeblocks-for-milvus/03-topologies/02-cluster",
    "description": " # Deploying a Milvus Cluster with KubeBlocks  Milvus Cluster is a distributed deployment for production workloads with multiple specialized components:  **Access Layer**  - Stateless proxies that handle client connections and request routing  **Compute Layer**  - Query Nodes: Execute search operati"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-milvus_04-operations_01-stop-start-restart",
    "title": "Milvus Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# Milvus Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a Milvus Cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a Milvus Cluster\n\nPlease refer to [Deploying a Milvus Cluster with KubeBlocks](../03-topologies/02-cluster) to deploy a milvus cluster.\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a Milvus Cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\nOption 1: OpsRequest API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: milvus-cluster-stop-ops\n  namespace: demo\nspec:\n  clusterName: milvus-cluster\n  type: Stop\n```\n\n\n\nOption 2: Cluster API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: milvus-cluster-stop-ops\n  namespace: demo\nspec:\n  clusterName: milvus-cluster\n  type: Stop\n```\n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster milvus-cluster -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME             CLUSTER-DEFINITION  TERMINATION-POLICY   STATUS     AGE\n    milvus-cluster   milvus              Delete               Stopping   ",
    "path": "docs/preview/kubeblocks-for-milvus/04-operations/01-stop-start-restart",
    "description": "  # Milvus Cluster Lifecycle Management  This guide demonstrates how to manage a Milvus Cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help optimize resource usage "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-milvus_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a Milvus  Cluster",
    "content": "\n\n\n\n# Vertical Scaling for Milvus Standalone Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a Milvus  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for Milvus instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks ensures minimal impact during scaling operations by following a controlled, role-aware update strategy:\n**Role-Aware Replicas (Primary/Secondary Replicas)**\n- Secondary replicas update first – Non-leader pods are upgraded to minimize disruption.\n- Primary updates last – Only after all secondaries are healthy does the primary pod restart.\n- Cluster state progresses from Updating → Running once all replicas are stable.\n\n**Role-Unaware Replicas (Ordinal-Based Scaling)**\nIf replicas have no defined roles, updates follow Kubernetes pod ordinal order:\n- Highest ordinal first (e.g., pod-2 → pod-1 → pod-0) to ensure deterministic rollouts.\n\n## Prerequisites\n\n\n\n## Deploy a Milvus Cluster\n\nPlease refer to [Deploying a Milvus Cluster with KubeBlocks](../03-topologies/02-cluster) to deploy a milvus cluster.\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Pods are updated in pod ordinal order, from highest to lowest, (e.g., pod-2 → pod-1 → pod-0)\n1. Cluster status transitions from `Updating` to `Running`\n\n**Check Components**\n\nThere are five components in Milvus Cluster. To get the list of components,\n```bash\nkubectl get cluster -n demo milvus-cluster -oyaml | yq '.spec.componentSpecs[].name'\n```\n\nExpected Output:\n```text\nproxy\nmixcoord\ndatanode\nindexnode\nquerynode\n```\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the **querynode** component",
    "path": "docs/preview/kubeblocks-for-milvus/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for Milvus Standalone Clusters with KubeBlocks  This guide demonstrates how to vertically scale a Milvus  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies compute resources ("
  },
  {
    "id": "docs_en_preview_kubeblocks-for-milvus_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of Milvus Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for Milvus Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a Milvus cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a Milvus Cluster\n\nPlease refer to [Deploying a Milvus Cluster with KubeBlocks](../03-topologies/02-cluster) to deploy a milvus cluster.\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running`.\n2. Cluster status changes from `Updating` to `Running`\n\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the Milvus cluster by adding 1 replica to milvus component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: milvus-cluster-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: milvus-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: querynode\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops milvus-cluster-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME                             TYPE                CLUSTER          STATUS    PROGRESS   AGE\n  milvus-cluster-scale-out-ops     HorizontalScaling   milvus-cluster   Running   0/1        9s\n  milvus-cluster-scale-out-ops     HorizontalScaling   milvus-cluster   Running   1/1        16s\n  milvus-cluster-scale-out-ops     HorizontalScaling   milvus-cluster   Succeed   1/1        16s\n  ```\n  \n\n\n  Option 2: Direct Cluster API Update\n\n  Alternatively, you can perform a direct update to the `replicas` field in the Cluster resource:\n\n  ```yaml\n  apiVersion: apps.kubeblocks.io/v1\n  kind: Cluster\n",
    "path": "docs/preview/kubeblocks-for-milvus/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for Milvus Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a Milvus cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequisites   "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-milvus_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy Milvus Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage Milvus Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing Milvus services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n\n## Deploy a Milvus Cluster\n\nPlease refer to [Deploying a Milvus Cluster with KubeBlocks](../03-topologies/02-cluster) to deploy a milvus cluster.\n\n## View Network Services\nList the Services created for the Milvus cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=milvus-cluster -n demo\n```\n\nExample Services:\n```bash\nNAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)              AGE\nmilvus-cluster-proxy   ClusterIP   10.96.157.187           19530/TCP,9091/TCP   133m\n```\n\n## Expose Milvus Service\n\nExternal service addresses enable public internet access to Milvus, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the Milvus service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: milvus-cluster-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: milvus-cluster\n    expose:\n    - componentName: milvus\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options are 'ClusterIP', 'NodePort', and 'LoadBalancer'.\n        serviceType: LoadBalancer\n   ",
    "path": "docs/preview/kubeblocks-for-milvus/04-operations/05-manage-loadbalancer",
    "description": "    # Manage Milvus Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing Milvus services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-milvus_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed Milvus Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed Milvus Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in Milvus clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a Milvus Cluster\n\nPlease refer to [Deploying a Milvus Cluster with KubeBlocks](../03-topologies/02-cluster) to deploy a milvus cluster.\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\n\nBefore decommissioning a specific pod from a component, make sure this component has more than one replicas.\nIf not, please scale out the componen ahead.\n\nE.g. you can patch the cluster CR with command, to declare there are 3 replicas in component querynode.\n\n```bash\nkubectl patch cluster milvus-cluster -n demo --type='json' -p='[\n  \n]'\n```\n\n\nTo decommission a specific Pod (e.g., 'milvus-cluster-querynode-1'), you can use one of the following methods:\n\n\n\n  Option 1: Using OpsRequest\n\n  Create an OpsRequest to mark the Pod as offline:\n\n  ```yaml\n  apiVersion:",
    "path": "docs/preview/kubeblocks-for-milvus/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed Milvus Clusters  This guide explains how to decommission (take offline) specific Pods in Milvus clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workloa"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-milvus_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for Milvus Clusters with the Prometheus Operator",
    "content": "\n\n# Milvus Monitoring with Prometheus Operator\n\nThis guide demonstrates how to configure comprehensive monitoring for Milvus clusters in KubeBlocks using:\n\n1. Prometheus Operator for metrics collection\n2. Built-in Milvus exporter for metrics exposure\n3. Grafana for visualization\n\n## Prerequisites\n\n\n\n## Install Monitoring Stack\n\n### 1. Install Prometheus Operator\nDeploy the kube-prometheus-stack using Helm:\n\n```bash\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  -n monitoring \\\n  --create-namespace\n```\n\n### 2. Verify Installation\nCheck all components are running:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-jmxg8                1/1     Running   0          2m1s\n```\n\n\n## Deploy a Milvus Cluster\n\nPlease refer to [Deploying a Milvus Cluster with KubeBlocks](../03-topologies/02-cluster) to deploy a milvus cluster.\n\n## Configure Metrics Collection\n\n### 1. Verify Exporter Endpoint\n\n```bash\nkubectl -n demo exec -it pods/milvus-cluster-proxy-0 -- \\\n  curl -s http://127.0.0.1:9091/metrics | head -n 50\n```\n\nPerfor",
    "path": "docs/preview/kubeblocks-for-milvus/08-monitoring/01-integrate-with-prometheus-operator",
    "description": "  # Milvus Monitoring with Prometheus Operator  This guide demonstrates how to configure comprehensive monitoring for Milvus clusters in KubeBlocks using:  1. Prometheus Operator for metrics collection 2. Built-in Milvus exporter for metrics exposure 3. Grafana for visualization  ## Prerequisites   "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-milvus__tpl__create-cluster",
    "title": "_create-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing Milvus Clusters.\nBelow is an example configuration for deploying a Milvus Cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: milvus-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: milvus\n  topology: clustermode\n  componentSpecs:\n    - name: milvus\n      serviceVersion: 3.13.7\n      replicas: 3\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n",
    "path": "docs/preview/kubeblocks-for-milvus/_tpl/_create-cluster",
    "description": "KubeBlocks uses a declarative approach for managing Milvus Clusters. Below is an example configuration for deploying a Milvus Cluster with 3 replicas.  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps.kubeblocks.io/v1 kind: Cluster metadata:   name: milvus-clus"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-milvus__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/preview/kubeblocks-for-milvus/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-milvus__tpl__verify-cluster",
    "title": "_verify-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster milvus-cluster -n demo -w\n```\n\nExpected Output:\n\n```bash\nNAME                CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nmilvus-standalone   milvus               Delete               Creating   40s\nmilvus-standalone   milvus               Delete               Creating   71s\nmilvus-standalone   milvus               Delete               Creating   71s\nmilvus-standalone   milvus               Delete               Updating   71s\nmilvus-standalone   milvus               Delete               Running    2m55s\n```\n\nCheck the component and pod status:\n\n```bash\nkubectl get component -n demo -l app.kubernetes.io/instance=milvus-standalone\n```\nExpected Output:\n```bash\nNAME                       DEFINITION                        SERVICE-VERSION   STATUS    AGE\nmilvus-standalone-etcd     etcd-3-1.0.0                      3.5.15            Running   3m5s\nmilvus-standalone-milvus   milvus-standalone-1.0.0           v2.3.2            Running   114s\nmilvus-standalone-minio    milvus-minio-1.0.0                8.0.17            Running   3m5s\n```\n\n\n```bash\nkubectl get pods -l app.kubernetes.io/instance=milvus-standalone -n demo\n```\n\nExpected Output:\n```bash\nNAME                         READY   STATUS    RESTARTS   AGE\nmilvus-standalone-etcd-0     2/2     Running   0          4m31s\nmilvus-standalone-milvus-0   1/1     Running   0          3m20s\nmilvus-standalone-minio-0    1/1     Running   0          4m31s\n```\n\nOnce the cluster status becomes Running, your Milvus cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::\n",
    "path": "docs/preview/kubeblocks-for-milvus/_tpl/_verify-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster milvus-cluster -n demo -w ```  Expected Output:  ```bash NAME                CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE milvus-standalone   milvus               Delete               Creati"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_04-operations_01-stop-start-restart",
    "title": "MongoDB ReplicaSet Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# MongoDB ReplicaSet Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a MongoDB ReplicaSet Cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB ReplicaSet Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a MongoDB ReplicaSet Cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Retains persistent storage (PVCs)\n3. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\nOption 1: OpsRequest API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: mongo-cluster-stop-ops\n  namespace: demo\nspec:\n  clusterName: mongo-cluster\n  type: Stop\n```\n\n\n\nOption 2: Cluster API Patch\n\nModify the cluster spec directly by patching the stop field:\n\n```bash\nkubectl patch cluster mongo-cluster -n demo --type='json' -p='[\n\n]'\n```\n\n\n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster mongo-cluster -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME            CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\n    mongo-cluster   mongodb              Delete               Stopping   6m3s\n    mongo-cluster   mongodb              Delete               Stopp",
    "path": "docs/preview/kubeblocks-for-mongodb/04-operations/01-stop-start-restart",
    "description": "  # MongoDB ReplicaSet Cluster Lifecycle Management  This guide demonstrates how to manage a MongoDB ReplicaSet Cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a MongoDB ReplicaSet Cluster",
    "content": "\n\n\n\n# Vertical Scaling for MongoDB Replication Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a MongoDB ReplicaSet Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for MongoDB instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks orchestrates scaling with minimal impact:\n1. Secondary replicas update first\n2. Primary updates last after secondaries are healthy\n3. Cluster status transitions from `Updating` to `Running`\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB ReplicaSet Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Secondary replicas are updated first (one at a time)\n1. Primary is updated last after secondary replicas are healthy\n1. Cluster status transitions from `Updating` to `Running`\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the mongodb component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: mongo-cluster-vscale-ops\n    namespace: demo\n  spec:\n    clusterName: mongo-cluster\n    type: VerticalScaling\n    verticalScaling:\n    - componentName: mongodb\n      requests:\n        cpu: '1'\n        memory: 1Gi\n      limits:\n        cpu: '1'\n        memory: 1Gi\n  ```\n  What Happens During Vertical Scaling?\n  - Secondary Pods are recreated first to ensure the primary Pod remains available.\n  - Once all secondary Pods are updated, the primary Pod is restarted with the new resource configuration.\n\n\n  You can check the progress of the scaling operation with the following command:\n\n  ```bash\n  kubectl -n demo get ops mongo-cluster-vscale-ops -w\n  ```\n\n  E",
    "path": "docs/preview/kubeblocks-for-mongodb/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for MongoDB Replication Clusters with KubeBlocks  This guide demonstrates how to vertically scale a MongoDB ReplicaSet Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies comput"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of MongoDB Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for MongoDB Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a MongoDB cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB ReplicaSet Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running` with `secondary` role\n2. Data synced from primary to new replica\n3. Cluster status changes from `Updating` to `Running`\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the MongoDB cluster by adding 1 replica to mongodb component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: mongo-cluster-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: mongo-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: mongodb\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops mongo-cluster-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME                          TYPE                CLUSTER         STATUS    PROGRESS   AGE\n  mongo-cluster-scale-out-ops   HorizontalScaling   mongo-cluster   Running   0/1        9s\n  mongo-cluster-scale-out-ops   HorizontalScaling   mongo-cluster   Running   1/1        20s\n  mongo-cluster-scale-out-ops   HorizontalScaling   mongo-cluster   Succeed   1/1        20s\n  ```\n  \n\n\n  Option 2: Direct Cluster API Update\n\n  Alternatively, you can perform a direct update to the `replicas` field in the Cluster resource:\n\n  ```yaml\n  apiVersion: apps.kubeblocks.io/v1\n  kind: Cluster\n  spec:\n    componentSpecs:",
    "path": "docs/preview/kubeblocks-for-mongodb/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for MongoDB Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a MongoDB cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequisites "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a MongoDB Cluster",
    "content": "\n\n\n\n# Expanding Volume in a MongoDB Cluster\n\nThis guide explains how to expand Persistent Volume Claims (PVCs) in a MongoDB cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the underlying storage class, this operation can be performed without downtime.\n\nVolume expansion allows you to increase the size of a Persistent Volume Claim (PVC) after it has been created. This feature was introduced in Kubernetes v1.11 and became generally available (GA) in Kubernetes v1.24.\n\n## Prerequisites\n\n\n\n### Check the Storage Class for Volume Expansion Support\n\nList all available storage classes and verify if volume expansion is supported by checking the `ALLOWVOLUMEEXPANSION` field:\n```bash\nkubectl get storageclass\n```\n\nExample Output:\n```bash\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4d10h\nkb-default-sc       ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   3d7h\nsc-s3-repo-2qsxfh   ru.yandex.s3.csi        Retain          Immediate              false                  3d7h\n```\nEnsure the storage class you are using has `ALLOWVOLUMEEXPANSION` set to true. If it is false, the storage class does not support volume expansion.\n\n## Deploy a MongoDB ReplicaSet Cluster with StorageClass\n\nKubeBlocks uses a declarative approach to manage MongoDB clusters. Below is an example configuration for deploying a MongoDB cluster with 2 replicas (1 primary, 1 secondary).\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: mongo-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: mongodb\n  topology: replicaset\n  componentSpecs:\n    - name: mongodb\n      serviceVersion: \"6",
    "path": "docs/preview/kubeblocks-for-mongodb/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a MongoDB Cluster  This guide explains how to expand Persistent Volume Claims (PVCs) in a MongoDB cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy MongoDB Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage MongoDB Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing MongoDB services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB ReplicaSet Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## View Network Services\nList the Services created for the MongoDB cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=mongo-cluster -n demo\n```\n\nExample Services:\n```bash\n# service for to all replicas\nmongo-cluster-mongodb              ClusterIP   10.96.249.157           27017/TCP   44m\n# read-write service\nmongo-cluster-mongodb-mongodb      ClusterIP   10.96.17.58             27017/TCP   44m\n# read-only servcie\nmongo-cluster-mongodb-mongodb-ro   ClusterIP   10.96.2.71              27017/TCP   44m\n```\n\n## Expose MongoDB Service\n\nExternal service addresses enable public internet access to MongoDB, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the MongoDB service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: mongo-cluster-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: mongo-cluster\n    expose:\n    - componentName: mongodb\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options ",
    "path": "docs/preview/kubeblocks-for-mongodb/04-operations/05-manage-loadbalancer",
    "description": "    # Manage MongoDB Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing MongoDB services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, mana"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_04-operations_08-switchover",
    "title": "MongoDB Cluster Switchover",
    "content": "\n\n# MongoDB Cluster Switchover\n\nA **switchover** is a planned operation that transfers the primary role from one MongoDB instance to another. Unlike failover which occurs during failures, switchover provides:\n- Controlled role transitions\n- Minimal downtime (typically a few hundred milliseconds)\n- Predictable maintenance windows\n\nSwitchover is ideal for:\n- Node maintenance/upgrades\n- Workload rebalancing\n- Testing high availability\n- Planned infrastructure changes\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Check Roles\nList the Pods and their roles (primary or secondary):\n\n```bash\nkubectl get pods -n demo -l app.kubernetes.io/instance=mongo-cluster,apps.kubeblocks.io/component-name=mongodb -L kubeblocks.io/role\n```\n\nExample Output:\n\n```text\nNAME                      READY   STATUS    RESTARTS   AGE   ROLE\nmongo-cluster-mongodb-0   2/2     Running   0          20m   primary\nmongo-cluster-mongodb-1   2/2     Running   0          21m   secondary\nmongo-cluster-mongodb-2   2/2     Running   0          19m   secondary\n```\n\n## Performing a Planned Switchover\n\nTo initiate a planned switchover, create an OpsRequest resource as shown below:\n\n  Option 1: Automatic Switchover (No preferred candidate)\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: mongodb-switchover-ops\n    namespace: demo\n  spec:\n    clusterName: mongo-cluster\n    type: Switchover\n    switchover:\n    - componentName: mongodb\n      instanceName: mongo-cluster-mongodb-0\n  ```\n **Key Parameters:**\n  - `instanceName`: Specifies the instance (Pod) that is primary or leader before a switchover operation.\n\n  \n  Option 2: Targeted Switchover (Specific candidate)\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: mongodb-switchover-targeted\n    namespace: demo\n  spec:\n    clusterName: mongo-cluster\n    type: Switchover\n    switchover:\n    - componentName: mongodb\n      # Specifi",
    "path": "docs/preview/kubeblocks-for-mongodb/04-operations/08-switchover",
    "description": "  # MongoDB Cluster Switchover  A **switchover** is a planned operation that transfers the primary role from one MongoDB instance to another. Unlike failover which occurs during failures, switchover provides: - Controlled role transitions - Minimal downtime (typically a few hundred milliseconds) - P"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed MongoDB Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed MongoDB Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in MongoDB clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\nTo decommission a specific Pod (e.g., 'mongo-cluster-mongodb-1'), you can use one of the following methods:\n\n\n\n  Option 1: Using OpsRequest\n\n  Create an OpsRequest to mark the Pod as offline:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: mongo-cluster-decommission-ops\n    namespace: demo\n  spec:\n    clusterName: mongo-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: mongodb\n      scaleIn:\n        onlineInstancesToOffline:\n          - 'mongo-cluster-mongodb-1'  # Specifies the instance names that need to be taken offline\n  ```\n\n  #### Monitor the Decommissio",
    "path": "docs/preview/kubeblocks-for-mongodb/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed MongoDB Clusters  This guide explains how to decommission (take offline) specific Pods in MongoDB clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workl"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_05-backup-restore_01-create-backuprepo",
    "title": "Create a Backup Repository for KubeBlocks",
    "content": "\n# Create a BackupRepo for KubeBlocks\n\nThis guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.\n\n## Prerequisites\n- AWS CLI configured with appropriate permissions to create S3 buckets.\n- kubectl access to your Kubernetes cluster.\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks)) and running in the kb-system namespace.\n\n## Step 1: Create S3 Bucket\n\nUse the AWS CLI to create an S3 bucket in your desired region. Replace `` with your target AWS region (e.g., `us-east-1`, `ap-southeast-1`).\n\n```bash\n aws s3api create-bucket --bucket kubeblocks-backup-repo --region  --create-bucket-configuration LocationConstraint=\n```\n\nExample (for us-west-1):\n```bash\naws s3api create-bucket \\\n  --bucket kubeblocks-backup-repo \\\n  --region us-west-1 \\\n  --create-bucket-configuration LocationConstraint=us-west-1\n```\n\nExample Output:\n\n```json\n\n```\n\nVerification:\nConfirm the bucket was created by listing its contents (it will be empty initially):\n\n```bash\naws s3 ls s3://kubeblocks-backup-repo\n```\n\n## Step 2: Create a Kubernetes Secret for AWS Credentials\n\nStore your AWS credentials securely in a Kubernetes Secret. Replace `` and `` with your actual AWS credentials:\n\n```bash\n# Create a secret to save the access key\nkubectl create secret generic s3-credential-for-backuprepo \\\n  --from-literal=accessKeyId= \\\n  --from-literal=secretAccessKey= \\\n  -n kb-system\n```\n\n## Step 3: Configure Backup Repository\n\nA BackupRepo is a custom resource that defines a storage repository for backups. In this step, you'll integrate your S3 bucket with KubeBlocks by creating a BackupRepo resource.\n\nApply the following YAML to create the BackupRepo. Replace fields(e.g., bucket name, region) with your specific settings.\n\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupRepo\nmetadata:\n  name: s3-repo\n  annotations:\n    # mark this backuprepo as default one\n    dataprotection.kubeblocks.io/is",
    "path": "docs/preview/kubeblocks-for-mongodb/05-backup-restore/01-create-backuprepo",
    "description": " # Create a BackupRepo for KubeBlocks  This guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.  ## Prerequisites - AWS CLI configured with appropriate permissions to create S3 buckets. - kubectl access to your Kubernetes cluster. -"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_05-backup-restore_02-create-full-backup",
    "title": "Create a Full Backup for a MongoDB Cluster on KubeBlocks",
    "content": "\n\n\n# Create a Full Backup for MongoDB on KubeBlocks\n\nThis guide demonstrates how to create and validate full backups for MongoDB clusters on KubeBlocks using the `pg-basebackup` method through both:\n- The Backup API (direct backup operations)\n- The OpsRequest API (managed backup operations with enhanced monitoring)\n\nWe will cover how to restore data from a backup in the [Restore From Full Backup](./05-restoring-from-full-backup) guide.\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Backup Prerequisites\n\nBefore creating backups, ensure:\n1. Backup repository is configured:\n   - `BackupRepo` resource exists\n   - Network connectivity between cluster and repository\n   - `BackupRepo` status shows \"Ready\"\n\n2. Cluster is ready:\n   - Cluster status is \"Running\"\n   - No ongoing operations (scaling, upgrades, etc.)\n\n## Identify Backup Configuration\n\nCheck available backup policies and schedules:\n\n```bash\n# List backup policies\nkubectl get backuppolicy -n demo -l app.kubernetes.io/instance=mongo-cluster\n\n# List backup schedules\nkubectl get backupschedule -n demo -l app.kubernetes.io/instance=mongo-cluster\n```\n\nExpected Output:\n```bash\nNAME                                  BACKUP-REPO   STATUS      AGE\nmongo-cluster-mongodb-backup-policy                 Available   62m\n\nNAME                                    STATUS      AGE\nmongo-cluster-mongodb-backup-schedule   Available   62m\n```\n\nView supported backup methods in the BackupPolicy CR 'mongo-cluster-mongodb-backup-policy':\n\n```bash\nkubectl get backuppolicy mongo-cluster-mongodb-backup-policy -n demo -oyaml | yq '.spec.backupMethods[].name'\n```\n**List of Backup methods**\n\nKubeBlocks MongoDB supports these backup methods:\n\n| Feature           | Method          | Description |\n|-------------|--------|------------|\n| Full Backup | dump   | Uses `mongodump`, a MongoDB utility used to create a binary export of the contents of a database  |\n| Full Backup | datafile | Backup the data files of the",
    "path": "docs/preview/kubeblocks-for-mongodb/05-backup-restore/02-create-full-backup",
    "description": "   # Create a Full Backup for MongoDB on KubeBlocks  This guide demonstrates how to create and validate full backups for MongoDB clusters on KubeBlocks using the `pg-basebackup` method through both: - The Backup API (direct backup operations) - The OpsRequest API (managed backup operations with enha"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_05-backup-restore_03-scheduled-full-backup",
    "title": "Setting Up a MongoDB Cluster with Scheduled Backups in KubeBlocks",
    "content": "\n\n# Setting Up a MongoDB Cluster with Scheduled Backups in KubeBlocks\n\nThis guide demonstrates how to deploy a MongoDB cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Prerequisites for Backup\n\n1. Backup Repository Configured:\n   - Configured `BackupRepo`\n   - Network connectivity between cluster and repo, `BackupRepo` status is `Ready`\n\n2. Cluster is Running:\n   - Cluster must be in `Running` state\n   - No ongoing operations (scaling, upgrades etc.)\n\n## Configure Scheduled Backups\n\nKubeBlocks automatically creates a `BackupSchedule` resource when the cluster is created. Follow these steps to enable and configure scheduled backups:\n\n1. Verify the default backup schedule configuration:\n\n```bash\nkubectl get backupschedule mongo-cluster-mongodb-backup-schedule  -n demo -oyaml\n```\n\nExample Output:\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupSchedule\nspec:\n  backupPolicyName: mongo-cluster-MongoDB-backup-policy\n  schedules:\n  - backupMethod: datafile\n    # ┌───────────── minute (0-59)\n    # │ ┌───────────── hour (0-23)\n    # │ │ ┌───────────── day of month (1-31)\n    # │ │ │ ┌───────────── month (1-12)\n    # │ │ │ │ ┌───────────── day of week (0-6) (Sunday=0)\n    # │ │ │ │ │\n    # 0 18 * * *\n    # schedule this job every day at 6:00 PM (18:00).\n    cronExpression: 0 18 * * * # update the cronExpression to your need\n    enabled: true # set to `true` to schedule base backup periodically\n    retentionPeriod: 7d # set the retention period to your need\n```\n\n2. Enable and customize the backup schedule:\n```bash\nkubectl edit backupschedule mongo-cluster-mongodb-backup-schedule -n demo\n```\n\nUpdate these key parameters:\n- `enabled`: Set to `true` to activate scheduled backups\n- `cronExpression`: Configure backup frequency using cron syntax\n- `retentionPeriod`: Set how long to keep backups (e.g., `7d`, `1mo`)\n\nExample configurat",
    "path": "docs/preview/kubeblocks-for-mongodb/05-backup-restore/03-scheduled-full-backup",
    "description": "  # Setting Up a MongoDB Cluster with Scheduled Backups in KubeBlocks  This guide demonstrates how to deploy a MongoDB cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.  ## Prerequisites    ## Deploy a MongoDB Cluster    ## Verifying the Deployment    ## Pr"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_05-backup-restore_04-scheduled-continuous-backup",
    "title": "Setting Up a MongoDB Cluster with Scheduled Continuous Backup in KubeBlocks",
    "content": "\n# Setting Up a MongoDB Cluster with Scheduled Continuous Backup Enabled in KubeBlocks\n\nThis guide demonstrates how to configure a MongoDB cluster on KubeBlocks with:\n\n- Scheduled full backups (base backups)\n- Continuous WAL (Write-Ahead Log) archiving\n- Point-In-Time Recovery (PITR) capabilities\n\nThis combination provides comprehensive data protection with minimal recovery point objectives (RPO).\n\n## What is PITR?\nPoint-In-Time Recovery (PITR) allows you to restore a database to a specific moment in time by combining full backups with continuous binlog/wal/archive log backups.\n\nFor details on restoring data from both full backups and continuous binlog backups, refer to the [Restore From PITR](restore-with-pitr.mdx)  guide.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Prerequisites for Backup\n\n1. Backup Repository Configured:\n   - Configured `BackupRepo`\n   - Network connectivity between cluster and repo, `BackupRepo` status is `Ready`\n\n2. Cluster is Running:\n   - Cluster must be in `Running` state\n   - No ongoing operations (scaling, upgrades etc.)\n\n## List of Backup methods\n\nKubeBlocks MongoDB supports these backup methods:\n\n| Feature           | Method          | Description |\n|-------------|--------|------------|\n| Full Backup | dump   | Uses `mongodump`, a MongoDB utility used to create a binary export of the contents of a database  |\n| Full Backup | datafile | Backup the data files of the database |\n| Continuous Backup | archive-oplog | Continuously archi",
    "path": "docs/preview/kubeblocks-for-mongodb/05-backup-restore/04-scheduled-continuous-backup",
    "description": " # Setting Up a MongoDB Cluster with Scheduled Continuous Backup Enabled in KubeBlocks  This guide demonstrates how to configure a MongoDB cluster on KubeBlocks with:  - Scheduled full backups (base backups) - Continuous WAL (Write-Ahead Log) archiving - Point-In-Time Recovery (PITR) capabilities  T"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_05-backup-restore_05-restoring-from-full-backup",
    "title": "Restore a MongoDB Cluster from Backup",
    "content": "\n\n# Restore a MongoDB Cluster from Backup\n\nThis guide demonstrates two methods to restore a MongoDB cluster from backup in KubeBlocks:\n\n1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations\n2. **OpsRequest API Method** - Enhanced operational control with progress monitoring\n\n## Prerequisites\n\n\n\n## Preparing for Restoration: Locate one Full Backup\nBefore restoring, ensure that there is a full backup available. The restoration process will use this backup to create a new MongoDB cluster.\n\n- Backup repository accessible from new cluster\n- Valid full backup in `Completed` state\n- Adequate CPU/memory resources\n- Sufficient storage capacity\n\nFind available full backups:\n\n```bash\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Full,app.kubernetes.io/instance=mongo-cluster # get the list of full backups\n```\n\nPick ONE of the Backups whose status is `Completed`.\n\n## Option 1: Cluster Annotation Restoration\n\n### Step 1: Create Restored Cluster\nCreate a new cluster with restore configuration:\n\nKey parameters:\n- `kubeblocks.io/restore-from-backup` annotation\n- Backup name and namespace located from the previous steps\n\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: mongo-cluster-restored\n  namespace: demo\n  annotations:\n    # NOTE: replace  with your backup\n    kubeblocks.io/restore-from-backup: '}'\nspec:\n  terminationPolicy: Delete\n  clusterDef: mongodb\n  topology: replicaset\n  componentSpecs:\n    - name: mongodb\n      serviceVersion: \"6.0.16\"\n      replicas: 3\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n\n### Step 2: Monitor Restoration\nTrack restore progress with:\n\n```bas",
    "path": "docs/preview/kubeblocks-for-mongodb/05-backup-restore/05-restoring-from-full-backup",
    "description": "  # Restore a MongoDB Cluster from Backup  This guide demonstrates two methods to restore a MongoDB cluster from backup in KubeBlocks:  1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations 2. **OpsRequest API Method** - Enhanced operational control with progress moni"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_05-backup-restore_06-restore-with-pitr",
    "title": "Restore a MongoDB Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks",
    "content": "\n\n# Restore a MongoDB Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks\n\nThis guide demonstrates how to perform Point-In-Time Recovery (PITR) for MongoDB clusters in KubeBlocks using:\n\n1. A full base backup\n2. Continuous WAL (Write-Ahead Log) backups\n3. Two restoration methods:\n   - Cluster Annotation (declarative approach)\n   - OpsRequest API (operational control)\n\nPITR enables recovery to any moment within the `timeRange` specified.\n\n## Prerequisites\n\n\n\n## Prepare for PITR Restoration\nTo perform a PITR restoration, both a full backup and continuous backup are required. Refer to the documentation to configure these backups if they are not already set up.\n\n- Completed full backup\n- Active continuous WAL backup\n- Backup repository accessible\n- Sufficient resources for new cluster\n\nTo identify the list of full and continuous backups, you may follow the steps:\n\n### 1. Verify Continuous Backup\nConfirm you have a continuous WAL backup, either running or completed:\n\n```bash\n# expect EXACTLY ONE continuous backup per cluster\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Continuous,app.kubernetes.io/instance=mongo-cluster\n```\n\n### 2. Check Backup Time Range\nGet the valid recovery window:\n\n```bash\nkubectl get backup  -n demo -o yaml | yq '.status.timeRange'\n```\n\nExpected Output:\n```text\nstart: \"2025-05-07T09:12:47Z\"\nend: \"2025-05-07T09:22:50Z\"\n```\n\n### 3. Identify Full Backup\nFind available full backups that meet:\n- Status: Completed\n- Completion time after continuous backup start time\n\n```bash\n# expect one or more Full backups\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Full,app.kubernetes.io/instance=mongo-cluster\n```\n\n:::tip\nKubeBlocks automatically selects the most recent qualifying full backup as the base.\nMake sure there is a full backup meets the condition: its `stopTime`/`completionTimestamp` must **AFTER** Continuous backup's `startTime`, otherwise PITR restoration will fail.\n:::\n\n## Option 1: Clu",
    "path": "docs/preview/kubeblocks-for-mongodb/05-backup-restore/06-restore-with-pitr",
    "description": "  # Restore a MongoDB Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks  This guide demonstrates how to perform Point-In-Time Recovery (PITR) for MongoDB clusters in KubeBlocks using:  1. A full base backup 2. Continuous WAL (Write-Ahead Log) backups 3. Two restoration methods:    "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb_06-custom-secret_01-custom-secret",
    "title": "Create a MongoDB Cluster with a Custom Root Password on KubeBlocks",
    "content": "\n# Create MongoDB Cluster With Custom Password on KubeBlocks\n\nThis guide demonstrates how to deploy a MongoDB cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.\n\n## Prerequisites\n\n\n\n## Deploying the MongoDB ReplicaSet Cluster\n\nKubeBlocks uses a declarative approach for managing MongoDB clusters. Below is an example configuration for deploying a MongoDB cluster with 2 nodes (1 primary, 1 replicas) and a custom root password.\n\n### Step 1: Create a Secret for the Defaults Account\n\nThe custom root password is stored in a Kubernetes Secret. Create the Secret by applying the following YAML:\n\n```yaml\napiVersion: v1\ndata:\n  password: Y3VzdG9tcGFzc3dvcmQ= # custompassword\n  username: cm9vdA== #root\nimmutable: true\nkind: Secret\nmetadata:\n  name: custom-secret\n  namespace: demo\n```\n- password: Replace custompassword with your desired password and encode it using Base64 (`echo -n \"custompassword\" | base64`).\n- username: The default MongoDB root user is 'root', encoded as 'cm9vdA=='.\n\n\n### Step 2: Deploy the MongoDB Cluster\n\nApply the following manifest to deploy the MongoDB cluster, referencing the Secret created in Step 1 for the root account:\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: mongo-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: mongodb\n  topology: replicaset\n  componentSpecs:\n    - name: mongodb\n      serviceVersion: \"6.0.16\"\n      replicas: 3\n      systemAccounts:  # override systemaccount password\n        - name: root\n          secretRef:\n            name: custom-secret\n            namespace: demo\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n**Explanation",
    "path": "docs/preview/kubeblocks-for-mongodb/06-custom-secret/01-custom-secret",
    "description": " # Create MongoDB Cluster With Custom Password on KubeBlocks  This guide demonstrates how to deploy a MongoDB cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.  ## Prerequisites    ## Deploying the MongoDB ReplicaSet Cluster  KubeBlocks uses a declarative approach for "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb__tpl__create-cluster",
    "title": "_create-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing MongoDB Replication Clusters.\nBelow is an example configuration for deploying a MongoDB ReplicaSet Cluster with one primary replica and two secondary replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: mongo-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: mongodb\n  topology: replicaset\n  componentSpecs:\n    - name: mongodb\n      serviceVersion: \"6.0.16\"\n      replicas: 3\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n",
    "path": "docs/preview/kubeblocks-for-mongodb/_tpl/_create-cluster",
    "description": "KubeBlocks uses a declarative approach for managing MongoDB Replication Clusters. Below is an example configuration for deploying a MongoDB ReplicaSet Cluster with one primary replica and two secondary replicas.  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/preview/kubeblocks-for-mongodb/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mongodb__tpl__verify-cluster",
    "title": "_verify-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster mongo-cluster -n demo -w\n```\n\nExpected Output:\n\n```bash\nkubectl get cluster mongo-cluster -n demo\nNAME            CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nmongo-cluster   mongodb              Delete               Creating   49s\nmongo-cluster   mongodb              Delete               Running    62s\n```\n\nCheck the pod status and roles:\n```bash\nkubectl get pods -l app.kubernetes.io/instance=mongo-cluster -L  kubeblocks.io/role -n demo\n```\n\nExpected Output:\n```bash\nNAME                      READY   STATUS    RESTARTS   AGE   ROLE\nmongo-cluster-mongodb-0   2/2     Running   0          78s   primary\nmongo-cluster-mongodb-1   2/2     Running   0          63s   secondary\nmongo-cluster-mongodb-2   2/2     Running   0          48s   secondary\n```\n\nOnce the cluster status becomes Running, your MongoDB cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::\n",
    "path": "docs/preview/kubeblocks-for-mongodb/_tpl/_verify-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster mongo-cluster -n demo -w ```  Expected Output:  ```bash kubectl get cluster mongo-cluster -n demo NAME            CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE mongo-cluster   mongodb        "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_03-topologies_01-semisync",
    "title": "Deploying a MySQL Semi-Synchronous Cluster with KubeBlocks",
    "content": "\n# Deploying a MySQL Semi-Synchronous Cluster with KubeBlocks\n\n**Semi-synchronous replication** improves data consistency between the primary and replica nodes by requiring the primary node to wait for acknowledgment from at least one replica before committing transactions. This guide walks you through the process of setting up a MySQL semi-synchronous replication cluster using KubeBlocks.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n  - A Kubernetes cluster is up and running.\n  - The kubectl CLI tool is configured to communicate with your cluster.\n  - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploying the MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode.\n\nApply the following YAML configuration:\n```yaml\nkubectl apply -f - \n\nTopology:\nCOMPONENT   INSTANCE                         ROLE        STATUS    AZ                NODE                                                       CREATED-TIME\nmysql       example-mysql-cluster-mysql-0   primary     Running   ap-southeast-1a   ip-10-0-1-93.ap-southeast-1.compute.internal/10.0.1.93     Dec 24,2024 09:09 UTC+0800\nmysql       example-mysql-cluster-mysql-1   secondary   Running   ap-southeast-1b   ip-10-0-2-253.ap-southeast-1.compute.internal/10.0.2.253   Dec 24,2024 09:09 UTC+0800\n\nResources Allocation:\nCOMPONENT   DEDICATED   CPU(REQUEST/LIMIT)   MEMORY(REQUEST/LIMIT)   STORAGE-SIZE   STORAGE-CLASS\nmysql       false       500m / 500m          512Mi / 512Mi           data:20Gi      \n\nImages:\nCOMPONENT   TYPE   I",
    "path": "docs/preview/kubeblocks-for-mysql/03-topologies/01-semisync",
    "description": " # Deploying a MySQL Semi-Synchronous Cluster with KubeBlocks  **Semi-synchronous replication** improves data consistency between the primary and replica nodes by requiring the primary node to wait for acknowledgment from at least one replica before committing transactions. This guide walks you thro"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_03-topologies_02-semisync-with-proxysql",
    "title": "Deploying a MySQL Semi-Synchronous Cluster with ProxySQL on KubeBlocks",
    "content": "\n# Deploying a MySQL Semi-Synchronous Cluster and ProxySQL with KubeBlocks\n\n**Semi-synchronous replication** enhances data consistency between the primary and replica nodes by ensuring that the primary waits for acknowledgment from at least one replica before committing transactions.\n\n**ProxySQL** is a high-performance MySQL proxy that acts as a middleware between MySQL clients and servers. It provides advanced features such as query routing, load balancing, query caching, and high availability. When combined with a MySQL semi-synchronous cluster, ProxySQL ensures seamless failover and efficient traffic management, resulting in optimal performance and reliability.\n\nThis guide walks you through deploying a MySQL semi-synchronous replication cluster integrated with ProxySQL using KubeBlocks.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n  - A Kubernetes cluster is up and running.\n  - The kubectl CLI tool is configured to communicate with your cluster.\n  - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploying the MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative configuration approach to manage MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 MySQL nodes (1 primary, 1 replica) and 2 ProxySQL instances.\n\nApply the following configuration:\n```yaml\nkubectl apply -f - \nproxysql    example-mysql-cluster-proxysql-proxy-ordinal-0.demo.svc.cluster.local:6032   \n            example-mysql-cluster-proxysql-proxy-ordinal-0.demo.svc.cluster.local:6033\n            example-mysql-cluster-proxysql-proxy-ordinal-1.demo.svc.cluster.local:6032\n            example-mysql-cluster-proxysql-proxy-ordinal-1.",
    "path": "docs/preview/kubeblocks-for-mysql/03-topologies/02-semisync-with-proxysql",
    "description": " # Deploying a MySQL Semi-Synchronous Cluster and ProxySQL with KubeBlocks  **Semi-synchronous replication** enhances data consistency between the primary and replica nodes by ensuring that the primary waits for acknowledgment from at least one replica before committing transactions.  **ProxySQL** i"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_03-topologies_03-mgr",
    "title": "Deploying a MySQL Group Replication Cluster Using KubeBlocks",
    "content": "\n# Deploying a MySQL Group Replication Cluster Using KubeBlocks\n\n**MySQL Group Replication (MGR)** offers high availability and scalability by synchronizing data across multiple MySQL instances. It ensures that all nodes in the cluster participate in replication seamlessly, with automatic failover and self-healing capabilities. This guide walks you through deploying a MySQL Group Replication cluster using **KubeBlocks**, which simplifies the management and deployment of MySQL clusters in Kubernetes.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploying the MySQL Group Replication Cluster\n\nKubeBlocks uses a declarative approach to manage MySQL clusters. Below is an example configuration for deploying a MySQL Group Replication cluster with three nodes.\n\nApply the following YAML configuration to deploy a MySQL Group Replication (MGR) cluster:\n```yaml\nkubectl apply -f - \n\nTopology:\nCOMPONENT   SERVICE-VERSION   INSTANCE                        ROLE        STATUS    AZ                NODE                                                       CREATED-TIME\nmysql       8.0.35            example-mysql-cluster-mysql-0   primary     Running   ap-southeast-1c   ip-10-0-3-155.ap-southeast-1.compute.internal/10.0.3.155   Feb 10,2025 22:23 UTC+0800\nmysql       8.0.35            example-mysql-cluster-mysql-1   secondary   Running   ap-southeast-1c   ip-10-0-3-204.ap-southeast-1.compute.internal/10.0.3.204   Feb 10,2025 22:23 UTC+0800\nmysql       8.0.35            example-mysql",
    "path": "docs/preview/kubeblocks-for-mysql/03-topologies/03-mgr",
    "description": " # Deploying a MySQL Group Replication Cluster Using KubeBlocks  **MySQL Group Replication (MGR)** offers high availability and scalability by synchronizing data across multiple MySQL instances. It ensures that all nodes in the cluster participate in replication seamlessly, with automatic failover a"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_03-topologies_04-mgr-with-proxysql",
    "title": "Deploying a MySQL Group Replication Cluster with ProxySQL Using KubeBlocks",
    "content": "\n# Deploying a MySQL Group Replication Cluster with ProxySQL Using KubeBlocks\n\n**MySQL Group Replication (MGR)** ensures high availability and fault tolerance by synchronizing data across multiple MySQL instances. It provides automatic failover, promoting a secondary node to primary in case of failure, ensuring continuous availability.\n\n**ProxySQL** is a high-performance MySQL proxy that acts as a middleware between MySQL clients and database servers. It provides features such as query routing, load balancing, query caching, and seamless failover. When combined with MGR, ProxySQL enhances cluster performance and enables efficient traffic management.\n\nThis guide explains how to deploy a **MySQL Group Replication (MGR) cluster with ProxySQL integration** using **KubeBlocks**, simplifying the process of managing MySQL clusters in Kubernetes.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploying the MySQL Group Replication Cluster\n\nKubeBlocks uses a declarative configuration approach to simplify MySQL cluster management. Below is an example configuration to deploy a MySQL Group Replication cluster with three MySQL nodes and two ProxySQL instances.\n\nApply the following YAML configuration:\n```yaml\nkubectl apply -f - \nproxysql    example-mysql-cluster-proxysql-proxy-ordinal-0.demo.svc.cluster.local:6032   \n            example-mysql-cluster-proxysql-proxy-ordinal-0.demo.svc.cluster.local:6033\n            example-mysql-cluster-proxysql-proxy-ordinal-1.demo.svc.cluster.",
    "path": "docs/preview/kubeblocks-for-mysql/03-topologies/04-mgr-with-proxysql",
    "description": " # Deploying a MySQL Group Replication Cluster with ProxySQL Using KubeBlocks  **MySQL Group Replication (MGR)** ensures high availability and fault tolerance by synchronizing data across multiple MySQL instances. It provides automatic failover, promoting a secondary node to primary in case of failu"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_03-topologies_05-orchestrator",
    "title": "Deploying a MySQL Cluster and Orchestrator with KubeBlocks",
    "content": "\n# Deploying a MySQL Cluster and Orchestrator with KubeBlocks\n\nSemi-synchronous replication improves data consistency between primary and replica nodes by requiring acknowledgment from at least one replica before committing transactions.\n\nOrchestrator is a robust MySQL High Availability (HA) and failover management tool. It provides automated monitoring, fault detection, and topology management for MySQL clusters, making it an essential component for managing large-scale MySQL deployments. With Orchestrator, you can:\n- **Monitor Replication Topology**: Orchestrator continuously monitors the MySQL replication topology and provides a real-time view of the cluster's state.\n- **Automated Failover**: In case of a primary node failure, Orchestrator automatically promotes a healthy replica to primary, ensuring minimal downtime.\n- **Topology Management**: Orchestrator allows you to reconfigure, rebalance, and recover your MySQL topology with ease.\n\nThis guide walks you through the process of setting up a MySQL semi-synchronous replication cluster using **KubeBlocks**, alongside **Orchestrator** for effective failover and recovery management.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n  - A Kubernetes cluster is up and running.\n  - The kubectl CLI tool is configured to communicate with your cluster.\n  - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Install the Orchestrator Addon\n\n1. View the Addon versions.\n```bash\n# including pre-release versions\nhelm search repo kubeblocks/orchestrator --devel --versions\n```\n\n2. Install the Addon. Specify a version with '--version'.\n```bash\nhelm install kb-addon-orc kubeblocks/orchestrator --namespace ",
    "path": "docs/preview/kubeblocks-for-mysql/03-topologies/05-orchestrator",
    "description": " # Deploying a MySQL Cluster and Orchestrator with KubeBlocks  Semi-synchronous replication improves data consistency between primary and replica nodes by requiring acknowledgment from at least one replica before committing transactions.  Orchestrator is a robust MySQL High Availability (HA) and fai"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_03-topologies_06-orchestrator-with-proxysql",
    "title": "Deploying a MySQL Cluster with Orchestrator and ProxySQL using KubeBlocks",
    "content": "\n# Deploying a MySQL Cluster and Orchestrator and ProxySQL with KubeBlocks\n\nSemi-synchronous replication enhances data consistency between primary and replica nodes by requiring acknowledgment from at least one replica before committing transactions.\n\nThis guide demonstrates how to deploy a MySQL cluster using **KubeBlocks** with **Orchestrator** for high availability and failover management, and **ProxySQL** for advanced query routing and load balancing. Together, these tools create a robust and efficient MySQL cluster infrastructure.\n\n### **What is Orchestrator?**\n\nOrchestrator is a powerful MySQL High Availability (HA) and failover management tool. It automates monitoring, fault detection, and topology management for MySQL clusters, making it ideal for managing large-scale deployments. Key features include:\n\n- **Replication Topology Monitoring**: Provides a real-time view of the MySQL replication topology.\n- **Automated Failover**: Promotes a healthy replica to primary in case of failure, ensuring minimal downtime.\n- **Topology Management**: Simplifies reconfiguration, rebalancing, and recovery of MySQL clusters.\n\n### **What is ProxySQL?**\n\nProxySQL is a high-performance MySQL proxy that acts as a middleware between MySQL clients and database servers. It enhances cluster performance with features such as:\n\n- **Query Routing**: Directs queries to the appropriate servers based on their purpose (e.g., read or write).\n- **Load Balancing**: Distributes traffic across replicas to optimize resource usage.\n- **Query Caching**: Reduces database load by caching frequent queries.\n- **Failover Support**: Seamlessly handles failover scenarios without interrupting application services.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../",
    "path": "docs/preview/kubeblocks-for-mysql/03-topologies/06-orchestrator-with-proxysql",
    "description": " # Deploying a MySQL Cluster and Orchestrator and ProxySQL with KubeBlocks  Semi-synchronous replication enhances data consistency between primary and replica nodes by requiring acknowledgment from at least one replica before committing transactions.  This guide demonstrates how to deploy a MySQL cl"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_04-operations_01-stop_start_restart",
    "title": "Managing MySQL Cluster Lifecycle (Stop, Start, and Restart)",
    "content": "\n\n\n# Managing MySQL Cluster Lifecycle\n\nThis guide demonstrates how to manage the lifecycle of a MySQL cluster in **KubeBlocks**, including stopping, starting, and restarting the cluster. Proper lifecycle management helps optimize resource usage, reduce operational costs, and ensure flexibility in your Kubernetes environment.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\nkubectl apply -f - \n\n\nOption 2: Using the Declarative Cluster API\n\nAlternatively, you may stop the cluster by setting the `spec.componentSpecs.stop` field to `true`  in the cluster configuration:\n\n```bash\nkubectl patch cluster example-mysql-cluster -n demo --type='json' -p='[\n  \n]'\n```\n\n    \n\n    \n\n### Verifying Cluster Stop\nMonitor the cluster's status to ensure it transitions to the Stopped state:\n```bash\nkubectl get cluster -n demo -w\n```\nExample Output:\n```bash\nNAME                       CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nexample-mysql-cluster   mysql                Delete               Stopping   93s\nexample-mysql-cluster   mysql                Delete               Stopped    101s\n```\n\nThere is no Pods running in the cluster, but the persistent storage is retained.\n```bash\nkubectl get pods -n demo\n```\nExpecte",
    "path": "docs/preview/kubeblocks-for-mysql/04-operations/01-stop_start_restart",
    "description": "   # Managing MySQL Cluster Lifecycle  This guide demonstrates how to manage the lifecycle of a MySQL cluster in **KubeBlocks**, including stopping, starting, and restarting the cluster. Proper lifecycle management helps optimize resource usage, reduce operational costs, and ensure flexibility in yo"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a MySQL Cluster",
    "content": "\n\n\n\n# Vertical Scaling in a MySQL Cluster\n\nThis guide explains how to perform **vertical scaling** in a MySQL cluster managed by KubeBlocks.\nVertical scaling adjusts the resource limits and requests (such as CPU and memory) allocated to the cluster components, allowing for better performance or resource optimization.\n\n## What is Vertical Scaling?\nVertical scaling involves increasing or decreasing the resources (e.g., CPU and memory) allocated to a running database cluster.\nUnlike horizontal scaling, which adjusts the number of replicas, vertical scaling focuses on scaling the capacity of individual Pods.\n\nResources that can be scaled include:\n- CPU cores: Processing power for the database.\n- Memory (RAM): Memory available for database operations.\n\nKubeBlocks ensures seamless vertical scaling by carefully orchestrating Pod restarts to minimize downtime. For example:\n- Secondary Pods are recreated first.\n- Primary Pods are updated last to maintain cluster availability.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node semi-sync MySQL cluster (1 primary, 1 secondary):\n\n```yaml\nkubectl apply -f - \n\nOption 2: Direct Cluster API Update\n\nAlternatively, you may update `spec.componentSpecs.resources` field to the desired resources for vertical scale.\n\n```yaml\nkubectl apply -f - \n\n    \n\n## Verification\nVerify the updated resources by inspecting the cluster configuration or Pod details:\n```bash\nkbcli cluster describe exa",
    "path": "docs/preview/kubeblocks-for-mysql/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling in a MySQL Cluster  This guide explains how to perform **vertical scaling** in a MySQL cluster managed by KubeBlocks. Vertical scaling adjusts the resource limits and requests (such as CPU and memory) allocated to the cluster components, allowing for better performance or reso"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of MySQL Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for MySQL Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a MySQL cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node MySQL cluster (1 primary, 1 replica) with semi-synchronous replication:\n\n```yaml\nkubectl apply -f - \n\n\n### Option 2.: Direct Cluster API Update\n\nAlternatively, you can perform a direct update to the `replicas` field in the Cluster resource:\n```yaml\nkubectl patch cluster example-mysql-cluster -n demo --type=json -p='[]'\n```\n\n    \n\n    \n\n\n### Verify Scale-Out\n\nAfter applying the operation, you will see a new pod created and the MySQL cluster status goes from `Updating` to `Running`, and the newly created pod has a new role `secondary`.\n\n```bash\nkubectl get pods -n demo -l app.kubernetes.io/instance=example-mysql-cluster\n```\n\nExample Output (3 Pods):\n```bash\nNAME                           READY   STATUS    RESTARTS   AGE\nexample-mysql-cluster-mysql-0   4/4     Running   0          4m30s\nexample-mysql-cluster-mysql-1   4/4     Running   0          4m30s\nexample-mysql-cluster-mysql-2   4/4     Running   0          49s\n```\n\nNew replicas automatically join as secondary nodes.\n```bash\nkubectl get pods -n demo -l app.kubernetes.io/instance=example-mysql-cluster -o jsonpath=''\n```\nExample Output:\n`",
    "path": "docs/preview/kubeblocks-for-mysql/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for MySQL Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a MySQL cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequisites  Bef"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a MySQL Cluster",
    "content": "\n\n\n\n# Expanding Volume in a MySQL Cluster\n\nThis guide explains how to expand the Persistent Volume Claims (PVCs) in a MySQL cluster managed by **KubeBlocks**. Volume expansion allows you to increase storage capacity dynamically, ensuring your database can scale seamlessly as data grows. If supported by the underlying storage class, this operation can be performed without downtime.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node semi-sync MySQL cluster (1 primary, 1 secondary):\n\n```yaml\nkubectl apply -f - \n\n\nOption 2: Direct Cluster API Update\n\nAlternatively, you may update the `spec.componentSpecs.volumeClaimTemplates.spec.resources.requests.storage` field to the desired size.\n\n```yaml\nkubectl apply -f - \n\n    \n\n## Verification\n\nUse the following command to inspect the updated cluster configuration:\n```bash\nkbcli cluster describe example-mysql-cluster -n demo\n```\nExpected Output:\n```bash\nResources Allocation:\nCOMPONENT   INSTANCE-TEMPLATE   CPU(REQUEST/LIMIT)   MEMORY(REQUEST/LIMIT)   STORAGE-SIZE   STORAGE-CLASS\nmysql                           500m / 500m          512Mi / 512Mi           data:30Gi      \n```\nThe volume size for the data PVC has been updated to the specified value (e.g., 30Gi in this case).\n\nCheck the status of the PVCs in the cluster to confirm that the resize operation has completed:\n```bash\nkubectl get pvc -l app.kubernetes.io/instance=example-mysql-cluster -n demo\n```\nExpected Output:\n```",
    "path": "docs/preview/kubeblocks-for-mysql/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a MySQL Cluster  This guide explains how to expand the Persistent Volume Claims (PVCs) in a MySQL cluster managed by **KubeBlocks**. Volume expansion allows you to increase storage capacity dynamically, ensuring your database can scale seamlessly as data grows. If supported"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy MySQL Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage MySQL Service Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions on how to expose a MySQL service managed by KubeBlocks, either externally or internally. You will learn how to configure external access using a cloud provider's LoadBalancer service, manage internal services, and correctly disable external exposure when no longer needed.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode.\n\nCluster Configuration\n```yaml\nkubectl apply -f -                                                                                3306/TCP                                                5m16s\nexample-mysql-cluster-mysql-headless   ClusterIP      None                                                                                            3306/TCP,3601/TCP,9104/TCP,3501/TCP,3502/TCP,9901/TCP   5m16s\n```\n\n## Expose MySQL Service Externally or Internally\n\nExternal addresses allow public internet access to the MySQL service, while internal addresses restrict access to the user’s VPC.\n\n### Service Types Comparison\n\n| Type | \tUse Case\t| Cloud Cost |\tSecurity |\n|----|---|----|---|\n| ClusterIP |\tInternal service communication |\tFree |\tHighest|\n| NodePort |\tDevelopment/testing\t| Low |\tModerate |\n| LoadBalancer\t| Production external access |\tHigh\t| ",
    "path": "docs/preview/kubeblocks-for-mysql/04-operations/05-manage-loadbalancer",
    "description": "    # Manage MySQL Service Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions on how to expose a MySQL service managed by KubeBlocks, either externally or internally. You will learn how to configure external access using a cloud provider's LoadBalancer ser"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_04-operations_06-minior-version-upgrade",
    "title": "Upgrading the Minor Version of a MySQL Cluster in KubeBlocks",
    "content": "\n# Upgrading the Minor Version of a MySQL Cluster in KubeBlocks\n\nThis guide walks you through the deployment and minor version upgrade of a MySQL cluster managed by KubeBlocks, ensuring minimal downtime during the process.\n\nTo minimize the impact on database availability, the upgrade process starts with the replicas (secondary instances). Once the replicas are upgraded, a switchover operation promotes one of the upgraded replicas to primary. The switchover process is very fast, typically completing in a few hundred milliseconds. After the switchover, the original primary instance is upgraded, ensuring minimal disruption to the application.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node semi-sync MySQL cluster (1 primary, 1 secondary):\n\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: Delete\n  componentSpecs:\n    - name: mysql\n      serviceVersion: 8.0.35\n      replicas: 2\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\nEOF\n```\n\n## Ver",
    "path": "docs/preview/kubeblocks-for-mysql/04-operations/06-minior-version-upgrade",
    "description": " # Upgrading the Minor Version of a MySQL Cluster in KubeBlocks  This guide walks you through the deployment and minor version upgrade of a MySQL cluster managed by KubeBlocks, ensuring minimal downtime during the process.  To minimize the impact on database availability, the upgrade process starts "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_04-operations_07-modify-parameters",
    "title": "Modify MySQL Parameters",
    "content": "\n# Modify MySQL Parameters\n\nReconfiguring a database involves modifying database parameters, settings, or configurations to improve performance, security, or availability. These changes can be categorized as:\n- Dynamic: Changes applied without requiring a database restart.\n- Static: Changes that require a database restart to take effect.\n\nEven for static parameters, **KubeBlocks** ensures minimal downtime. It modifies and restarts the replica nodes first, then performs a **switchover** to promote the updated replica as the primary node (a process typically completed within a few milliseconds). Finally, it restarts the original primary node.\n\nThis guide demonstrates how to modify both dynamic and static parameters of a MySQL cluster managed by KubeBlocks using a Reconfiguring OpsRequest.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploying the MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode.\n\nDeploy the cluster using the following YAML manifest:\n\n```yaml\nkubectl apply -f -  SHOW VARIABLES LIKE 'max_connections';\n+-----------------+-------+\n| Variable_name   | Value |\n+-----------------+-------+\n| max_connections | 83    |\n+-----------------+-------+\n1 row in set (0.00 sec)\n\nmysql> SHOW VARIABLES LIKE 'performance_schema';\n+--------------------+-------+\n| Variable_name      | Value |\n+-",
    "path": "docs/preview/kubeblocks-for-mysql/04-operations/07-modify-parameters",
    "description": " # Modify MySQL Parameters  Reconfiguring a database involves modifying database parameters, settings, or configurations to improve performance, security, or availability. These changes can be categorized as: - Dynamic: Changes applied without requiring a database restart. - Static: Changes that req"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_04-operations_08-switchover",
    "title": "Planned Switchover in a MySQL Cluster",
    "content": "\n# Planned Switchover in a MySQL Cluster\n\nA **switchover** is a planned operation where the primary instance in a MySQL cluster proactively transfers its role to a secondary instance. Unlike an unplanned failover, which occurs during unexpected failures, a switchover ensures a controlled and predictable role transition with minimal service disruption.\n\n## **Benefits of Switchover**\n1. **Minimal Downtime**: The primary instance actively transfers its role to the secondary instance, resulting in very short service downtime (typically a few hundred milliseconds)\n2. **Controlled Transition**: Ensures a seamless and predictable role change compared to failover, which involves detecting and recovering from a failure, often causing longer delays (several seconds or more).\n3. **Maintenance-Friendly**: Ideal for planned maintenance tasks, such as node upgrades or decommissioning, while ensuring uninterrupted service.\n\n## **Switchover vs. Failover**\n\n| **Aspect**                  | **Switchover**                            | **Failover**                         |\n|-----------------------------|-------------------------------------------|---------------------------------------|\n| **Initiation**              | Planned and manually triggered            | Unplanned and automatically triggered|\n| **Downtime**                | Few hundred milliseconds                  | Several seconds or more               |\n| **Primary Role Transition** | Proactively transferred                   | Reactively promoted                   |\n| **Use Case**                | Planned maintenance (e.g., upgrades)      | Handling unexpected failures          |\n\nUsing a switchover ensures smooth transitions and minimal service disruption, making it the preferred choice for planned maintenance activities.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n  - A Kubernetes cluster is up and running.\n  - The kubectl CLI tool is configured to communicate with your cluster.\n  - [K",
    "path": "docs/preview/kubeblocks-for-mysql/04-operations/08-switchover",
    "description": " # Planned Switchover in a MySQL Cluster  A **switchover** is a planned operation where the primary instance in a MySQL cluster proactively transfers its role to a secondary instance. Unlike an unplanned failover, which occurs during unexpected failures, a switchover ensures a controlled and predict"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed MySQL Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in a KubeBlocks-Managed MySQL Clusters\n\nThis guide explains how to decommission (take offline) a specific Pod in a MySQL cluster managed by KubeBlocks. Decommissioning a Pod allows precise control over cluster resources without disrupting the cluster's overall functionality. This is particularly useful for workload rebalancing, node maintenance, or addressing specific failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 3-node MySQL semi-synchronous cluster (1 primary, 2 replicas):\n\n```yaml\nkubectl apply -f - \n\n\n### Option 2.: Using Cluster API\nAlternatively, update the Cluster resource directly to decommission the Pod:\n\n```yaml\nkubectl apply -f - \n\n    \n\n### Verify the D",
    "path": "docs/preview/kubeblocks-for-mysql/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in a KubeBlocks-Managed MySQL Clusters  This guide explains how to decommission (take offline) a specific Pod in a MySQL cluster managed by KubeBlocks. Decommissioning a Pod allows precise control over cluster resources without disrupting the cluster's overall funct"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_04-operations_11-rebuild-replica",
    "title": "Recovering MySQL Replica in KubeBlocks",
    "content": "\n# Recovering MySQL Replica in KubeBlocks\n\nThis guide demonstrates how to perform the following tasks in a MySQL semi-synchronous cluster managed by KubeBlocks:\n- Write a record to the primary instance and verify replication on the replica.\n- Stop HA, break replication, modify data on the replica, and remove replication.\n- Rebuild the replica using both 'in-place' repair and 'non-in-place' repair methods.\n- Verify data recovery on the replica.\n\n> **Note**: Above steps are intended for testing purpose only. Disabling HA, breaking replication, and modifying data on a replica can compromise database consistency. Do not perform these operations on a production database.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode.\n\nCluster Configuration\n```yaml\nkubectl apply -f -  CREATE DATABASE test;\nmysql> USE test;\nmysql> CREATE TABLE t1 (id INT PRIMARY KEY, name VARCHAR(255));\nmysql> INSERT INTO t1 VALUES (1, 'John Doe');\n```\n\n### Step 3: Verify Data Replication\nConnect to the replica instance (example-mysql-cluster-mysql-0) to verify that the data has been replicated:\n```bash\nkubectl exec -ti -n demo example-mysql-cluster-mysql-0 -- mysql  -uroot -pR0z5Z1DS02\n```\nNote: If the primary instance is 'example-mysql-cluster-mysql-0', you should conne",
    "path": "docs/preview/kubeblocks-for-mysql/04-operations/11-rebuild-replica",
    "description": " # Recovering MySQL Replica in KubeBlocks  This guide demonstrates how to perform the following tasks in a MySQL semi-synchronous cluster managed by KubeBlocks: - Write a record to the primary instance and verify replication on the replica. - Stop HA, break replication, modify data on the replica, a"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_05-backup-restore_01-create-backuprepo",
    "title": "Create a Backup Repository for KubeBlocks",
    "content": "\n# Create a BackupRepo for KubeBlocks\n\nThis guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.\n\n## Prerequisites\n- AWS CLI configured with appropriate permissions to create S3 buckets.\n- kubectl access to your Kubernetes cluster.\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks)) and running.\n\n## Step 1: Create S3 Bucket\n\nUse the AWS CLI to create an S3 bucket in your desired region. Replace `` with your target AWS region (e.g., `us-east-1`, `ap-southeast-1`).\n\n```bash\naws s3api create-bucket --bucket kubeblocks-backup-repo --region  --create-bucket-configuration LocationConstraint=\n```\n\nExample (for us-west-1):\n```bash\naws s3api create-bucket \\\n  --bucket kubeblocks-backup-repo \\\n  --region us-west-1 \\\n  --create-bucket-configuration LocationConstraint=us-west-1\n```\n\nExample Output:\n\n```json\n\n```\n\nVerification:\nConfirm the bucket was created by listing its contents (it will be empty initially):\n\n```bash\naws s3 ls s3://kubeblocks-backup-repo\n```\n\n## Step 2: Create a Kubernetes Secret for AWS Credentials\n\nStore your AWS credentials securely in a Kubernetes Secret. Replace `` and `` with your actual AWS credentials:\n\n```bash\n# Create a secret to save the access key\nkubectl create secret generic s3-credential-for-backuprepo \\\n  --from-literal=accessKeyId= \\\n  --from-literal=secretAccessKey= \\\n  -n kb-system\n```\n\n## Step 3: Configure Backup Repository\n\nA BackupRepo is a custom resource that defines a storage repository for backups. In this step, you'll integrate your S3 bucket with KubeBlocks by creating a BackupRepo resource.\n\nApply the following YAML to create the BackupRepo. Replace fields(e.g., bucket name, region) with your specific settings.\n\n```yaml\nkubectl apply -f - <<EOF\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupRepo\nmetadata:\n  name: s3-repo\n  annotations:\n    dataprotection.kubeblocks.io/is-default-repo: 'true'\nspec:\n  # Currently, Ku",
    "path": "docs/preview/kubeblocks-for-mysql/05-backup-restore/01-create-backuprepo",
    "description": " # Create a BackupRepo for KubeBlocks  This guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.  ## Prerequisites - AWS CLI configured with appropriate permissions to create S3 buckets. - kubectl access to your Kubernetes cluster. -"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_05-backup-restore_02-create-full-backup",
    "title": "Create a Full Backup for a MySQL Cluster on KubeBlocks",
    "content": "\n\n\n# Create a Full Backup for a MySQL Cluster on KubeBlocks\n\nThis guide demonstrates how to create and validate full backups for a MySQL cluster deployed on KubeBlocks using XtraBackup through both Backup API and Ops API.\nThe Ops API is essentially a wrapper around the Backup API, offering enhanced control and progress monitoring capabilities for backup operations.\n\nWe will cover how to restore data from a backup in the [Restore From Full Backup](restoring-from-full-backup.md) guide.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode.\n\nCluster Configuration\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: WipeOut\n  componentSpecs:\n    - name: mysql\n      serviceVersion: 8.0.35\n      replicas: 2\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\nEOF\n```\n**Key Notes:**\n",
    "path": "docs/preview/kubeblocks-for-mysql/05-backup-restore/02-create-full-backup",
    "description": "   # Create a Full Backup for a MySQL Cluster on KubeBlocks  This guide demonstrates how to create and validate full backups for a MySQL cluster deployed on KubeBlocks using XtraBackup through both Backup API and Ops API. The Ops API is essentially a wrapper around the Backup API, offering enhanced "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_05-backup-restore_03-scheduled-full-backup",
    "title": "Setting Up a MySQL Cluster with Scheduled Backups in KubeBlocks",
    "content": "\n\n# Setting Up a MySQL Cluster with Scheduled Backups in KubeBlocks\n\nThis guide demonstrates how to deploy a MySQL cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode and scheduled backups.\n\nCluster Configuration\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: WipeOut\n  componentSpecs:\n    - name: mysql\n      serviceVersion: 8.0.35\n      replicas: 2\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n  backup:\n    enabled: true\n    retentionPeriod: 30d\n    method: xtrabackup\n    cronExpression: '0 0 * * *'\n    repoName: s3-repo\nEOF\n```\n\n**Explanation of Key Fields**\n- `terminationPolicy: WipeOut`:\n  - When set to 'WipeOut', deleting the cluster also deletes all associated data,",
    "path": "docs/preview/kubeblocks-for-mysql/05-backup-restore/03-scheduled-full-backup",
    "description": "  # Setting Up a MySQL Cluster with Scheduled Backups in KubeBlocks  This guide demonstrates how to deploy a MySQL cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.  ## Prerequisites  Before proceeding, ensure the following: - Environment Setup:     - A Kub"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_05-backup-restore_04-scheduled-continuous-backup",
    "title": "Setting Up a MySQL Cluster with Scheduled Continuous Backup in KubeBlocks",
    "content": "\n# Setting Up a MySQL Cluster with Scheduled Continuous Backup Enabled in KubeBlocks\n\nThis guide explains how to deploy a MySQL cluster on KubeBlocks with scheduled full backups and continuous binlog backups, enabling Point-In-Time Recovery (PITR) for enhanced data protection and recovery capabilities.\n\n## What is PITR?\nPoint-In-Time Recovery (PITR) allows you to restore a database to a specific moment in time by combining full backups with incremental binlog backups.\n\nFor details on restoring data from both full backups and continuous binlog backups, refer to the [Restore From PITR](restore-with-pitr.mdx)  guide.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode, along with scheduled backups (both full backup and continuous backup).\n\nCluster Configuration\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: WipeOut\n  componentSpecs:\n    - name: mysql\n      serviceVersion: 8.0.35\n      replicas: 2\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n",
    "path": "docs/preview/kubeblocks-for-mysql/05-backup-restore/04-scheduled-continuous-backup",
    "description": " # Setting Up a MySQL Cluster with Scheduled Continuous Backup Enabled in KubeBlocks  This guide explains how to deploy a MySQL cluster on KubeBlocks with scheduled full backups and continuous binlog backups, enabling Point-In-Time Recovery (PITR) for enhanced data protection and recovery capabiliti"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_05-backup-restore_05-restoring-from-full-backup",
    "title": "Restore a MySQL Cluster from Backup",
    "content": "\n\n# Restore a MySQL Cluster from Backup\n\nThis guide explains how to restore a new MySQL cluster from an existing backup in KubeBlocks. It showcases two methods: using **Cluster Annotation** or the **Ops API**.\n\nThe **Ops API** provides enhanced control and progress monitoring capabilities for restore operations, acting as a wrapper around the Cluster Annotation.\n\n\n## Prerequisites\n- KubeBlocks Environment:\n  - KubeBlocks operator and required CRDs installed.\n  - kubectl configured to access your Kubernetes cluster.\n- Existing Backup:\n  - A valid backup named example-mysql-backup-backup in the 'demo' namespace.\n\n## Verify Backup Status\n\nBefore restoring, ensure that a full backup is available. The restoration process will use this backup to create a new MySQL cluster.\n\nRun the following command to check the backup status:\n```bash\nkubectl get backup -n demo\n```\n\nExpected Output:\n```bash\nNAME                           POLICY                                      METHOD       REPO      STATUS      TOTAL-SIZE   DURATION   DELETION-POLICY   CREATION-TIME          COMPLETION-TIME        EXPIRATION-TIME\nexample-mysql-cluster-backup   example-mysql-cluster-mysql-backup-policy   xtrabackup   s3-repo   Completed   1633717      18s        Delete            2025-03-07T03:25:22Z   2025-03-07T03:25:40Z\n```\n\n## Restore the Backup Using Cluster Annotation\n\n### Create Restored Cluster\n\nCreate a new cluster with restoration configuration referencing the backup.\n\n```yaml\nkubectl apply -f -    0/1     Init:0/1   0          6s\nrestore-preparedata-XXXXX-   1/1     Running    0          12s\nrestore-preparedata-XXXXX-   0/1     Completed 0          20s\n```\nThese pods copy backup data to Persistent Volumes (PVCs).\n\n\n2. MySQL Cluster Pods:\n```bash\nexample-mysql-cluster-restored-mysql-0     0/4     Pending        0          0s\nexample-mysql-cluster-restored-mysql-0     4/4     Running        0          20s\n```\nPods initialize with restored data and start MySQL services.\n\n## Perform Restoration ",
    "path": "docs/preview/kubeblocks-for-mysql/05-backup-restore/05-restoring-from-full-backup",
    "description": "  # Restore a MySQL Cluster from Backup  This guide explains how to restore a new MySQL cluster from an existing backup in KubeBlocks. It showcases two methods: using **Cluster Annotation** or the **Ops API**.  The **Ops API** provides enhanced control and progress monitoring capabilities for restor"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_05-backup-restore_06-restore-with-pitr",
    "title": "Restore a MySQL Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks",
    "content": "\n\n# Restore a MySQL Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks\n\nThis guide provides a step-by-step walk-through for restoring a MySQL cluster from an existing full backup in KubeBlocks, along with continuous binlog backups for Point-In-Time Recovery (PITR).\n\n## Prerequisites\nBefore proceeding, ensure the following:\n- Environment Setup:\n  - A Kubernetes cluster is up and running.\n  - The kubectl CLI tool is configured to communicate with your cluster.\n  - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Check Existing Backups\nTo perform a PITR restoration, both a full backup and continuous backup are required. Refer to the documentation to configure these backups if they are not already set up.\n\nList available backups with the following command:\n```bash\nkubectl get backup -n demo\n```\n\nExpected Output:\n```bash\nNAME                                               POLICY                                      METHOD           REPO      STATUS      TOTAL-SIZE   DURATION   DELETION-POLICY   CREATION-TIME          COMPLETION-TIME        EXPIRATION-TIME\n77a788fa-example-mysql-cluster-archive-binlog      example-mysql-cluster-mysql-backup-policy   archive-binlog   s3-repo   Running     2110030                 Delete            2025-03-04T02:28:55Z                          2025-04-03T02:28:55Z\nexample-mysql-cluster-xtrabackup-20250305000008    example-mysql-cluster-mysql-backup-policy   xtrabackup       s3-repo   Completed   3102161      18s        Delete            2025-03-05T00:00:11Z   2025-03-05T00:00:29Z   2025-04-04T00:00:29Z\n```\n- '77a788fa-example-mysql-cluster-archive-binlog': Continuous backup (binlog archive).\n- 'example-mysql-cluster-xtrabackup-2",
    "path": "docs/preview/kubeblocks-for-mysql/05-backup-restore/06-restore-with-pitr",
    "description": "  # Restore a MySQL Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks  This guide provides a step-by-step walk-through for restoring a MySQL cluster from an existing full backup in KubeBlocks, along with continuous binlog backups for Point-In-Time Recovery (PITR).  ## Prerequisites"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_06-custom-secret_01-custom-secret",
    "title": "Create a MySQL Cluster with a Custom Root Password on KubeBlocks",
    "content": "\n# Create MySQL Cluster With Custom Password on KubeBlocks\n\nThis guide demonstrates how to deploy a MySQL cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\n```\nExpected Output:\n```bash\nnamespace/demo created\n```\n\n## Deploying the MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode and a custom root password.\n\n### Step 1: Create a Secret for the Root Account\n\nThe custom root password is stored in a Kubernetes Secret. Create the Secret by applying the following YAML:\n\n```yaml\nkubectl apply -f - <<EOF\napiVersion: v1\ndata:\n  password: Y3VzdG9tcGFzc3dvcmQ= # custompassword\n  username: cm9vdA== #root\nimmutable: true\nkind: Secret\nmetadata:\n  name: custom-mysql-root-secret\n  namespace: demo\nEOF\n```\n- password: Replace custompassword with your desired password and encode it using Base64 (`echo -n \"custompassword\" | base64`).\n- username: The default MySQL root user is 'root', encoded as 'cm9vdA=='.\n\n\n### Step 2: Deploy the MySQL Cluster\n\nApply the following manifest to deploy the MySQL cluster, referencing the Secret created in Step 1 for the root account:\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPoli",
    "path": "docs/preview/kubeblocks-for-mysql/06-custom-secret/01-custom-secret",
    "description": " # Create MySQL Cluster With Custom Password on KubeBlocks  This guide demonstrates how to deploy a MySQL cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.  ## Prerequisites  Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_06-custom-secret_02-custom-password-generation-policy",
    "title": "Deploy a MySQL Cluster with a Custom Password Generation Policy on KubeBlocks",
    "content": "\n# Create a MySQL Cluster With Custom Password Generation Policy on KubeBlocks\nThis guide explains how to deploy a MySQL cluster in KubeBlocks with a custom password generation policy for the root user. By defining specific password rules, you can ensure strong, secure credentials for your cluster.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n  - A Kubernetes cluster is up and running.\n  - The kubectl CLI tool is configured to communicate with your cluster.\n  - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploying the MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode and a custom root password that adheres to a specific pattern.\n\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: Delete\n  componentSpecs:\n    - name: mysql\n      serviceVersion: 8.0.35\n      replicas: 2\n      systemAccounts:\n        - name: root\n          passwordConfig:\n            length: 20           # Password length: 20 characters\n            numDigits: 4         # At least 4 digits\n            numSymbols: 4        # At least 4 symbols\n            letterCase: MixedCases # Uppercase and lowercase letters\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassNam",
    "path": "docs/preview/kubeblocks-for-mysql/06-custom-secret/02-custom-password-generation-policy",
    "description": " # Create a MySQL Cluster With Custom Password Generation Policy on KubeBlocks This guide explains how to deploy a MySQL cluster in KubeBlocks with a custom password generation policy for the root user. By defining specific password rules, you can ensure strong, secure credentials for your cluster. "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_07-tls_01-tls-overview",
    "title": "Deploying a MySQL Cluster with TLS on KubeBlocks",
    "content": "\n# Deploying a MySQL Cluster with TLS on KubeBlocks\n\nThis guide demonstrates how to deploy a MySQL cluster with **TLS encryption** using KubeBlocks. TLS ensures secure communication between the MySQL client and server by encrypting data in transit, protecting sensitive information. You will learn how to deploy the cluster, connect securely using TLS, and clean up resources after testing.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploying the MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode with TLS enabled.\n\nApply the following YAML configuration:\n```yaml\nkubectl apply -f -  STATUS;\n--------------\n\nSSL:\t\t\tCipher in use is TLS_AES_256_GCM_SHA384\n```\nIf the SSL field displays a cipher, the connection is successfully encrypted using TLS.\n\n## Cleanup\nTo remove all resources created in this tutorial, run the following commands:\n```bash\nkubectl delete cluster example-mysql-cluster -n demo\nkubectl delete ns demo\n```\n\n## Summary\nIn this guide, you learned how to:\n- Deploy a MySQL cluster using KubeBlocks and enable TLS encryption for secure communication between the MySQL client and server.\n- Establish a secure MySQL connection with TLS.\n- Verify the secure connection using the MySQL shell.\n\nTLS encryption ensures secure communication by encrypting data in transit and protecting sensitive in",
    "path": "docs/preview/kubeblocks-for-mysql/07-tls/01-tls-overview",
    "description": " # Deploying a MySQL Cluster with TLS on KubeBlocks  This guide demonstrates how to deploy a MySQL cluster with **TLS encryption** using KubeBlocks. TLS ensures secure communication between the MySQL client and server by encrypting data in transit, protecting sensitive information. You will learn ho"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_07-tls_02-tls-custom-cert",
    "title": "Deploy a MySQL Cluster with User-Provided TLS on KubeBlocks",
    "content": "\n# Deploy a MySQL Cluster with User-Provided TLS on KubeBlocks\n\nThis guide explains how to deploy a MySQL cluster with **user-provided TLS certificates** using KubeBlocks. By supplying your own certificates, you have full control over the security configuration for encrypted communication between the MySQL client and server. This guide covers generating certificates, deploying the cluster, and verifying the secure connection.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Generating Certificates\n\nTo enable TLS encryption, you will need to provide a Certificate Authority (CA), a server certificate, and a private key. Follow these steps to generate these using OpenSSL:\n\n1. Generate the Root Certificate (CA)\n```bash\n# Create the CA private key (password optional)\nopenssl genrsa -aes256 -out ca-key.pem 4096\n\n# Generate a self-signed root certificate (valid for 10 years)\nopenssl req -x509 -new -nodes -key ca-key.pem -sha256 -days 3650 -out ca.pem\n# Enter the required information (e.g., Common Name can be \"MySQL Root CA\")\n```\n\n2. Generate the Server Certificate & Key\n```bash\n# Generate the server private key\nopenssl genrsa -out server-key.pem 4096\n\n# Create a Certificate Signing Request (CSR)\nopenssl req -new -key server-key.pem -out server-req.pem\n# Enter server identification details, such as:\n# Common Name (CN) = Server domain name or IP (must match the MySQL server address!)\n\n# Sign the server certificate with the CA (valid for 10 years)\nopenssl x509 -req -in serve",
    "path": "docs/preview/kubeblocks-for-mysql/07-tls/02-tls-custom-cert",
    "description": " # Deploy a MySQL Cluster with User-Provided TLS on KubeBlocks  This guide explains how to deploy a MySQL cluster with **user-provided TLS certificates** using KubeBlocks. By supplying your own certificates, you have full control over the security configuration for encrypted communication between th"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_07-tls_03-mtls",
    "title": "Deploy a MySQL Cluster with mTLS on KubeBlocks",
    "content": "\n# Create a MySQL Cluster With mTLS on KubeBlocks\n\nThis guide explains how to configure a MySQL cluster with **mutual TLS (mTLS)** encryption using KubeBlocks. mTLS ensures both the server and client authenticate each other during a connection, providing enhanced security for your database infrastructure. This guide covers certificate generation, cluster deployment, user configuration for mTLS, and secure connection verification.\n\n\n## What is mTLS?\nMutual TLS (mTLS) is an enhanced security protocol that ensures both the server and the client authenticate each other during a connection. Unlike traditional TLS, where only the client verifies the server's identity, mTLS adds an extra layer of security by requiring both sides to present valid certificates issued by a trusted Certificate Authority (CA).\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Generating Certificates\n\nTo enable TLS encryption, you will need to provide a Certificate Authority (CA), a server certificate, and a private key. Follow these steps to generate these using OpenSSL:\n\n1. Generate the Root Certificate (CA)\n```bash\n# Create the CA private key (password optional)\nopenssl genrsa -aes256 -out ca-key.pem 4096\n\n# Generate a self-signed root certificate (valid for 10 years)\nopenssl req -x509 -new -nodes -key ca-key.pem -sha256 -days 3650 -out ca.pem\n# Enter the required information (e.g., Common Name can be \"MySQL Root CA\")\n```\n\n2. Generate the Server Certificate & Key\n```bash\n# Generate the server",
    "path": "docs/preview/kubeblocks-for-mysql/07-tls/03-mtls",
    "description": " # Create a MySQL Cluster With mTLS on KubeBlocks  This guide explains how to configure a MySQL cluster with **mutual TLS (mTLS)** encryption using KubeBlocks. mTLS ensures both the server and client authenticate each other during a connection, providing enhanced security for your database infrastru"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for MySQL Clusters with the Prometheus Operator",
    "content": "\n# Observability for MySQL Clusters with the Prometheus Operator\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Installing the Prometheus Operator\n\nIf the Prometheus Operator is not already installed, you can install it using Helm:\n\n```bash\nkubectl create namespace monitoring\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack -n monitoring --create-namespace\n```\n\nOr you can follow the steps in [How to install the Prometheus Operator](../docs/install-prometheus.md) to install the Prometheus Operator.\n\nCheck the status of deployed pods:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs             ",
    "path": "docs/preview/kubeblocks-for-mysql/08-monitoring/01-integrate-with-prometheus-operator",
    "description": " # Observability for MySQL Clusters with the Prometheus Operator  ## Prerequisites  Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_09-advanced-pod-management_01-custom-scheduling-policy",
    "title": "Configuring Custom Scheduling Policies for MySQL Cluster Pods in KubeBlocks",
    "content": "\n# Configuring Custom Scheduling Policies for MySQL Cluster Pods in KubeBlocks\n\nThis guide demonstrates how to configure custom scheduling policies for MySQL Cluster Pods in KubeBlocks. For example:\n1. Distribute Pods across different availability zones (AZs) to improve high availability.\n2. Deploy Pods in the same AZ to reduce latency.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n   - A Kubernetes cluster is up and running.\n   - The kubectl CLI tool is configured to communicate with your cluster.\n   - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Verify K8s Node Distribution\n\nOur Kubernetes cluster (EKS) consists of 9 nodes distributed across 3 availability zones, with 3 nodes in each AZ. To confirm the node distribution across availability zones, run the following command:\n\n```bash\nkubectl get nodes -o jsonpath='' | while read node; do echo -n \"Node: $node, Zone: \"; kubectl get node \"$node\" -o jsonpath=''; echo; done\n```\nExpected Output:\n```bash\nip-10-0-1-107.ap-southeast-1.compute.internal   Ready       91m     v1.31.5-eks-5d632ec\nip-10-0-1-183.ap-southeast-1.compute.internal   Ready       71m     v1.31.5-eks-5d632ec\nip-10-0-1-217.ap-southeast-1.compute.internal   Ready       2m13s   v1.31.5-eks-5d632ec\nip-10-0-2-186.ap-southeast-1.compute.internal   Ready       91m     v1.31.5-eks-5d632ec\nip-10-0-2-252.ap-southeast-1.compute.internal   Ready       71m     v1.31.5-eks-5d632ec\nip-10-0-2-71.ap-southeast-1.compute.internal    Ready       2m24s   v1.31.5-eks-5d632ec\nip-10-0-3-143.ap-southeast-1.compute.internal   Ready       91m     v1.31.5-eks-5d632ec\nip-10-0-3-205.ap-southeast-1.compute.internal   Ready       36s     v1.31.5-eks",
    "path": "docs/preview/kubeblocks-for-mysql/09-advanced-pod-management/01-custom-scheduling-policy",
    "description": " # Configuring Custom Scheduling Policies for MySQL Cluster Pods in KubeBlocks  This guide demonstrates how to configure custom scheduling policies for MySQL Cluster Pods in KubeBlocks. For example: 1. Distribute Pods across different availability zones (AZs) to improve high availability. 2. Deploy "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_09-advanced-pod-management_02-custom-pod-resources",
    "title": "Customizing Pod Resource Configurations and Labels in a MySQL Cluster Managed by KubeBlocks",
    "content": "\n# Customizing Pod Resource Configurations and Labels in a MySQL Cluster Managed by KubeBlocks\n\nIn certain scenarios, different Pods within the same database cluster may require varying resource allocations. For example:\n- Some replicas dedicated to report generation might leverage additional resources to handle analytical queries efficiently.\n\nWith KubeBlocks, you can customize Pod resource configurations and labels to tailor each instance to meet its unique requirements.\nThis guide demonstrates how to deploy a MySQL cluster with a custom replica provisioned with higher CPU and memory resources.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node semi-sync MySQL cluster (1 primary, 1 secondary):\n\n```yaml\nkubectl apply -f - \nmysql                           500m / 500m          512Mi / 512Mi           data:20Gi      \n```\n**Observation**:\n- The default replica has 0.5 CPU and 0.5Gi memory.\n- The custom replica has 1 CPU and 1Gi memory.\n\n## Expose the custom Pod as a Service\nTo expose the custom Pod via a separate Service, use the following configuration:\n```yaml\nkubectl apply -f -         3306/TCP                                                12m\nexample-mysql-cluster-mysql            ClusterIP   172.20.11.166            3306/TCP                                                12m\nexample-mysql-cluster-mysql-headless   ClusterIP   None                     3306/TCP,3601/TCP,9104/TCP,3501/TCP,3502/TCP,9901/TCP   12m",
    "path": "docs/preview/kubeblocks-for-mysql/09-advanced-pod-management/02-custom-pod-resources",
    "description": " # Customizing Pod Resource Configurations and Labels in a MySQL Cluster Managed by KubeBlocks  In certain scenarios, different Pods within the same database cluster may require varying resource allocations. For example: - Some replicas dedicated to report generation might leverage additional resour"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_09-advanced-pod-management_03-parallel-pod-management-concurrency",
    "title": "Configuring MySQL Cluster with Controlled Pod Creation, Scaling, and Deletion Parallelism in KubeBlocks",
    "content": "\n# Configuring a MySQL Cluster with Controlled Pod Creation, Scaling, and Deletion Parallelism in KubeBlocks\n\nThis guide demonstrates how to control pod creation, scaling, and deletion parallelism for MySQL clusters in KubeBlocks using the `parallelPodManagementConcurrency` parameter. By defining the maximum number of pods that can be managed in parallel, it allows users to balance operational speed and system stability. Unlike the `podManagementPolicy` in StatefulSet, which only provides two fixed options (`OrderedReady` or `Parallel`), `parallelPodManagementConcurrency` offers more flexibility, making it ideal for both resource-sensitive and production environments.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node semi-sync MySQL cluster (1 primary, 1 secondary) and set the `parallelPodManagementConcurrency` parameter to 1 to enforce sequential pod creation.\n\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: Delete\n  componentSpecs:\n    - name: mysql\n      serviceVersion: 8.0.35\n      replicas: 2\n      parallelPodManagementConcurrency: 1\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            ",
    "path": "docs/preview/kubeblocks-for-mysql/09-advanced-pod-management/03-parallel-pod-management-concurrency",
    "description": " # Configuring a MySQL Cluster with Controlled Pod Creation, Scaling, and Deletion Parallelism in KubeBlocks  This guide demonstrates how to control pod creation, scaling, and deletion parallelism for MySQL clusters in KubeBlocks using the `parallelPodManagementConcurrency` parameter. By defining th"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_09-advanced-pod-management_04-instance-update-strategy-ondelete",
    "title": "Setting Instance Update Strategy to OnDelete for MySQL Clusters in KubeBlocks",
    "content": "\n# Set Instance Update Strategy to OnDelete for MySQL Clusters in KubeBlocks\n\nThe `instanceUpdateStrategy.type` field supports two values: 'OnDelete' and 'RollingUpdate'.\n- 'OnDelete': Updates that require a Pod restart are blocked until the Pods are manually deleted. This provides fine-grained control over updates, as ordered rolling restarts are disabled. You decide when and how to restart the Pods, ensuring minimal disruption to your workload.\n- 'RollingUpdate' (default): Updates are applied automatically with ordered rolling restarts. The Operator restarts Pods in a controlled manner to ensure availability and a seamless update process.\n\nBy using the OnDelete strategy, you can tailor update behavior to meet specific requirements, such as maintaining maximum stability during updates or scheduling restarts during maintenance windows.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node MySQL cluster with semi-synchronous replication (1 primary, 1 secondary) using the following YAML configuration:\n\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: Delete\n  componentSpecs:\n    - name: mysql\n      serviceVersion: 8.0.35\n      replicas: 2\n      instanceUpdateStrategy:\n        type: OnDelete\n      resources:\n        limits:\n          cpu: '0.5'\n          memory:",
    "path": "docs/preview/kubeblocks-for-mysql/09-advanced-pod-management/04-instance-update-strategy-ondelete",
    "description": " # Set Instance Update Strategy to OnDelete for MySQL Clusters in KubeBlocks  The `instanceUpdateStrategy.type` field supports two values: 'OnDelete' and 'RollingUpdate'. - 'OnDelete': Updates that require a Pod restart are blocked until the Pods are manually deleted. This provides fine-grained cont"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-mysql_09-advanced-pod-management_05-gradual-rolling-update",
    "title": "Configuring a MySQL Cluster with Gradual Rolling Update in KubeBlocks",
    "content": "\n# Configuring a MySQL Cluster with Gradual Rolling Update in KubeBlocks\n\nThis guide demonstrates how to configure gradual rolling updates for MySQL clusters in KubeBlocks using the rollingUpdate strategy. Gradual rolling updates allow you to control the number of Pods updated at a time, ensuring minimal disruption during updates.\n**Key Parameters:**\n- `rollingUpdate.replicas`: Specifies the number of instances to update during each step of the rolling update. The remaining instances stay unaffected.\n  - The value can be an absolute number (e.g., 5) or a percentage of desired instances (e.g., 10%).\n  - Absolute numbers are calculated by rounding up percentages.\n  - The default is the total number of replicas, meaning all instances are updated.\n- `rollingUpdate.maxUnavailable`: Limits the maximum number of instances that can be unavailable during the update.\n  - The value can also be an absolute number (e.g., 5) or a percentage (e.g., 10%).\n  - This value cannot be 0. The default is 1.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node MySQL cluster with semi-synchronous replication (1 primary, 1 secondary) using the following YAML configuration:\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: Delete\n  componentSpecs:\n    - name: mysql\n      serviceVersion",
    "path": "docs/preview/kubeblocks-for-mysql/09-advanced-pod-management/05-gradual-rolling-update",
    "description": " # Configuring a MySQL Cluster with Gradual Rolling Update in KubeBlocks  This guide demonstrates how to configure gradual rolling updates for MySQL clusters in KubeBlocks using the rollingUpdate strategy. Gradual rolling updates allow you to control the number of Pods updated at a time, ensuring mi"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_04-operations_01-stop-start-restart",
    "title": "PostgreSQL Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# PostgreSQL Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a PostgreSQL cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a PostgreSQL cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Retains persistent storage (PVCs)\n3. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\nOption 1: OpsRequest API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: pg-cluster-stop-ops\n  namespace: demo\nspec:\n  clusterName: pg-cluster\n  type: Stop\n```\n\n\n\nOption 2: Cluster API Patch\n\nModify the cluster spec directly by patching the stop field:\n\n```bash\nkubectl patch cluster pg-cluster -n demo --type='json' -p='[\n  \n]'\n```\n\n\n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster pg-cluster -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME         CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\n    pg-cluster   postgresql           Delete               Stopping   6m3s\n    pg-cluster   postgresql           Delete               Stopped    6m55s\n    ```\n\n2. Verify no running pods:\n   ",
    "path": "docs/preview/kubeblocks-for-postgresql/04-operations/01-stop-start-restart",
    "description": "  # PostgreSQL Cluster Lifecycle Management  This guide demonstrates how to manage a PostgreSQL cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help optimize resourc"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a PostgreSQL Cluster",
    "content": "\n\n\n\n# Vertical Scaling for PostgreSQL Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a PostgreSQL cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for PostgreSQL instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks orchestrates scaling with minimal impact:\n1. Secondary replicas update first\n2. Primary updates last after secondaries are healthy\n3. Cluster status transitions from `Updating` to `Running`\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Secondary replicas are updated first (one at a time)\n1. Primary is updated last after secondary replicas are healthy\n1. Cluster status transitions from `Updating` to `Running`\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the postgresql component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: pg-cluster-vscale-ops\n    namespace: demo\n  spec:\n    clusterName: pg-cluster\n    type: VerticalScaling\n    verticalScaling:\n    - componentName: postgresql\n      requests:\n        cpu: '1'\n        memory: 1Gi\n      limits:\n        cpu: '1'\n        memory: 1Gi\n  ```\n  What Happens During Vertical Scaling?\n  - Secondary Pods are recreated first to ensure the primary Pod remains available.\n  - Once all secondary Pods are updated, the primary Pod is restarted with the new resource configuration.\n\n\n  You can check the progress of the scaling operation with the following command:\n\n  ```bash\n  kubectl -n demo get ops pg-cluster-vscale-ops -w\n  ```\n\n  Expected Result:\n  ```bash",
    "path": "docs/preview/kubeblocks-for-postgresql/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for PostgreSQL Clusters with KubeBlocks  This guide demonstrates how to vertically scale a PostgreSQL cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies compute resources (CPU "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of PostgreSQL Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for PostgreSQL Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a PostgreSQL cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running` with `secondary` role\n2. Data synced from primary to new replica\n3. Cluster status changes from `Updating` to `Running`\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the PostgreSQL cluster by adding 1 replica:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: pg-cluster-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: pg-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: postgresql\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops pg-cluster-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME           TYPE                CLUSTER      STATUS    PROGRESS   AGE\n  pg-scale-out   HorizontalScaling   pg-cluster   Running   0/1        8s\n  pg-scale-out   HorizontalScaling   pg-cluster   Running   1/1        24s\n  pg-scale-out   HorizontalScaling   pg-cluster   Succeed   1/1        24s\n  ```\n  \n\n\n  Option 2: Direct Cluster API Update\n\n  Alternatively, you can perform a direct update to the `replicas` field in the Cluster resource:\n\n  ```yaml\n  apiVersion: apps.kubeblocks.io/v1\n  kind: Cluster\n  metadata:\n    name: pg-cluster\n    namespace: demo\n  spec:\n    terminationPolicy: Delete\n    clusterDef: postgresql\n    top",
    "path": "docs/preview/kubeblocks-for-postgresql/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for PostgreSQL Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a PostgreSQL cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequi"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a PostgreSQL Cluster",
    "content": "\n\n\n\n# Expanding Volume in a PostgreSQL Cluster\n\nThis guide explains how to expand Persistent Volume Claims (PVCs) in a PostgreSQL cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the underlying storage class, this operation can be performed without downtime.\n\nVolume expansion allows you to increase the size of a Persistent Volume Claim (PVC) after it has been created. This feature was introduced in Kubernetes v1.11 and became generally available (GA) in Kubernetes v1.24.\n\n## Prerequisites\n\n\n\n### Check the Storage Class for Volume Expansion Support\n\nList all available storage classes and verify if volume expansion is supported by checking the `ALLOWVOLUMEEXPANSION` field:\n```bash\nkubectl get storageclass\n```\n\nExample Output:\n```bash\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4d10h\nkb-default-sc       ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   3d7h\nsc-s3-repo-2qsxfh   ru.yandex.s3.csi        Retain          Immediate              false                  3d7h\n```\nEnsure the storage class you are using has `ALLOWVOLUMEEXPANSION` set to true. If it is false, the storage class does not support volume expansion.\n\n## Deploy a PostgreSQL Replication Cluster with StorageClass\n\nKubeBlocks uses a declarative approach to manage PostgreSQL clusters. Below is an example configuration for deploying a PostgreSQL cluster with 2 replicas (1 primary, 1 secondary).\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: pg-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: postgresql\n  topology: replication\n  componentSpecs:\n    - name: postgresql\n    ",
    "path": "docs/preview/kubeblocks-for-postgresql/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a PostgreSQL Cluster  This guide explains how to expand Persistent Volume Claims (PVCs) in a PostgreSQL cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported b"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy PostgreSQL Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage PostgreSQL Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing PostgreSQL services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## View Network Services\nList the Services created for the PostgreSQL cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=pg-cluster -n demo\n```\n\nExample Services:\n```bash\nNAME                               TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\npg-cluster-postgresql-postgresql   ClusterIP   10.96.19.237           5432/TCP,6432/TCP   157m\n```\n\n:::note\n\nThere are two ports here 5432 and 6432, where 5432 is for postgresql and 6432 for PgBouncer.\n\n:::\n\n## Expose PostgreSQL Service\n\nExternal service addresses enable public internet access to PostgreSQL, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the PostgreSQL service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: pg-cluster-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: pg-cluster\n    expose:\n    - componentName: postgresql\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options are 'ClusterIP', 'NodePor",
    "path": "docs/preview/kubeblocks-for-postgresql/04-operations/05-manage-loadbalancer",
    "description": "    # Manage PostgreSQL Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing PostgreSQL services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_04-operations_06-minior-version-upgrade",
    "title": "Upgrading the Minor Version of a PostgreSQL Cluster in KubeBlocks",
    "content": "\n\n# Upgrading the Minor Version of a PostgreSQL Cluster in KubeBlocks\n\nThis guide walks you through the deployment and minor version upgrade of a PostgreSQL cluster managed by KubeBlocks, ensuring minimal downtime during the process.\n\nTo minimize the impact on database availability, the upgrade process starts with the replicas (secondary instances). Once the replicas are upgraded, a switchover operation promotes one of the upgraded replicas to primary. The switchover process is very fast, typically completing in a few hundred milliseconds. After the switchover, the original primary instance is upgraded, ensuring minimal disruption to the application.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a PostgreSQL Replication Cluster\n\nKubeBlocks uses a declarative approach for managing PostgreSQL clusters. Below is an example configuration for deploying a PostgreSQL cluster with 2 replicas (1 primary, 1 replicas).\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: pg-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: postgresql\n  topology: replication\n  componentSpecs:\n    - name: postgresql\n      serviceVersion: 14.7.2  # use 14.7.2 here to test minor version upgrade\n      labels:\n        apps.kubeblocks.postgres.patroni/scope: pg-cluster-postgresql\n      disableExporter: true\n      replicas: 2\n      resources:\n        limits:\n          cpu: \"0.5\"\n   ",
    "path": "docs/preview/kubeblocks-for-postgresql/04-operations/06-minior-version-upgrade",
    "description": "  # Upgrading the Minor Version of a PostgreSQL Cluster in KubeBlocks  This guide walks you through the deployment and minor version upgrade of a PostgreSQL cluster managed by KubeBlocks, ensuring minimal downtime during the process.  To minimize the impact on database availability, the upgrade proc"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_04-operations_07-modify-parameters",
    "title": "Modify PostgreSQL Parameters",
    "content": "\n# Modify PostgreSQL Parameters\n\nDatabase reconfiguration involves modifying parameters, settings, or configurations to optimize performance, security, or availability. Parameter changes fall into two categories:\n\n| Type | Restart Required | Scope | Example Parameters |\n|------|------------------|-------|--------------------|\n| **Dynamic** | No | Immediate effect | `max_connections` |\n| **Static** | Yes | After restart | `shared_buffers` |\n\nFor static parameters, KubeBlocks minimizes downtime by:\n1. Modifying and restarting replica nodes first\n2. Performing a switchover to promote the updated replica as primary (typically completes in milliseconds)\n3. Restarting the original primary node\n\nThis guide demonstrates how to modify both dynamic and static parameters of a PostgreSQL cluster managed by KubeBlocks using a Reconfiguring OpsRequest.\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Check Parameter Values\n\n### Retrieve Credentials\nKubeBlocks automatically creates a secret containing the PostgreSQL postgres credentials. Retrieve the credentials with the following commands:\n```bash\nNAME=`kubectl get secrets -n demo pg-cluster-postgresql-account-postgres -o jsonpath='' | base64 -d`\nPASSWD=`kubectl get secrets -n demo pg-cluster-postgresql-account-postgres -o jsonpath='' | base64 -d`\n```\n\n### Access PostgreSQL Cluster\nTo connect to the cluster's primary node, use the PostgreSQL client:\n```bash\nkubectl exec -it -n demo pg-cluster-postgresql-0 -c postgresql -- env PGUSER=$ PGPASSWORD=$ psql\n```\n\n### Query Parameter Values\n\nOnce connected, you can query the current value of 'max_connections' and 'shared_buffers':\n```sql\npostgres=# SHOW max_connections;\n max_connections\n-----------------\n 56\n(1 row)\n\npostgres=# show pgaudit.log;\n pgaudit.log\n-------------\n ddl,read,write\n(1 row)\n\npostgres=# show shared_buffers;\n shared_buffers\n----------------\n 128MB\n(1 row)\n```\n\n## Dynamic Parameter Example: Modifying max_connections and pgaudit.",
    "path": "docs/preview/kubeblocks-for-postgresql/04-operations/07-modify-parameters",
    "description": " # Modify PostgreSQL Parameters  Database reconfiguration involves modifying parameters, settings, or configurations to optimize performance, security, or availability. Parameter changes fall into two categories:  | Type | Restart Required | Scope | Example Parameters | |------|------------------|--"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_04-operations_08-switchover",
    "title": "PostgreSQL Cluster Switchover",
    "content": "\n\n# PostgreSQL Cluster Switchover\n\nA **switchover** is a planned operation that transfers the primary role from one PostgreSQL instance to another. Unlike failover which occurs during failures, switchover provides:\n- Controlled role transitions\n- Minimal downtime (typically a few hundred milliseconds)\n- Predictable maintenance windows\n\nSwitchover is ideal for:\n- Node maintenance/upgrades\n- Workload rebalancing\n- Testing high availability\n- Planned infrastructure changes\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Check Roles\nList the Pods and their roles (primary or secondary):\n\n```bash\nkubectl get pods -n demo -l app.kubernetes.io/instance=pg-cluster -L kubeblocks.io/role\n```\n\nExample Output:\n\n```text\nNAME                      READY   STATUS    RESTARTS   AGE     ROLE\npg-cluster-postgresql-0   4/4     Running   0          9m59s   primary\npg-cluster-postgresql-1   4/4     Running   0          11m     secondary\n```\n\n## Performing a Planned Switchover\n\nTo initiate a planned switchover, create an OpsRequest resource as shown below:\n\n  Option 1: Automatic Switchover (No preferred candidate)\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: pg-switchover-ops\n    namespace: demo\n  spec:\n    clusterName: pg-cluster\n    type: Switchover\n    switchover:\n    - componentName: postgresql\n      instanceName: pg-cluster-postgresql-0\n  ```\n **Key Parameters:**\n  - `instanceName`: Specifies the instance (Pod) that is primary or leader before a switchover operation.\n\n  \n  Option 2: Targeted Switchover (Specific candidate)\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: pg-switchover-targeted\n    namespace: demo\n  spec:\n    clusterName: pg-cluster\n    type: Switchover\n    switchover:\n    - componentName: postgresql\n      # Specifies the instance whose role will be transferred.\n      # A typical usage is to transfer the leader role in a con",
    "path": "docs/preview/kubeblocks-for-postgresql/04-operations/08-switchover",
    "description": "  # PostgreSQL Cluster Switchover  A **switchover** is a planned operation that transfers the primary role from one PostgreSQL instance to another. Unlike failover which occurs during failures, switchover provides: - Controlled role transitions - Minimal downtime (typically a few hundred millisecond"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed PostgreSQL Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed PostgreSQL Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in PostgreSQL clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\nTo decommission a specific Pod (e.g., 'pg-cluster-postgresql-1'), you can use one of the following methods:\n\n\n\n  Option 1: Using OpsRequest\n\n  Create an OpsRequest to mark the Pod as offline:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: pg-cluster-decommission-ops\n    namespace: demo\n  spec:\n    clusterName: pg-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: postgresql\n      scaleIn:\n        onlineInstancesToOffline:\n          - 'pg-cluster-postgresql-1'  # Specifies the instance names that need to be taken offline\n  ```\n\n  #### Monitor the Decom",
    "path": "docs/preview/kubeblocks-for-postgresql/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed PostgreSQL Clusters  This guide explains how to decommission (take offline) specific Pods in PostgreSQL clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_04-operations_11-rebuild-replica",
    "title": "Recovering PostgreSQL Replica in KubeBlocks",
    "content": "\n# Rebuilding PostgreSQL Replicas in KubeBlocks\n\nThis guide demonstrates how to rebuild replicas using both in-place and non-in-place methods.\n\n**What is Replica Rebuilding**?\n\nReplica rebuilding is the process of recreating a PostgreSQL replica from scratch or from a backup while maintaining:\n- **Data Consistency**: Ensures the replica has an exact copy of primary data\n- **High Availability**: Minimizes downtime during the rebuild process\n\nDuring this process:\n1. The problematic replica is identified and isolated\n2. A new base backup is taken from the primary\n3. WAL (Write-Ahead Log) segments are streamed to catch up\n4. The replica rejoins the replication cluster\n\n**When to Rebuild a PostgreSQL Instance**?\n\nRebuilding becomes necessary in these common scenarios:\n- Replica falls too far behind primary (irrecoverable lag), or Replication slot corruption\n- WAL file gaps that can't be automatically resolved\n- Data Corruption: with storage-level corruption (disk/volume issues), inconsistent data between primary and replica, etc\n- Infrastructure Issues: Node failure, storage device failure or cross Zone/Region migration\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Connect to the Primary PostgreSQL Replcia and Write Mock Data\n\nCheck replica roles with command:\n```bash\nkubectl get pods -n demo -l app.kubernetes.io/instance=pg-cluster -L kubeblocks.io/role\n```\n\nExample Output:\n```bash\nNAME                      READY   STATUS    RESTARTS   AGE     ROLE\npg-cluster-postgresql-0   4/4     Running   0          13m     secondary\npg-cluster-postgresql-1   4/4     Running   0          12m     primary\n```\n\n### Step 1: Connect to the Primary Instance\n\nKubeBlocks automatically creates a Secret containing the PostgreSQL postgres credentials. Retrieve the PostgreSQL postgres credentials:\n\n```bash\nNAME=`kubectl get secrets -n demo pg-cluster-postgresql-account-postgres -o jsonpath='' | base64 -d`\nPASSWD=`kubectl get secrets -n demo pg-cluster-p",
    "path": "docs/preview/kubeblocks-for-postgresql/04-operations/11-rebuild-replica",
    "description": " # Rebuilding PostgreSQL Replicas in KubeBlocks  This guide demonstrates how to rebuild replicas using both in-place and non-in-place methods.  **What is Replica Rebuilding**?  Replica rebuilding is the process of recreating a PostgreSQL replica from scratch or from a backup while maintaining: - **D"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_05-backup-restore_01-create-backuprepo",
    "title": "Create a Backup Repository for KubeBlocks",
    "content": "\n# Create a BackupRepo for KubeBlocks\n\nThis guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.\n\n## Prerequisites\n- AWS CLI configured with appropriate permissions to create S3 buckets.\n- kubectl access to your Kubernetes cluster.\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks)) and running in the kb-system namespace.\n\n## Step 1: Create S3 Bucket\n\nUse the AWS CLI to create an S3 bucket in your desired region. Replace `` with your target AWS region (e.g., `us-east-1`, `ap-southeast-1`).\n\n```bash\n aws s3api create-bucket --bucket kubeblocks-backup-repo --region  --create-bucket-configuration LocationConstraint=\n```\n\nExample (for us-west-1):\n```bash\naws s3api create-bucket \\\n  --bucket kubeblocks-backup-repo \\\n  --region us-west-1 \\\n  --create-bucket-configuration LocationConstraint=us-west-1\n```\n\nExample Output:\n\n```json\n\n```\n\nVerification:\nConfirm the bucket was created by listing its contents (it will be empty initially):\n\n```bash\naws s3 ls s3://kubeblocks-backup-repo\n```\n\n## Step 2: Create a Kubernetes Secret for AWS Credentials\n\nStore your AWS credentials securely in a Kubernetes Secret. Replace `` and `` with your actual AWS credentials:\n\n```bash\n# Create a secret to save the access key\nkubectl create secret generic s3-credential-for-backuprepo \\\n  --from-literal=accessKeyId= \\\n  --from-literal=secretAccessKey= \\\n  -n kb-system\n```\n\n## Step 3: Configure Backup Repository\n\nA BackupRepo is a custom resource that defines a storage repository for backups. In this step, you'll integrate your S3 bucket with KubeBlocks by creating a BackupRepo resource.\n\nApply the following YAML to create the BackupRepo. Replace fields(e.g., bucket name, region) with your specific settings.\n\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupRepo\nmetadata:\n  name: s3-repo\n  annotations:\n    # mark this backuprepo as default one\n    dataprotection.kubeblocks.io/is",
    "path": "docs/preview/kubeblocks-for-postgresql/05-backup-restore/01-create-backuprepo",
    "description": " # Create a BackupRepo for KubeBlocks  This guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.  ## Prerequisites - AWS CLI configured with appropriate permissions to create S3 buckets. - kubectl access to your Kubernetes cluster. -"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_05-backup-restore_02-create-full-backup",
    "title": "Create a Full Backup for a PostgreSQL Cluster on KubeBlocks",
    "content": "\n\n\n# Create a Full Backup for PostgreSQL on KubeBlocks\n\nThis guide demonstrates how to create and validate full backups for PostgreSQL clusters on KubeBlocks using the `pg-basebackup` method through both:\n- The Backup API (direct backup operations)\n- The OpsRequest API (managed backup operations with enhanced monitoring)\n\nWe will cover how to restore data from a backup in the [Restore From Full Backup](./05-restoring-from-full-backup) guide.\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Backup Prerequisites\n\nBefore creating backups, ensure:\n1. Backup repository is configured:\n   - `BackupRepo` resource exists\n   - Network connectivity between cluster and repository\n   - `BackupRepo` status shows \"Ready\"\n\n2. Cluster is ready:\n   - Cluster status is \"Running\"\n   - No ongoing operations (scaling, upgrades, etc.)\n\n## Identify Backup Configuration\n\nCheck available backup policies and schedules:\n\n```bash\n# List backup policies\nkubectl get backuppolicy -n demo -l app.kubernetes.io/instance=pg-cluster\n\n# List backup schedules\nkubectl get backupschedule -n demo -l app.kubernetes.io/instance=pg-cluster\n```\n\nExpected Output:\n```bash\nNAME                                            BACKUP-REPO   STATUS      AGE\npg-cluster-postgresql-backup-policy                           Available   58m\n\nNAME                                              STATUS      AGE\npg-cluster-postgresql-backup-schedule             Available   60m\n```\n\nView supported backup methods in the BackupPolicy CR 'pg-cluster-postgresql-backup-policy':\n\n```bash\nkubectl get backuppolicy pg-cluster-postgresql-backup-policy -n demo -oyaml | yq '.spec.backupMethods[].name'\n```\n\nExample Output:\n```bash\npg-basebackup\nwal-g\nwal-g-incremental\narchive-wal\nwal-g-archive\n```\n\n\n**List of Backup methods**\n\nKubeBlocks PostgreSQL supports these backup methods:\n\n| Feature           | Method          | Description |\n|-------------------|-----------------|-------------|\n| Full Backup       | p",
    "path": "docs/preview/kubeblocks-for-postgresql/05-backup-restore/02-create-full-backup",
    "description": "   # Create a Full Backup for PostgreSQL on KubeBlocks  This guide demonstrates how to create and validate full backups for PostgreSQL clusters on KubeBlocks using the `pg-basebackup` method through both: - The Backup API (direct backup operations) - The OpsRequest API (managed backup operations wit"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_05-backup-restore_03-scheduled-full-backup",
    "title": "Setting Up a PostgreSQL Cluster with Scheduled Backups in KubeBlocks",
    "content": "\n\n# Setting Up a PostgreSQL Cluster with Scheduled Backups in KubeBlocks\n\nThis guide demonstrates how to deploy a PostgreSQL cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Prerequisites for Backup\n\n1. Backup Repository Configured:\n   - Configured `BackupRepo`\n   - Network connectivity between cluster and repo, `BackupRepo` status is `Ready`\n\n2. Cluster is Running:\n   - Cluster must be in `Running` state\n   - No ongoing operations (scaling, upgrades etc.)\n\n## Configure Scheduled Backups\n\nKubeBlocks automatically creates a `BackupSchedule` resource when the cluster is created. Follow these steps to enable and configure scheduled backups:\n\n1. Verify the default backup schedule configuration:\n\n```bash\nkubectl get backupschedule pg-cluster-postgresql-backup-schedule  -n demo -oyaml\n```\n\nExample Output:\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupSchedule\nspec:\n  backupPolicyName: pg-cluster-postgresql-backup-policy\n  schedules:\n  - backupMethod: pg-basebackup\n    # ┌───────────── minute (0-59)\n    # │ ┌───────────── hour (0-23)\n    # │ │ ┌───────────── day of month (1-31)\n    # │ │ │ ┌───────────── month (1-12)\n    # │ │ │ │ ┌───────────── day of week (0-6) (Sunday=0)\n    # │ │ │ │ │\n    # 0 18 * * *\n    # schedule this job every day at 6:00 PM (18:00).\n    cronExpression: 0 18 * * * # update the cronExpression to your need\n    enabled: false # set to `true` to schedule base backup periodically\n    retentionPeriod: 7d # set the retention period to your need\n```\n\n2. Enable and customize the backup schedule:\n```bash\nkubectl edit backupschedule pg-cluster-postgresql-backup-schedule -n demo\n```\n\nUpdate these key parameters:\n- `enabled`: Set to `true` to activate scheduled backups\n- `cronExpression`: Configure backup frequency using cron syntax\n- `retentionPeriod`: Set how long to keep backups (e.g., `7d`, `1mo`)\n\nExa",
    "path": "docs/preview/kubeblocks-for-postgresql/05-backup-restore/03-scheduled-full-backup",
    "description": "  # Setting Up a PostgreSQL Cluster with Scheduled Backups in KubeBlocks  This guide demonstrates how to deploy a PostgreSQL cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.  ## Prerequisites    ## Deploy a PostgreSQL Cluster    ## Verifying the Deployment"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_05-backup-restore_04-scheduled-continuous-backup",
    "title": "Setting Up a PostgreSQL Cluster with Scheduled Continuous Backup in KubeBlocks",
    "content": "\n\n# Setting Up a PostgreSQL Cluster with Scheduled Continuous Backup Enabled in KubeBlocks\n\nThis guide demonstrates how to configure a PostgreSQL cluster on KubeBlocks with:\n\n- Scheduled full backups (base backups)\n- Continuous WAL (Write-Ahead Log) archiving\n- Point-In-Time Recovery (PITR) capabilities\n\nThis combination provides comprehensive data protection with minimal recovery point objectives (RPO).\n\n## What is PITR?\nPoint-In-Time Recovery (PITR) allows you to restore a database to a specific moment in time by combining full backups with continuous binlog/wal/archive log backups.\n\nFor details on restoring data from both full backups and continuous binlog backups, refer to the [Restore From PITR](restore-with-pitr.mdx)  guide.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Prerequisites for Backup\n\n1. Backup Repository Configured:\n   - Configured `BackupRepo`\n   - Network connectivity between cluster and repo, `BackupRepo` status is `Ready`\n\n2. Cluster is Running:\n   - Cluster must be in `Running` state\n   - No ongoing operations (scaling, upgrades etc.)\n\n\n## List of Backup methods\n\nKubeBlocks PostgreSQL supports these backup methods:\n\n| Feature           | Method          | Description |\n|-------------------|-----------------|-------------|\n| Full Backup       | pg-basebackup   | Uses `pg_basebackup`, a PostgreSQL utility to create a base backup |\n| Full Backup       | wal-g  | Uses `wal-g` to create a full backup (requires WAL-G configuration) |\n| Continuou",
    "path": "docs/preview/kubeblocks-for-postgresql/05-backup-restore/04-scheduled-continuous-backup",
    "description": "  # Setting Up a PostgreSQL Cluster with Scheduled Continuous Backup Enabled in KubeBlocks  This guide demonstrates how to configure a PostgreSQL cluster on KubeBlocks with:  - Scheduled full backups (base backups) - Continuous WAL (Write-Ahead Log) archiving - Point-In-Time Recovery (PITR) capabili"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_05-backup-restore_05-restoring-from-full-backup",
    "title": "Restore a PostgreSQL Cluster from Backup",
    "content": "\n\n# Restore a PostgreSQL Cluster from Backup\n\nThis guide demonstrates two methods to restore a PostgreSQL cluster from backup in KubeBlocks:\n\n1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations\n2. **OpsRequest API Method** - Enhanced operational control with progress monitoring\n\n## Prerequisites\n\n\n\n## Preparing for Restoration: Locate one Full Backup\nBefore restoring, ensure that there is a full backup available. The restoration process will use this backup to create a new PostgreSQL cluster.\n\n- Backup repository accessible from new cluster\n- Valid full backup in `Completed` state\n- Adequate CPU/memory resources\n- Sufficient storage capacity\n\nFind available full backups:\n\n```bash\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Full,app.kubernetes.io/instance=pg-cluster # get the list of full backups\n```\n\nPick ONE of the Backups whose status is `Completed`.\n\n### Step 1: Create Restored Cluster\nCreate a new cluster with restore configuration:\n\nKey parameters:\n- `kubeblocks.io/restore-from-backup` annotation\n- Backup name and namespace located from the previous steps\n\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: pg-restored\n  namespace: demo\n  annotations:\n    # NOTE: replcae  with the backup name\n    kubeblocks.io/restore-from-backup: '}'\nspec:\n  terminationPolicy: Delete\n  clusterDef: postgresql\n  topology: replication\n  componentSpecs:\n    - name: postgresql\n      serviceVersion: 16.4.0\n      disableExporter: true\n      labels:\n        apps.kubeblocks.postgres.patroni/scope: pg-restored-postgresql\n      replicas: 2\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20",
    "path": "docs/preview/kubeblocks-for-postgresql/05-backup-restore/05-restoring-from-full-backup",
    "description": "  # Restore a PostgreSQL Cluster from Backup  This guide demonstrates two methods to restore a PostgreSQL cluster from backup in KubeBlocks:  1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations 2. **OpsRequest API Method** - Enhanced operational control with progres"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_05-backup-restore_06-restore-with-pitr",
    "title": "Restore a PostgreSQL Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks",
    "content": "\n\n# Restore a PostgreSQL Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks\n\nThis guide demonstrates how to perform Point-In-Time Recovery (PITR) for PostgreSQL clusters in KubeBlocks using:\n\n1. A full base backup\n2. Continuous WAL (Write-Ahead Log) backups\n3. Two restoration methods:\n   - Cluster Annotation (declarative approach)\n   - OpsRequest API (operational control)\n\nPITR enables recovery to any moment within the `timeRange` specified.\n\n## Prerequisites\n\n\n\n## Prepare for PITR Restoration\nTo perform a PITR restoration, both a full backup and continuous backup are required. Refer to the documentation to configure these backups if they are not already set up.\n\n- Completed full backup\n- Active continuous WAL backup\n- Backup repository accessible\n- Sufficient resources for new cluster\n\nTo identify the list of full and continuous backups, you may follow the steps:\n\n### 1. Verify Continuous Backup\nConfirm you have a continuous WAL backup, either running or completed:\n\n```bash\n# expect EXACTLY ONE continuous backup per cluster\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Continuous,app.kubernetes.io/instance=pg-cluster\n```\n\n### 2. Check Backup Time Range\nGet the valid recovery window:\n\n```bash\nkubectl get backup  -n demo -o yaml | yq '.status.timeRange'\n```\n\nExpected Output:\n```text\nstart: \"2025-05-07T09:12:47Z\"\nend: \"2025-05-07T09:22:50Z\"\n```\n\n### 3. Identify Full Backup\nFind available full backups that meet:\n- Status: Completed\n- Completion time after continuous backup start time\n\n```bash\n# expect one or more Full backups\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Full,app.kubernetes.io/instance=pg-cluster\n```\n\n:::tip\nKubeBlocks automatically selects the most recent qualifying full backup as the base.\nMake sure there is a full backup meets the condition: its `stopTime`/`completionTimestamp` must **AFTER** Continuous backup's `startTime`, otherwise PITR restoration will fail.\n:::\n\n## Option 1: Clu",
    "path": "docs/preview/kubeblocks-for-postgresql/05-backup-restore/06-restore-with-pitr",
    "description": "  # Restore a PostgreSQL Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks  This guide demonstrates how to perform Point-In-Time Recovery (PITR) for PostgreSQL clusters in KubeBlocks using:  1. A full base backup 2. Continuous WAL (Write-Ahead Log) backups 3. Two restoration method"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_06-custom-secret_01-custom-secret",
    "title": "Create a PostgreSQL Cluster with a Custom Root Password on KubeBlocks",
    "content": "\n# Create PostgreSQL Cluster With Custom Password on KubeBlocks\n\nThis guide demonstrates how to deploy a PostgreSQL cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.\n\n## Prerequisites\n\n\n\n## Deploying the PostgreSQL Replication Cluster\n\nKubeBlocks uses a declarative approach for managing PostgreSQL clusters. Below is an example configuration for deploying a PostgreSQL cluster with 2 nodes (1 primary, 1 replicas) and a custom root password.\n\n### Step 1: Create a Secret for the Root Account\n\nThe custom root password is stored in a Kubernetes Secret. Create the Secret by applying the following YAML:\n\n```yaml\napiVersion: v1\ndata:\n  password: Y3VzdG9tcGFzc3dvcmQ= # custompassword\n  username: cm9vdA== #root\nimmutable: true\nkind: Secret\nmetadata:\n  name: custom-pg-secret\n  namespace: demo\n```\n- password: Replace custompassword with your desired password and encode it using Base64 (`echo -n \"custompassword\" | base64`).\n- username: The default PostgreSQL postgres user is 'root', encoded as 'cm9vdA=='.\n\n\n### Step 2: Deploy the PostgreSQL Cluster\n\nApply the following manifest to deploy the PostgreSQL cluster, referencing the Secret created in Step 1 for the root account:\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: pg-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: postgresql\n  topology: replication\n  componentSpecs:\n    - name: postgresql\n      serviceVersion: 16.4.0\n      labels:\n        apps.kubeblocks.postgres.patroni/scope: pg-cluster-postgresql\n      disableExporter: true\n      replicas: 2\n      systemAccounts:\n        - name: postgres\n          secretRef:\n            name: custom-pg-secret\n            namespace: demo\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            accessModes:\n              - ReadWriteOnce\n         ",
    "path": "docs/preview/kubeblocks-for-postgresql/06-custom-secret/01-custom-secret",
    "description": " # Create PostgreSQL Cluster With Custom Password on KubeBlocks  This guide demonstrates how to deploy a PostgreSQL cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.  ## Prerequisites    ## Deploying the PostgreSQL Replication Cluster  KubeBlocks uses a declarative app"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_07-tls_01-tls-overview",
    "title": "Deploying a PostgreSQL Cluster with TLS on KubeBlocks",
    "content": "\n\n# Deploying a PostgreSQL Cluster with TLS on KubeBlocks\n\nThis guide demonstrates how to deploy a PostgreSQL cluster with TLS encryption using KubeBlocks. Transport Layer Security (TLS) ensures secure communication between PostgreSQL clients and servers by encrypting data in transit, protecting sensitive information from interception. You'll learn how to:\n\n- Deploy a PostgreSQL cluster with TLS enabled\n- Establish secure connections using different TLS modes\n- Verify the TLS configuration\n- Clean up resources after testing\n\n## Prerequisites\n\n\n\n## Deploying the PostgreSQL Replication Cluster\n\nKubeBlocks uses a declarative approach for managing PostgreSQL clusters. Below is a configuration example for deploying a PostgreSQL cluster with TLS enabled (1 primary, 1 replica):\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: pg-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: postgresql\n  topology: replication\n  componentSpecs:\n    - name: postgresql\n      serviceVersion: 16.4.0\n      tls: true  # Enable TLS encryption\n      issuer:\n        name: KubeBlocks  # Use KubeBlocks' built-in certificate authority\n      labels:\n        apps.kubeblocks.postgres.patroni/scope: pg-cluster-postgresql\n      disableExporter: true\n      replicas: 2\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n\n**Key Configuration Fields**:\n- `tls: true`: Enables TLS encryption for all connections\n- `issuer: KubeBlocks`: Uses KubeBlocks' built-in certificate authority (alternatively: `UserProvided` for custom certificates)\n\n## Verifying the Deployment\n\nMonitor the cluster status until it reaches the `Running` state:\n```bash\nkubectl get cluster pg-cluster -n d",
    "path": "docs/preview/kubeblocks-for-postgresql/07-tls/01-tls-overview",
    "description": "  # Deploying a PostgreSQL Cluster with TLS on KubeBlocks  This guide demonstrates how to deploy a PostgreSQL cluster with TLS encryption using KubeBlocks. Transport Layer Security (TLS) ensures secure communication between PostgreSQL clients and servers by encrypting data in transit, protecting sen"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_07-tls_02-tls-custom-cert",
    "title": "Deploy a PostgreSQL Cluster with Custom TLS Certificates on KubeBlocks",
    "content": "\n\n# Deploy a PostgreSQL Cluster with Custom TLS Certificates on KubeBlocks\n\nThis guide demonstrates how to deploy a PostgreSQL cluster with **custom TLS certificates** using KubeBlocks. By providing your own certificates, you maintain complete control over the security configuration for encrypted client-server communication.\n\n## Prerequisites\n\n\n\n## Generate Certificates\n\nGenerate the required certificates using OpenSSL:\n\n1. **Root Certificate (CA)**\n```bash\n# Generate CA private key (password protected)\nopenssl genrsa -aes256 -out ca-key.pem 4096\n\n# Create self-signed root certificate (10-year validity)\nopenssl req -x509 -new -nodes -key ca-key.pem -sha256 -days 3650 -out ca.pem\n# Enter certificate details (e.g., Common Name = \"PostgreSQL Root CA\")\n```\n\n2. **Server Certificate**\n```bash\n# Generate server private key\nopenssl genrsa -out server-key.pem 4096\n\n# Create Certificate Signing Request\nopenssl req -new -key server-key.pem -out server-req.pem\n# Enter server details (Common Name must match PostgreSQL server address)\n\n# Sign server certificate with CA (10-year validity)\nopenssl x509 -req -in server-req.pem -CA ca.pem -CAkey ca-key.pem \\\n  -CAcreateserial -out server-cert.pem -days 3650 -sha256\n```\n\n:::note\n\nThe Common Name (CN) must match your PostgreSQL server address (e.g., service name `pg-cluster-postgresql-postgresql`).\n\n:::\n\n3. **Verify Certificates**\n```bash\nopenssl verify -CAfile ca.pem server-cert.pem\n# Example Output: server-cert.pem: OK\n```\n\n## Create Kubernetes Secret\n\nStore certificates in a Kubernetes Secret for cluster access:\n\n```bash\nkubectl create secret generic postgresql-tls-secret \\\n  --namespace=demo \\\n  --from-file=ca.crt=ca.pem \\\n  --from-file=tls.crt=server-cert.pem \\\n  --from-file=tls.key=server-key.pem \\\n  --type=kubernetes.io/tls\n```\n\n## Deploy PostgreSQL Cluster\n\nDeploy a 2-node PostgreSQL cluster (1 primary, 1 replica) with TLS:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: pg-cluster\n  namespace: demo\ns",
    "path": "docs/preview/kubeblocks-for-postgresql/07-tls/02-tls-custom-cert",
    "description": "  # Deploy a PostgreSQL Cluster with Custom TLS Certificates on KubeBlocks  This guide demonstrates how to deploy a PostgreSQL cluster with **custom TLS certificates** using KubeBlocks. By providing your own certificates, you maintain complete control over the security configuration for encrypted cl"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for PostgreSQL Clusters with the Prometheus Operator",
    "content": "\n\n# PostgreSQL Monitoring with Prometheus Operator\n\nThis guide demonstrates how to configure comprehensive monitoring for PostgreSQL clusters in KubeBlocks using:\n\n1. Prometheus Operator for metrics collection\n2. Built-in PostgreSQL exporter for metrics exposure\n3. Grafana for visualization\n\n## Prerequisites\n\n\n\n## Install Monitoring Stack\n\n### 1. Install Prometheus Operator\nDeploy the kube-prometheus-stack using Helm:\n\n```bash\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  -n monitoring \\\n  --create-namespace\n```\n\n### 2. Verify Installation\nCheck all components are running:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-jmxg8                1/1     Running   0          2m1s\n```\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n**Key Monitoring Configuration**\n- `disableExporter: false` enables the built-in metrics exporter\n- Exporter runs as sidecar container in each PostgreSQL pod\n- Scrapes PostgreSQL metrics on port 9187\n\n## Verifying the Deployment\nMonitor the cluster status until it transitions to the Running state",
    "path": "docs/preview/kubeblocks-for-postgresql/08-monitoring/01-integrate-with-prometheus-operator",
    "description": "  # PostgreSQL Monitoring with Prometheus Operator  This guide demonstrates how to configure comprehensive monitoring for PostgreSQL clusters in KubeBlocks using:  1. Prometheus Operator for metrics collection 2. Built-in PostgreSQL exporter for metrics exposure 3. Grafana for visualization  ## Prer"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql__tpl__create-pg-replication-cluster",
    "title": "_create-pg-replication-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing PostgreSQL clusters. Below is an example configuration for deploying a PostgreSQL cluster with 2 replicas (1 primary, 1 replicas).\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: pg-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: postgresql\n  topology: replication\n  componentSpecs:\n    - name: postgresql\n      serviceVersion: 16.4.0\n      labels:\n        apps.kubeblocks.postgres.patroni/scope: pg-cluster-postgresql\n      disableExporter: true\n      replicas: 2\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n",
    "path": "docs/preview/kubeblocks-for-postgresql/_tpl/_create-pg-replication-cluster",
    "description": "KubeBlocks uses a declarative approach for managing PostgreSQL clusters. Below is an example configuration for deploying a PostgreSQL cluster with 2 replicas (1 primary, 1 replicas).  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps.kubeblocks.io/v1 kind: Clust"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/preview/kubeblocks-for-postgresql/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-postgresql__tpl__verify-pg-replication-cluster",
    "title": "_verify-pg-replication-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster pg-cluster -n demo -w\n```\n\nExpected Output:\n\n```bash\nNAME         CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\npg-cluster   postgresql           Delete               Creating   50s\npg-cluster   postgresql           Delete               Running    4m2s\n```\nOnce the cluster status becomes Running, your PostgreSQL cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::",
    "path": "docs/preview/kubeblocks-for-postgresql/_tpl/_verify-pg-replication-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster pg-cluster -n demo -w ```  Expected Output:  ```bash NAME         CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE pg-cluster   postgresql           Delete               Creating   50s pg-cluste"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant_04-operations_01-stop-start-restart",
    "title": "Qdrant Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# Qdrant Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a Qdrant Cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a Qdrant Cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Retains persistent storage (PVCs)\n3. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\nOption 1: OpsRequest API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: qdrant-cluster-stop-ops\n  namespace: demo\nspec:\n  clusterName: qdrant-cluster\n  type: Stop\n```\n\n\n\nOption 2: Cluster API Patch\n\nModify the cluster spec directly by patching the stop field:\n\n\n```bash\nkubectl patch cluster qdrant-cluster -n demo --type='json' -p='[\n\n]'\n```\n\n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster qdrant-cluster -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME             CLUSTER-DEFINITION  TERMINATION-POLICY   STATUS     AGE\n    qdrant-cluster   qdrant              Delete               Stopping   6m3s\n    qdrant-cluster   qdrant              Delete               Stopped    6m55s\n    ```\n\n2. Verify no running po",
    "path": "docs/preview/kubeblocks-for-qdrant/04-operations/01-stop-start-restart",
    "description": "  # Qdrant Cluster Lifecycle Management  This guide demonstrates how to manage a Qdrant Cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help optimize resource usage "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a Qdrant Cluster",
    "content": "\n\n\n\n# Vertical Scaling for Qdrant Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a Qdrant Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for Qdrant instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks ensures minimal impact during scaling operations by following a controlled, role-aware update strategy:\n**Role-Aware Replicas (Primary/Secondary Replicas)**\n- Secondary replicas update first – Non-leader pods are upgraded to minimize disruption.\n- Primary updates last – Only after all secondaries are healthy does the primary pod restart.\n- Cluster state progresses from Updating → Running once all replicas are stable.\n\n**Role-Unaware Replicas (Ordinal-Based Scaling)**\nIf replicas have no defined roles, updates follow Kubernetes pod ordinal order:\n- Highest ordinal first (e.g., pod-2 → pod-1 → pod-0) to ensure deterministic rollouts.\n\n## Prerequisites\n\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Pods are updated in pod ordinal order, from highest to lowest, (e.g., pod-2 → pod-1 → pod-0)\n1. Cluster status transitions from `Updating` to `Running`\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the qdrant component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: qdrant-cluster-vscale-ops\n    namespace: demo\n  spec:\n    clusterName: qdrant-cluster\n    type: VerticalScaling\n    verticalScaling:\n    - componentName: qdrant\n      requests:\n        cpu: '1'\n        memory: 1Gi\n      limits:\n        cpu: '1'\n        memory: 1Gi\n  ```\n\n ",
    "path": "docs/preview/kubeblocks-for-qdrant/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for Qdrant Clusters with KubeBlocks  This guide demonstrates how to vertically scale a Qdrant Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies compute resources (CPU and memo"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of Qdrant Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for Qdrant Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a Qdrant cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running`.\n2. Cluster status changes from `Updating` to `Running`\n\n:::note\n\nQdrant uses the **Raft consensus protocol** to maintain consistency regarding the cluster topology and the collections structure.\nBetter to have an odd number of replicas, such as 3, 5, 7, to avoid split-brain scenarios, after scaling out/in the cluster.\n\n:::\n\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the Qdrant cluster by adding 1 replica to qdrant component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: qdrant-cluster-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: qdrant-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: qdrant\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops qdrant-cluster-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME                           TYPE                CLUSTER          STATUS    PROGRESS   AGE\n  qdrant-cluster-scale-out-ops   HorizontalScaling   qdrant-cluster   Running   0/1        9s\n  qdrant-cluster-scale-out-ops   HorizontalScaling   qdrant-cluster   Running   1/1        16s\n  qdrant-cluster-scale-out-ops   HorizontalScaling   qdrant-cluster   Succeed   1/1        16s\n  ```\n  \n\n\n  Option 2: Direct Cluster",
    "path": "docs/preview/kubeblocks-for-qdrant/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for Qdrant Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a Qdrant cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequisites   "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a Qdrant Cluster",
    "content": "\n\n\n\n# Expanding Volume in a Qdrant Cluster\n\nThis guide explains how to expand Persistent Volume Claims (PVCs) in a Qdrant cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the underlying storage class, this operation can be performed without downtime.\n\nVolume expansion allows you to increase the size of a Persistent Volume Claim (PVC) after it has been created. This feature was introduced in Kubernetes v1.11 and became generally available (GA) in Kubernetes v1.24.\n\n## Prerequisites\n\n\n\n### Check the Storage Class for Volume Expansion Support\n\nList all available storage classes and verify if volume expansion is supported by checking the `ALLOWVOLUMEEXPANSION` field:\n```bash\nkubectl get storageclass\n```\n\nExample Output:\n```bash\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4d10h\nkb-default-sc       ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   3d7h\nsc-s3-repo-2qsxfh   ru.yandex.s3.csi        Retain          Immediate              false                  3d7h\n```\nEnsure the storage class you are using has `ALLOWVOLUMEEXPANSION` set to true. If it is false, the storage class does not support volume expansion.\n\n## Deploy a Qdrant Cluster with StorageClass\n\nKubeBlocks uses a declarative approach to manage Qdrant clusters. Below is an example configuration for deploying a Qdrant cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: qdrant-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: qdrant\n  topology: cluster\n  componentSpecs:\n    - name: qdrant\n      serviceVersion: 1.10.0\n      replicas: 3\n      resources:\n     ",
    "path": "docs/preview/kubeblocks-for-qdrant/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a Qdrant Cluster  This guide explains how to expand Persistent Volume Claims (PVCs) in a Qdrant cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the un"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy Qdrant Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage Qdrant Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing Qdrant services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## View Network Services\nList the Services created for the Qdrant cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=qdrant-cluster -n demo\n```\n\nExample Services:\n```bash\nNAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\nqdrant-cluster-qdrant-qdrant   ClusterIP   10.96.111.81           6333/TCP,6334/TCP   28m\n```\n\n## Expose Qdrant Service\n\nExternal service addresses enable public internet access to Qdrant, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the Qdrant service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: qdrant-cluster-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: qdrant-cluster\n    expose:\n    - componentName: qdrant\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options are 'ClusterIP', 'NodePort', and 'LoadBalancer'.\n        serviceType: LoadBalancer\n        # Contains cloud provider related parameters if ServiceType is LoadBala",
    "path": "docs/preview/kubeblocks-for-qdrant/04-operations/05-manage-loadbalancer",
    "description": "    # Manage Qdrant Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing Qdrant services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant_04-operations_06-minior-version-upgrade",
    "title": "Upgrading the Minor Version of a Qdrant Cluster in KubeBlocks",
    "content": "\n\n# Upgrading the Minor Version of a Qdrant Cluster in KubeBlocks\n\nThis guide walks you through the deployment and minor version upgrade of a Qdrant Cluster managed by KubeBlocks, ensuring minimal downtime during the process.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a Qdrant Cluster\n\nKubeBlocks uses a declarative approach for managing Qdrant Clusters. Below is an example configuration for deploying a Qdrant Cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: qdrant-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: qdrant\n  topology: cluster\n  componentSpecs:\n    - name: qdrant\n      serviceVersion: 1.10.0\n      replicas: 3\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n\n## Verifying the Deployment\nMonitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster qdrant-cluster -n demo -w\n```\n\nExample Output:\n\n```bash\nNAME             CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nqdrant-cluster   qdrant               Delete               Creating   49",
    "path": "docs/preview/kubeblocks-for-qdrant/04-operations/06-minior-version-upgrade",
    "description": "  # Upgrading the Minor Version of a Qdrant Cluster in KubeBlocks  This guide walks you through the deployment and minor version upgrade of a Qdrant Cluster managed by KubeBlocks, ensuring minimal downtime during the process.  ## Prerequisites  Before proceeding, ensure the following: - Environment "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed Qdrant Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed Qdrant Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in Qdrant clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\nTo decommission a specific Pod (e.g., 'qdrant-cluster-qdrant-1'), you can use one of the following methods:\n\n\n\n  Option 1: Using OpsRequest\n\n  Create an OpsRequest to mark the Pod as offline:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: qdrant-cluster-decommission-ops\n    namespace: demo\n  spec:\n    clusterName: qdrant-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: qdrant\n      scaleIn:\n        onlineInstancesToOffline:\n          - 'qdrant-cluster-qdrant-1'  # Specifies the instance names that need to be taken offline\n  ```\n\n  #### Monitor the Decommissioni",
    "path": "docs/preview/kubeblocks-for-qdrant/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed Qdrant Clusters  This guide explains how to decommission (take offline) specific Pods in Qdrant clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workloa"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant_05-backup-restore_01-create-backuprepo",
    "title": "Create a Backup Repository for KubeBlocks",
    "content": "\n# Create a BackupRepo for KubeBlocks\n\nThis guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.\n\n## Prerequisites\n- AWS CLI configured with appropriate permissions to create S3 buckets.\n- kubectl access to your Kubernetes cluster.\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks)) and running in the kb-system namespace.\n\n## Step 1: Create S3 Bucket\n\nUse the AWS CLI to create an S3 bucket in your desired region. Replace `` with your target AWS region (e.g., `us-east-1`, `ap-southeast-1`).\n\n```bash\n aws s3api create-bucket --bucket kubeblocks-backup-repo --region  --create-bucket-configuration LocationConstraint=\n```\n\nExample (for us-west-1):\n```bash\naws s3api create-bucket \\\n  --bucket kubeblocks-backup-repo \\\n  --region us-west-1 \\\n  --create-bucket-configuration LocationConstraint=us-west-1\n```\n\nExample Output:\n\n```json\n\n```\n\nVerification:\nConfirm the bucket was created by listing its contents (it will be empty initially):\n\n```bash\naws s3 ls s3://kubeblocks-backup-repo\n```\n\n## Step 2: Create a Kubernetes Secret for AWS Credentials\n\nStore your AWS credentials securely in a Kubernetes Secret. Replace `` and `` with your actual AWS credentials:\n\n```bash\n# Create a secret to save the access key\nkubectl create secret generic s3-credential-for-backuprepo \\\n  --from-literal=accessKeyId= \\\n  --from-literal=secretAccessKey= \\\n  -n kb-system\n```\n\n## Step 3: Configure Backup Repository\n\nA BackupRepo is a custom resource that defines a storage repository for backups. In this step, you'll integrate your S3 bucket with KubeBlocks by creating a BackupRepo resource.\n\nApply the following YAML to create the BackupRepo. Replace fields(e.g., bucket name, region) with your specific settings.\n\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupRepo\nmetadata:\n  name: s3-repo\n  annotations:\n    # mark this backuprepo as default one\n    dataprotection.kubeblocks.io/is",
    "path": "docs/preview/kubeblocks-for-qdrant/05-backup-restore/01-create-backuprepo",
    "description": " # Create a BackupRepo for KubeBlocks  This guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.  ## Prerequisites - AWS CLI configured with appropriate permissions to create S3 buckets. - kubectl access to your Kubernetes cluster. -"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant_05-backup-restore_02-create-full-backup",
    "title": "Create a Full Backup for a Qdrant Cluster on KubeBlocks",
    "content": "\n\n\n# Create a Full Backup for Qdrant on KubeBlocks\n\nThis guide demonstrates how to create and validate full backups for Qdrant clusters on KubeBlocks using the `pg-basebackup` method through both:\n- The Backup API (direct backup operations)\n- The OpsRequest API (managed backup operations with enhanced monitoring)\n\nWe will cover how to restore data from a backup in the [Restore From Full Backup](./05-restoring-from-full-backup) guide.\n\n## Prerequisites\n\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Backup Prerequisites\n\nBefore creating backups, ensure:\n1. Backup repository is configured:\n   - `BackupRepo` resource exists\n   - Network connectivity between cluster and repository\n   - `BackupRepo` status shows \"Ready\"\n\n2. Cluster is ready:\n   - Cluster status is \"Running\"\n   - No ongoing operations (scaling, upgrades, etc.)\n\n## Identify Backup Configuration\n\nCheck available backup policies and schedules:\n\n```bash\n# List backup policies\nkubectl get backuppolicy -n demo -l app.kubernetes.io/instance=qdrant-cluster\n\n# List backup schedules\nkubectl get backupschedule -n demo -l app.kubernetes.io/instance=qdrant-cluster\n```\n\nExpected Output:\n```bash\nNAME                                  BACKUP-REPO   STATUS      AGE\nqdrant-cluster-qdrant-backup-policy                 Available   36m\n\nNAME                                    STATUS      AGE\nqdrant-cluster-qdrant-backup-schedule   Available   36m\n```\n\nView supported backup methods in the BackupPolicy CR 'qdrant-cluster-qdrant-backup-policy':\n\n```bash\nkubectl get backuppolicy qdrant-cluster-qdrant-backup-policy -n demo -oyaml | yq '.spec.backupMethods[].name'\n```\n**List of Backup methods**\n\nKubeBlocks Qdrant supports these backup methods:\n\n| Feature           | Method          | Description |\n|-------------|--------|------------|\n| Full Backup | datafile | uses HTTP API `snapshot` to create snapshot for all collections. |\n\n## Backup via Backup API\n\n### 1. Create On-Demand Backup\n\nThe `datafile` method backup ",
    "path": "docs/preview/kubeblocks-for-qdrant/05-backup-restore/02-create-full-backup",
    "description": "   # Create a Full Backup for Qdrant on KubeBlocks  This guide demonstrates how to create and validate full backups for Qdrant clusters on KubeBlocks using the `pg-basebackup` method through both: - The Backup API (direct backup operations) - The OpsRequest API (managed backup operations with enhanc"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant_05-backup-restore_03-scheduled-full-backup",
    "title": "Setting Up a Qdrant Cluster with Scheduled Backups in KubeBlocks",
    "content": "\n\n# Setting Up a Qdrant Cluster with Scheduled Backups in KubeBlocks\n\nThis guide demonstrates how to deploy a Qdrant cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.\n\n## Prerequisites\n\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Prerequisites for Backup\n\n1. Backup Repository Configured:\n   - Configured `BackupRepo`\n   - Network connectivity between cluster and repo, `BackupRepo` status is `Ready`\n\n2. Cluster is Running:\n   - Cluster must be in `Running` state\n   - No ongoing operations (scaling, upgrades etc.)\n\n## Configure Scheduled Backups\n\nKubeBlocks automatically creates a `BackupSchedule` resource when the cluster is created. Follow these steps to enable and configure scheduled backups:\n\n1. Verify the default backup schedule configuration:\n\n```bash\nkubectl get backupschedule qdrant-cluster-qdrant-backup-schedule  -n demo -oyaml\n```\n\nExample Output:\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupSchedule\nspec:\n  backupPolicyName: qdrant-cluster-Qdrant-backup-policy\n  schedules:\n  - backupMethod: datafile\n    # ┌───────────── minute (0-59)\n    # │ ┌───────────── hour (0-23)\n    # │ │ ┌───────────── day of month (1-31)\n    # │ │ │ ┌───────────── month (1-12)\n    # │ │ │ │ ┌───────────── day of week (0-6) (Sunday=0)\n    # │ │ │ │ │\n    # 0 18 * * *\n    # schedule this job every day at 6:00 PM (18:00).\n    cronExpression: 0 18 * * * # update the cronExpression to your need\n    enabled: true # set to `true` to schedule base backup periodically\n    retentionPeriod: 7d # set the retention period to your need\n```\n\n2. Enable and customize the backup schedule:\n```bash\nkubectl edit backupschedule qdrant-cluster-qdrant-backup-schedule -n demo\n```\n\nUpdate these key parameters:\n- `enabled`: Set to `true` to activate scheduled backups\n- `cronExpression`: Configure backup frequency using cron syntax\n- `retentionPeriod`: Set how long to keep backups (e.g., `7d`, `1mo`)\n\nExample configuration",
    "path": "docs/preview/kubeblocks-for-qdrant/05-backup-restore/03-scheduled-full-backup",
    "description": "  # Setting Up a Qdrant Cluster with Scheduled Backups in KubeBlocks  This guide demonstrates how to deploy a Qdrant cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.  ## Prerequisites    ## Deploy a Qdrant Cluster    ## Verifying the Deployment    ## Prere"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant_05-backup-restore_05-restoring-from-full-backup",
    "title": "Restore a Qdrant Cluster from Backup",
    "content": "\n\n# Restore a Qdrant Cluster from Backup\n\nThis guide demonstrates two methods to restore a Qdrant cluster from backup in KubeBlocks:\n\n1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations\n2. **OpsRequest API Method** - Enhanced operational control with progress monitoring\n\n## Prerequisites\n\n\n\n## Preparing for Restoration: Locate one Full Backup\nBefore restoring, ensure that there is a full backup available. The restoration process will use this backup to create a new Qdrant cluster.\n\n- Backup repository accessible from new cluster\n- Valid full backup in `Completed` state\n- Adequate CPU/memory resources\n- Sufficient storage capacity\n\nFind available full backups:\n\n```bash\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Full,app.kubernetes.io/instance=qdrant-cluster # get the list of full backups\n```\n\nPick ONE of the Backups whose status is `Completed`.\n\n## Option 1: Cluster Annotation Restoration\n\n### Step 1: Create Restored Cluster\nCreate a new cluster with restore configuration:\n\nKey parameters:\n- `kubeblocks.io/restore-from-backup` annotation\n- Backup name and namespace located from the previous steps\n\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: qdrant-cluster-restored\n  namespace: demo\n  annotations:\n    # NOTE: replace  with your backup\n    kubeblocks.io/restore-from-backup: '}'\nspec:\n  terminationPolicy: Delete\n  clusterDef: qdrant\n  topology: cluster\n  componentSpecs:\n    - name: qdrant\n      serviceVersion: 1.10.0\n      replicas: 3\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n\n### Step 2: Monitor Restoration\nTrack restore progress with:\n\n```bash\n# ",
    "path": "docs/preview/kubeblocks-for-qdrant/05-backup-restore/05-restoring-from-full-backup",
    "description": "  # Restore a Qdrant Cluster from Backup  This guide demonstrates two methods to restore a Qdrant cluster from backup in KubeBlocks:  1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations 2. **OpsRequest API Method** - Enhanced operational control with progress monito"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for Qdrant Clusters with the Prometheus Operator",
    "content": "\n\n# Qdrant Monitoring with Prometheus Operator\n\nThis guide demonstrates how to configure comprehensive monitoring for Qdrant clusters in KubeBlocks using:\n\n1. Prometheus Operator for metrics collection\n2. Built-in Qdrant exporter for metrics exposure\n3. Grafana for visualization\n\n## Prerequisites\n\n\n\n## Install Monitoring Stack\n\n### 1. Install Prometheus Operator\nDeploy the kube-prometheus-stack using Helm:\n\n```bash\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  -n monitoring \\\n  --create-namespace\n```\n\n### 2. Verify Installation\nCheck all components are running:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-jmxg8                1/1     Running   0          2m1s\n```\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Configure Metrics Collection\n\n### 1. Verify Exporter Endpoint\n\n```bash\nkubectl -n demo exec -it pods/qdrant-cluster-qdrant-0 -c kbagent -- \\\n  curl -s http://127.0.0.1:6333/metrics | head -n 50\n```\n\n### 2. Create PodMonitor\n```yaml\napiVersion: monitoring.coreos.com/v1\nkind: PodM",
    "path": "docs/preview/kubeblocks-for-qdrant/08-monitoring/01-integrate-with-prometheus-operator",
    "description": "  # Qdrant Monitoring with Prometheus Operator  This guide demonstrates how to configure comprehensive monitoring for Qdrant clusters in KubeBlocks using:  1. Prometheus Operator for metrics collection 2. Built-in Qdrant exporter for metrics exposure 3. Grafana for visualization  ## Prerequisites   "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant__tpl__create-cluster",
    "title": "_create-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing Qdrant Clusters.\nBelow is an example configuration for deploying a Qdrant Cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: qdrant-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: qdrant\n  topology: cluster\n  componentSpecs:\n    - name: qdrant\n      serviceVersion: 1.10.0\n      replicas: 3\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n",
    "path": "docs/preview/kubeblocks-for-qdrant/_tpl/_create-cluster",
    "description": "KubeBlocks uses a declarative approach for managing Qdrant Clusters. Below is an example configuration for deploying a Qdrant Cluster with 3 replicas.  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps.kubeblocks.io/v1 kind: Cluster metadata:   name: qdrant-clus"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/preview/kubeblocks-for-qdrant/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-qdrant__tpl__verify-cluster",
    "title": "_verify-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster qdrant-cluster -n demo -w\n```\n\nExpected Output:\n\n```bash\nkubectl get cluster qdrant-cluster -n demo\nNAME             CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nqdrant-cluster   qdrant              Delete               Creating   49s\nqdrant-cluster   qdrant              Delete               Running    62s\n```\n\nCheck the pod status and roles:\n```bash\nkubectl get pods -l app.kubernetes.io/instance=qdrant-cluster -n demo\n```\n\nExpected Output:\n```bash\nNAME                      READY   STATUS    RESTARTS   AGE\nqdrant-cluster-qdrant-0   2/2     Running   0          1m43s\nqdrant-cluster-qdrant-1   2/2     Running   0          1m28s\nqdrant-cluster-qdrant-2   2/2     Running   0          1m14s\n```\n\nOnce the cluster status becomes Running, your Qdrant cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::\n",
    "path": "docs/preview/kubeblocks-for-qdrant/_tpl/_verify-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster qdrant-cluster -n demo -w ```  Expected Output:  ```bash kubectl get cluster qdrant-cluster -n demo NAME             CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE qdrant-cluster   qdrant     "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-rabbitmq_04-operations_01-stop-start-restart",
    "title": "RabbitMQ  Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# RabbitMQ  Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a RabbitMQ  Cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a RabbitMQ  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a RabbitMQ  Cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Retains persistent storage (PVCs)\n3. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\nOption 1: OpsRequest API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: rabbitmq-cluster-stop-ops\n  namespace: demo\nspec:\n  clusterName: rabbitmq-cluster\n  type: Stop\n```\n\n\n\nOption 2: Cluster API Patch\n\nModify the cluster spec directly by patching the stop field:\n\n```bash\nkubectl patch cluster rabbitmq-cluster -n demo --type='json' -p='[\n\n]'\n```\n\n\n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster rabbitmq-cluster -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME               CLUSTER-DEFINITION    TERMINATION-POLICY   STATUS     AGE\n    rabbitmq-cluster   rabbitmq              Delete               Stopping   6m3s\n    rabbitmq-cluster   rabbitmq              Delete               Stopped    6m55s\n",
    "path": "docs/preview/kubeblocks-for-rabbitmq/04-operations/01-stop-start-restart",
    "description": "  # RabbitMQ  Cluster Lifecycle Management  This guide demonstrates how to manage a RabbitMQ  Cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help optimize resource "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-rabbitmq_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a RabbitMQ  Cluster",
    "content": "\n\n\n\n# Vertical Scaling for RabbitMQ  Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a RabbitMQ  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for RabbitMQ instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks ensures minimal impact during scaling operations by following a controlled, role-aware update strategy:\n**Role-Aware Replicas (Primary/Secondary Replicas)**\n- Secondary replicas update first – Non-leader pods are upgraded to minimize disruption.\n- Primary updates last – Only after all secondaries are healthy does the primary pod restart.\n- Cluster state progresses from Updating → Running once all replicas are stable.\n\n**Role-Unaware Replicas (Ordinal-Based Scaling)**\nIf replicas have no defined roles, updates follow Kubernetes pod ordinal order:\n- Highest ordinal first (e.g., pod-2 → pod-1 → pod-0) to ensure deterministic rollouts.\n\n## Prerequisites\n\n\n\n## Deploy a RabbitMQ  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Pods are updated in pod ordinal order, from highest to lowest, (e.g., pod-2 → pod-1 → pod-0)\n1. Cluster status transitions from `Updating` to `Running`\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the rabbitmq component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: rabbitmq-cluster-vscale-ops\n    namespace: demo\n  spec:\n    clusterName: rabbitmq-cluster\n    type: VerticalScaling\n    verticalScaling:\n    - componentName: rabbitmq\n      requests:\n        cpu: '1'\n        memory: 1Gi\n      limits:\n        cpu: '1'\n        m",
    "path": "docs/preview/kubeblocks-for-rabbitmq/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for RabbitMQ  Clusters with KubeBlocks  This guide demonstrates how to vertically scale a RabbitMQ  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies compute resources (CPU an"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-rabbitmq_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of RabbitMQ Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for RabbitMQ Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a RabbitMQ cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a RabbitMQ  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running`.\n2. Cluster status changes from `Updating` to `Running`\n\n:::note\n\nRabbitMQ quorum queue are designed based on the **Raft consensus algorithm**.\nBetter to have an odd number of replicas, such as 3, 5, 7, to avoid split-brain scenarios, after scaling out/in the cluster.\n\n:::\n\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the RabbitMQ cluster by adding 1 replica to rabbitmq component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: rabbitmq-cluster-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: rabbitmq-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: rabbitmq\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops rabbitmq-cluster-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME                             TYPE                CLUSTER          STATUS    PROGRESS   AGE\n  rabbitmq-cluster-scale-out-ops   HorizontalScaling   rabbitmq-cluster   Running   0/1        9s\n  rabbitmq-cluster-scale-out-ops   HorizontalScaling   rabbitmq-cluster   Running   1/1        16s\n  rabbitmq-cluster-scale-out-ops   HorizontalScaling   rabbitmq-cluster   Succeed   1/1        16s\n  ```\n  \n\n\n  Option 2: Direct Cluster API Update\n\n  Alte",
    "path": "docs/preview/kubeblocks-for-rabbitmq/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for RabbitMQ Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a RabbitMQ cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequisite"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-rabbitmq_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a RabbitMQ Cluster",
    "content": "\n\n\n\n# Expanding Volume in a RabbitMQ Cluster\n\nThis guide explains how to expand Persistent Volume Claims (PVCs) in a RabbitMQ cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the underlying storage class, this operation can be performed without downtime.\n\nVolume expansion allows you to increase the size of a Persistent Volume Claim (PVC) after it has been created. This feature was introduced in Kubernetes v1.11 and became generally available (GA) in Kubernetes v1.24.\n\n## Prerequisites\n\n\n\n### Check the Storage Class for Volume Expansion Support\n\nList all available storage classes and verify if volume expansion is supported by checking the `ALLOWVOLUMEEXPANSION` field:\n```bash\nkubectl get storageclass\n```\n\nExample Output:\n```bash\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4d10h\nkb-default-sc       ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   3d7h\nsc-s3-repo-2qsxfh   ru.yandex.s3.csi        Retain          Immediate              false                  3d7h\n```\nEnsure the storage class you are using has `ALLOWVOLUMEEXPANSION` set to true. If it is false, the storage class does not support volume expansion.\n\n## Deploy a RabbitMQ  Cluster with StorageClass\n\nKubeBlocks uses a declarative approach to manage RabbitMQ clusters. Below is an example configuration for deploying a RabbitMQ cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: rabbitmq-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: rabbitmq\n  topology: cluster\n  componentSpecs:\n    - name: rabbitmq\n      serviceVersion: 1.10.0\n      replicas: 3\n     ",
    "path": "docs/preview/kubeblocks-for-rabbitmq/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a RabbitMQ Cluster  This guide explains how to expand Persistent Volume Claims (PVCs) in a RabbitMQ cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by th"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-rabbitmq_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy RabbitMQ Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage RabbitMQ Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing RabbitMQ services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n## Deploy a RabbitMQ  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## View Network Services\nList the Services created for the RabbitMQ cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=rabbitmq-cluster -n demo\n```\n\nExample Services:\n```bash\nNAME                        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)              AGE\nrabbitmq-cluster-rabbitmq   ClusterIP   10.96.6.67           5672/TCP,15672/TCP   33m\n```\n\n## Expose RabbitMQ Service\n\nExternal service addresses enable public internet access to RabbitMQ, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the RabbitMQ service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: rabbitmq-cluster-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: rabbitmq-cluster\n    expose:\n    - componentName: rabbitmq\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options are 'ClusterIP', 'NodePort', and 'LoadBalancer'.\n        serviceType: LoadBalancer\n        ports:\n          - name: managment\n            port: 156",
    "path": "docs/preview/kubeblocks-for-rabbitmq/04-operations/05-manage-loadbalancer",
    "description": "    # Manage RabbitMQ Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing RabbitMQ services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, ma"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-rabbitmq_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed RabbitMQ Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed RabbitMQ Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in RabbitMQ clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a RabbitMQ Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\n\nBefore decommissioning a specific pod from a component, make sure this component has more than one replicas.\nIf not, please scale out the component ahead.\n\nE.g. you can patch the cluster CR with command, to declare there are 3 replicas in component querynode.\n\n```bash\nkubectl patch cluster milvus-cluster -n demo --type='json' -p='[\n  \n]'\n```\n\n\nTo decommission a specific Pod (e.g., 'rabbitmq-cluster-rabbitmq-1'), you can use one of the following methods:\n\n\n\n  Option 1: Using OpsRequest\n\n  Create an OpsRequest to mark the Pod as offline:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: ra",
    "path": "docs/preview/kubeblocks-for-rabbitmq/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed RabbitMQ Clusters  This guide explains how to decommission (take offline) specific Pods in RabbitMQ clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for wor"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-rabbitmq_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for RabbitMQ Clusters with the Prometheus Operator",
    "content": "\n\n# RabbitMQ Monitoring with Prometheus Operator\n\nThis guide demonstrates how to configure comprehensive monitoring for RabbitMQ clusters in KubeBlocks using:\n\n1. Prometheus Operator for metrics collection\n2. Built-in RabbitMQ exporter for metrics exposure\n3. Grafana for visualization\n\n## Prerequisites\n\n\n\n## Install Monitoring Stack\n\n### 1. Install Prometheus Operator\nDeploy the kube-prometheus-stack using Helm:\n\n```bash\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  -n monitoring \\\n  --create-namespace\n```\n\n### 2. Verify Installation\nCheck all components are running:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-jmxg8                1/1     Running   0          2m1s\n```\n\n\n## Deploy a RabbitMQ Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Configure Metrics Collection\n\n### 1. Verify Exporter Endpoint\n\n```bash\n# prot-forward\nkubectl -n demo port-forward pods/rabbitmq-cluster-rabbitmq-0  15692:15692\n# check metrics\ncurl -s http://127.0.0.1:15692/metrics | head -n 50\n```\n\n### 2. Create PodMonitor\n```yaml\napiV",
    "path": "docs/preview/kubeblocks-for-rabbitmq/08-monitoring/01-integrate-with-prometheus-operator",
    "description": "  # RabbitMQ Monitoring with Prometheus Operator  This guide demonstrates how to configure comprehensive monitoring for RabbitMQ clusters in KubeBlocks using:  1. Prometheus Operator for metrics collection 2. Built-in RabbitMQ exporter for metrics exposure 3. Grafana for visualization  ## Prerequisi"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-rabbitmq__tpl__create-cluster",
    "title": "_create-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing RabbitMQ  Clusters.\nBelow is an example configuration for deploying a RabbitMQ  Cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: rabbitmq-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: rabbitmq\n  topology: clustermode\n  componentSpecs:\n    - name: rabbitmq\n      serviceVersion: 3.13.7\n      replicas: 3\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n",
    "path": "docs/preview/kubeblocks-for-rabbitmq/_tpl/_create-cluster",
    "description": "KubeBlocks uses a declarative approach for managing RabbitMQ  Clusters. Below is an example configuration for deploying a RabbitMQ  Cluster with 3 replicas.  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps.kubeblocks.io/v1 kind: Cluster metadata:   name: rabbi"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-rabbitmq__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/preview/kubeblocks-for-rabbitmq/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-rabbitmq__tpl__verify-cluster",
    "title": "_verify-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster rabbitmq-cluster -n demo -w\n```\n\nExpected Output:\n\n```bash\nkubectl get cluster rabbitmq-cluster -n demo\nNAME               CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nrabbitmq-cluster   rabbitmq             Delete               Creating   15s\nrabbitmq-cluster   rabbitmq             Delete               Running    83s\n```\n\nCheck the pod status and roles:\n```bash\nkubectl get pods -l app.kubernetes.io/instance=rabbitmq-cluster -n demo\n```\n\nExpected Output:\n```bash\nNAME                          READY   STATUS    RESTARTS   AGE\nrabbitmq-cluster-rabbitmq-0   2/2     Running   0          106s\nrabbitmq-cluster-rabbitmq-1   2/2     Running   0          82s\nrabbitmq-cluster-rabbitmq-2   2/2     Running   0          47s\n```\n\nOnce the cluster status becomes Running, your RabbitMQ cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::\n",
    "path": "docs/preview/kubeblocks-for-rabbitmq/_tpl/_verify-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster rabbitmq-cluster -n demo -w ```  Expected Output:  ```bash kubectl get cluster rabbitmq-cluster -n demo NAME               CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE rabbitmq-cluster   rab"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_03-topologies_01-standlone",
    "title": "Deploying a Redis Standalone Cluster with KubeBlocks",
    "content": "\n# Deploying a Redis Standalone Cluster with KubeBlocks\n\nA standalone Redis deployment consists of a single Redis server instance running independently without any replication or clustering. It is the simplest and most lightweight deployment model.\n\n**Use Cases**\n- Development & testing environments.\n- Small applications with low traffic.\n\n## Prerequisites\n\n\n\n## Deploying the Redis Standalone Cluster\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: redis-standalone\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: redis    # set to reids\n  topology: standalone # set topology to standalone\n  componentSpecs:\n  - name: redis\n    replicas: 1       # set replica to 1\n    serviceVersion: 7.2.4\n    resources:\n      limits:\n        cpu: \"0.5\"\n        memory: \"0.5Gi\"\n      requests:\n        cpu: \"0.5\"\n        memory: \"0.5Gi\"\n    volumeClaimTemplates:\n      - name: data\n        spec:\n          accessModes:\n            - ReadWriteOnce\n          resources:\n            requests:\n              storage: 20Gi\n```\n\n**Key Configuration Details**:\n- `clusterDef: redis`: Specifies the ClusterDefinition CR for the cluster.\n- `topology: standalone`: Configures the cluster to use standalone topology.\n- `componentSpecs`: Defines the components in the cluster:\n  - Component 'redis':\n    - `serviceVersion: 7.2.4`: Specifies the version of the Redis service to be deployed.\n\n\n## Verifying the Deployment\n\n### Check the Cluster Status\nOnce the cluster is deployed, check its status:\n```bash\nkubectl get cluster redis-standalone  -n demo -w\n```\nExpected Output:\n```bash\nNAME               CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS    AGE\nredis-standalone   redis                Delete               Running   34s\n```\n\n### Verify Component Status\n```bash\nkubectl get component redis-standalone-redis -n demo\n```\nExpected Output:\n```bash\nNAME                     DEFINITION              SERVICE-VERSION   STATUS    AGE\nredis-standalone-redis   redis-7-1.0.0   ",
    "path": "docs/preview/kubeblocks-for-redis/03-topologies/01-standlone",
    "description": " # Deploying a Redis Standalone Cluster with KubeBlocks  A standalone Redis deployment consists of a single Redis server instance running independently without any replication or clustering. It is the simplest and most lightweight deployment model.  **Use Cases** - Development & testing environments"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_03-topologies_02-replication",
    "title": "Deploying a Redis Replication Cluster with KubeBlocks",
    "content": "\n# Deploying a Redis Replication Cluster with KubeBlocks\n\nRedis Replication involves a primary (master) node that handles writes and one or more replica (slave) nodes that replicate data from the master for read scaling and failover.\n\n**Use Cases**\n- Read-heavy applications (e.g., analytics workload).\n- High-availability setups with Redis Sentinel for automatic failover.\n\n## Prerequisites\n\n\n\n## Deploying the Redis Replication Cluster\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: redis-replication\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: redis\n  topology: replication\n  componentSpecs:\n    - name: redis\n      serviceVersion: \"7.2.4\"\n      disableExporter: false\n      replicas: 2\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n    - name: redis-sentinel\n      replicas: 3\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n\n**Key Configuration Details**:\n- `clusterDef: redis`: Specifies the ClusterDefinition CR for the cluster.\n- `topology: replication`: Configures the cluster to use replication topology.\n- `componentSpecs`: Defines the components in the cluster:\n  - Component 'redis':\n    - `serviceVersion: 7.2.4`: Specifies the version of the Redis service to be deployed.\n  - Component 'redis-sentinel':\n    - Redis Sentinel is a high availability solution for Redis. Recommended to ",
    "path": "docs/preview/kubeblocks-for-redis/03-topologies/02-replication",
    "description": " # Deploying a Redis Replication Cluster with KubeBlocks  Redis Replication involves a primary (master) node that handles writes and one or more replica (slave) nodes that replicate data from the master for read scaling and failover.  **Use Cases** - Read-heavy applications (e.g., analytics workload"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_03-topologies_03-sharding",
    "title": "Deploying a Redis Sharding Cluster with KubeBlocks",
    "content": "\n# Deploying a Redis Sharding Cluste (Cluster Mode) with KubeBlocks\n\nRedis Cluster distributes data across multiple nodes (shards) using hash-based partitioning, allowing horizontal scaling for both reads and writes.\n\n**Use Cases**\n- Large-scale applications requiring high throughput.\n- Distributed caching and session storage.\n- Write-heavy workloads (e.g., real-time analytics).\n\n## Prerequisites\n\n\n\n## Deploying the Redis Sharding Cluster\n\nTo create a redis sharding cluster (cluster mode)  with 3 shards, and 2 replica for each shard:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: redis-sharding\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  shardings:\n  - name: shard\n    shards: 3\n    template:\n      name: redis\n      componentDef: redis-cluster-7\n      disableExporter: true\n      replicas: 2\n      resources:\n        limits:\n          cpu: '1'\n          memory: 1.1Gi\n        requests:\n          cpu: '1'\n          memory: 1.1Gi\n      serviceVersion: 7.2.4\n      volumeClaimTemplates:\n      - name: data\n        spec:\n          accessModes:\n          - ReadWriteOnce\n          resources:\n            requests:\n              storage: 20Gi\n      services:\n      - name: redis-advertised # This is a per-pod svc, and will be used to parse advertised endpoints\n        podService: true\n        #  - NodePort\n        #  - LoadBalancer\n        serviceType: NodePort\n```\n\n**Key Configuration Details**:\n- `shardings`: Specifies a list of ShardingSpec objects that configure the sharding topology for components of a Cluster.\n\n## Verifying the Deployment\n\n### Check the Cluster Status\nOnce the cluster is deployed, check its status:\n```bash\nkubectl get cluster redis-sharding  -n demo -w\n```\nExpected Output:\n```bash\nNAME             CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS    AGE\nredis-sharding                        Delete               Running   103s\n```\n\n### Verify Component and Pod Status\n\nGet all componets working for this cluster:\n```bash\nkub",
    "path": "docs/preview/kubeblocks-for-redis/03-topologies/03-sharding",
    "description": " # Deploying a Redis Sharding Cluste (Cluster Mode) with KubeBlocks  Redis Cluster distributes data across multiple nodes (shards) using hash-based partitioning, allowing horizontal scaling for both reads and writes.  **Use Cases** - Large-scale applications requiring high throughput. - Distributed "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_04-operations_01-stop-start-restart",
    "title": "Redis Replication Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# Redis Replication Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a Redis Replication Cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a Redis Replication Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a Redis Replication Cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Retains persistent storage (PVCs)\n3. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\nOption 1: OpsRequest API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: redis-replication-stop-ops\n  namespace: demo\nspec:\n  clusterName: redis-replication\n  type: Stop\n```\n\n\n\nOption 2: Cluster API Patch\n\nModify the cluster spec directly by patching the stop field:\n\n```bash\nkubectl patch cluster redis-replication -n demo --type='json' -p='[\n,\n\n]'\n```\n\n  \n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster redis-replication -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME                CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\n    redis-replication   redis                Delete               Stopping   6m3s\n    redis-replication   redis              ",
    "path": "docs/preview/kubeblocks-for-redis/04-operations/01-stop-start-restart",
    "description": "  # Redis Replication Cluster Lifecycle Management  This guide demonstrates how to manage a Redis Replication Cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help op"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a Redis Replication Cluster",
    "content": "\n\n\n\n# Vertical Scaling for Redis Replication Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a Redis Replication Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for Redis instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks orchestrates scaling with minimal impact:\n1. Secondary replicas update first\n2. Primary updates last after secondaries are healthy\n3. Cluster status transitions from `Updating` to `Running`\n\n## Prerequisites\n\n\n\n## Deploy a Redis Replication Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Secondary replicas are updated first (one at a time)\n1. Primary is updated last after secondary replicas are healthy\n1. Cluster status transitions from `Updating` to `Running`\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the redis component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: redis-replication-vscale-ops\n    namespace: demo\n  spec:\n    clusterName: redis-replication\n    type: VerticalScaling\n    verticalScaling:\n    - componentName: redis\n      requests:\n        cpu: '1'\n        memory: 1Gi\n      limits:\n        cpu: '1'\n        memory: 1Gi\n  ```\n  What Happens During Vertical Scaling?\n  - Secondary Pods are recreated first to ensure the primary Pod remains available.\n  - Once all secondary Pods are updated, the primary Pod is restarted with the new resource configuration.\n\n\n  You can check the progress of the scaling operation with the following command:\n\n  ```bash\n  kubectl -n demo get ops redis-replication-vscale-ops -w\n  ```\n\n ",
    "path": "docs/preview/kubeblocks-for-redis/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for Redis Replication Clusters with KubeBlocks  This guide demonstrates how to vertically scale a Redis Replication Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies compute r"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of Redis Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for Redis Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a Redis cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a Redis Replication Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running` with `secondary` role\n2. Data synced from primary to new replica\n3. Cluster status changes from `Updating` to `Running`\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the Redis cluster by adding 1 replica to redis component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: redis-replication-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: redis-replication\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: redis\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops redis-replication-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME                              TYPE                CLUSTER             STATUS    PROGRESS   AGE\n  redis-replication-scale-out-ops   HorizontalScaling   redis-replication   Running   0/1        9s\n  redis-replication-scale-out-ops   HorizontalScaling   redis-replication   Running   1/1        20s\n  redis-replication-scale-out-ops   HorizontalScaling   redis-replication   Succeed   1/1        20s\n  ```\n  \n\n\n  Option 2: Direct Cluster API Update\n\n  Alternatively, you can perform a direct update to the `replicas` field in the Cluster resource:\n\n  ```yaml\n  apiVersion: apps.kubeblocks.io/v1\n  kind: Cl",
    "path": "docs/preview/kubeblocks-for-redis/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for Redis Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a Redis cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequisites    #"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a Redis Cluster",
    "content": "\n\n\n\n# Expanding Volume in a Redis Cluster\n\nThis guide explains how to expand Persistent Volume Claims (PVCs) in a Redis cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the underlying storage class, this operation can be performed without downtime.\n\nVolume expansion allows you to increase the size of a Persistent Volume Claim (PVC) after it has been created. This feature was introduced in Kubernetes v1.11 and became generally available (GA) in Kubernetes v1.24.\n\n## Prerequisites\n\n\n\n### Check the Storage Class for Volume Expansion Support\n\nList all available storage classes and verify if volume expansion is supported by checking the `ALLOWVOLUMEEXPANSION` field:\n```bash\nkubectl get storageclass\n```\n\nExample Output:\n```bash\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4d10h\nkb-default-sc       ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   3d7h\nsc-s3-repo-2qsxfh   ru.yandex.s3.csi        Retain          Immediate              false                  3d7h\n```\nEnsure the storage class you are using has `ALLOWVOLUMEEXPANSION` set to true. If it is false, the storage class does not support volume expansion.\n\n## Deploy a Redis Replication Cluster with StorageClass\n\nKubeBlocks uses a declarative approach to manage Redis clusters. Below is an example configuration for deploying a Redis cluster with 2 replicas (1 primary, 1 secondary).\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: redis-replication\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: redis\n  topology: replication\n  componentSpecs:\n    - name: redis\n      serviceVersion: \"7.2.4\"\n  ",
    "path": "docs/preview/kubeblocks-for-redis/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a Redis Cluster  This guide explains how to expand Persistent Volume Claims (PVCs) in a Redis cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the unde"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy Redis Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage Redis Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing Redis services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n## Deploy a Redis Replication Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## View Network Services\nList the Services created for the Redis cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=redis-replication -n demo\n```\n\nExample Services:\n```bash\nNAME                                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE\nredis-replication-redis-redis                     ClusterIP   10.96.102.140           6379/TCP    31s\nredis-replication-redis-sentinel-redis-sentinel   ClusterIP   10.96.157.4             26379/TCP   51s\n```\n\n## Expose Redis Service\n\nExternal service addresses enable public internet access to Redis, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the Redis service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: redis-replication-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: redis-replication\n    expose:\n    - componentName: redis\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options are 'ClusterIP', 'NodeP",
    "path": "docs/preview/kubeblocks-for-redis/04-operations/05-manage-loadbalancer",
    "description": "    # Manage Redis Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing Redis services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage i"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_04-operations_07-modify-parameters",
    "title": "Modify Redis Parameters",
    "content": "\n# Modify Redis Parameters\n\nDatabase reconfiguration involves modifying parameters, settings, or configurations to optimize performance, security, or availability. Parameter changes fall into two categories:\n\n| Type | Restart Required | Scope | Example Parameters |\n|------|------------------|-------|--------------------|\n| **Dynamic** | No | Immediate effect | `max_connections` |\n| **Static** | Yes | After restart | `shared_buffers` |\n\nFor static parameters, KubeBlocks minimizes downtime by:\n1. Modifying and restarting replica nodes first\n2. Performing a switchover to promote the updated replica as primary (typically completes in milliseconds)\n3. Restarting the original primary node\n\n:::note\n\nKubeBlocks Redis Addon does not implement any dynamic reload action for `Dynamic Parameters`, thus changes on any parameters will cause a restart.\n\n:::\n\nThis guide demonstrates how to modify static parameters of a Redis cluster managed by KubeBlocks using a Reconfiguring OpsRequest.\n\n## Prerequisites\n\n\n\n## Deploy a Redis Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Check Parameter Values\n\n### Retrieve Credentials\nKubeBlocks automatically creates a secret containing the Redis root credentials. Retrieve the credentials with the following commands:\n```bash\nNAME=`kubectl get secrets -n demo redis-replication-redis-account-default -o jsonpath='' | base64 -d`\nPASSWD=`kubectl get secrets -n demo redis-replication-redis-account-default -o jsonpath='' | base64 -d`\n```\n\n### Access Redis Cluster\nTo connect to the cluster's primary node, use the Redis client:\n```bash\nkubectl exec -it -n demo redis-replication-redis-0 -c redis -- redis-cli -a $\n```\n\n### Query Parameter Values\n\nOnce connected, you can query the current value of 'max_connections' and 'shared_buffers':\n```sql\n127.0.0.1:6379> CONFIG GET aof-timestamp-enabled\n1) \"aof-timestamp-enabled\"\n2) \"no\"\n```\n\n## Static Parameter Example: Modifying aof-timestamp-enabled\n\nCreate a Reconfigure OpsRequest. Apply the following OpsRequest YAML t",
    "path": "docs/preview/kubeblocks-for-redis/04-operations/07-modify-parameters",
    "description": " # Modify Redis Parameters  Database reconfiguration involves modifying parameters, settings, or configurations to optimize performance, security, or availability. Parameter changes fall into two categories:  | Type | Restart Required | Scope | Example Parameters | |------|------------------|-------"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_04-operations_08-switchover",
    "title": "Redis Cluster Switchover",
    "content": "\n\n# Redis Cluster Switchover\n\nA **switchover** is a planned operation that transfers the primary role from one Redis instance to another. Unlike failover which occurs during failures, switchover provides:\n- Controlled role transitions\n- Minimal downtime (typically a few hundred milliseconds)\n- Predictable maintenance windows\n\nSwitchover is ideal for:\n- Node maintenance/upgrades\n- Workload rebalancing\n- Testing high availability\n- Planned infrastructure changes\n\n## Prerequisites\n\n\n\n## Deploy a Redis Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Check Roles\nList the Pods and their roles (primary or secondary):\n\n```bash\nkubectl get pods -n demo -l app.kubernetes.io/instance=redis-replication,apps.kubeblocks.io/component-name=redis -L kubeblocks.io/role\n```\n\nExample Output:\n\n```text\nNAME                      READY   STATUS    RESTARTS   AGE     ROLE\nredis-replication-redis-0   4/4     Running   0          9m59s   primary\nredis-replication-redis-1   4/4     Running   0          11m     secondary\n```\n\n## Performing a Planned Switchover\n\nTo initiate a planned switchover, create an OpsRequest resource as shown below:\n\n  Option 1: Automatic Switchover (No preferred candidate)\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: redis-switchover-ops\n    namespace: demo\n  spec:\n    clusterName: redis-replication\n    type: Switchover\n    switchover:\n    - componentName: redis\n      instanceName: redis-replication-redis-0\n  ```\n **Key Parameters:**\n  - `instanceName`: Specifies the instance (Pod) that is primary or leader before a switchover operation.\n\n  \n  Option 2: Targeted Switchover (Specific candidate)\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: redis-switchover-targeted\n    namespace: demo\n  spec:\n    clusterName: redis-replication\n    type: Switchover\n    switchover:\n    - componentName: redis\n      # Specifies the instance whose role will be transferred.\n      # A typic",
    "path": "docs/preview/kubeblocks-for-redis/04-operations/08-switchover",
    "description": "  # Redis Cluster Switchover  A **switchover** is a planned operation that transfers the primary role from one Redis instance to another. Unlike failover which occurs during failures, switchover provides: - Controlled role transitions - Minimal downtime (typically a few hundred milliseconds) - Predi"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed Redis Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed Redis Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in Redis clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a Redis Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\nTo decommission a specific Pod (e.g., 'redis-replication-redis-1'), you can use one of the following methods:\n\n\n\n  Option 1: Using OpsRequest\n\n  Create an OpsRequest to mark the Pod as offline:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: redis-replication-decommission-ops\n    namespace: demo\n  spec:\n    clusterName: redis-replication\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: redis\n      scaleIn:\n        onlineInstancesToOffline:\n          - 'redis-replication-redis-1'  # Specifies the instance names that need to be taken offline\n  ```\n\n  #### Monitor the Decommi",
    "path": "docs/preview/kubeblocks-for-redis/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed Redis Clusters  This guide explains how to decommission (take offline) specific Pods in Redis clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_05-backup-restore_01-create-backuprepo",
    "title": "Create a Backup Repository for KubeBlocks",
    "content": "\n# Create a BackupRepo for KubeBlocks\n\nThis guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.\n\n## Prerequisites\n- AWS CLI configured with appropriate permissions to create S3 buckets.\n- kubectl access to your Kubernetes cluster.\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks)) and running in the kb-system namespace.\n\n## Step 1: Create S3 Bucket\n\nUse the AWS CLI to create an S3 bucket in your desired region. Replace `` with your target AWS region (e.g., `us-east-1`, `ap-southeast-1`).\n\n```bash\n aws s3api create-bucket --bucket kubeblocks-backup-repo --region  --create-bucket-configuration LocationConstraint=\n```\n\nExample (for us-west-1):\n```bash\naws s3api create-bucket \\\n  --bucket kubeblocks-backup-repo \\\n  --region us-west-1 \\\n  --create-bucket-configuration LocationConstraint=us-west-1\n```\n\nExample Output:\n\n```json\n\n```\n\nVerification:\nConfirm the bucket was created by listing its contents (it will be empty initially):\n\n```bash\naws s3 ls s3://kubeblocks-backup-repo\n```\n\n## Step 2: Create a Kubernetes Secret for AWS Credentials\n\nStore your AWS credentials securely in a Kubernetes Secret. Replace `` and `` with your actual AWS credentials:\n\n```bash\n# Create a secret to save the access key\nkubectl create secret generic s3-credential-for-backuprepo \\\n  --from-literal=accessKeyId= \\\n  --from-literal=secretAccessKey= \\\n  -n kb-system\n```\n\n## Step 3: Configure Backup Repository\n\nA BackupRepo is a custom resource that defines a storage repository for backups. In this step, you'll integrate your S3 bucket with KubeBlocks by creating a BackupRepo resource.\n\nApply the following YAML to create the BackupRepo. Replace fields(e.g., bucket name, region) with your specific settings.\n\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupRepo\nmetadata:\n  name: s3-repo\n  annotations:\n    # mark this backuprepo as default one\n    dataprotection.kubeblocks.io/is",
    "path": "docs/preview/kubeblocks-for-redis/05-backup-restore/01-create-backuprepo",
    "description": " # Create a BackupRepo for KubeBlocks  This guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.  ## Prerequisites - AWS CLI configured with appropriate permissions to create S3 buckets. - kubectl access to your Kubernetes cluster. -"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_05-backup-restore_02-create-full-backup",
    "title": "Create a Full Backup for a Redis Cluster on KubeBlocks",
    "content": "\n\n\n# Create a Full Backup for Redis on KubeBlocks\n\nThis guide demonstrates how to create and validate full backups for Redis clusters on KubeBlocks using the `pg-basebackup` method through both:\n- The Backup API (direct backup operations)\n- The OpsRequest API (managed backup operations with enhanced monitoring)\n\nWe will cover how to restore data from a backup in the [Restore From Full Backup](./05-restoring-from-full-backup) guide.\n\n## Prerequisites\n\n\n\n## Deploy a Redis Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Backup Prerequisites\n\nBefore creating backups, ensure:\n1. Backup repository is configured:\n   - `BackupRepo` resource exists\n   - Network connectivity between cluster and repository\n   - `BackupRepo` status shows \"Ready\"\n\n2. Cluster is ready:\n   - Cluster status is \"Running\"\n   - No ongoing operations (scaling, upgrades, etc.)\n\n## Identify Backup Configuration\n\nCheck available backup policies and schedules:\n\n```bash\n# List backup policies\nkubectl get backuppolicy -n demo -l app.kubernetes.io/instance=redis-replication\n\n# List backup schedules\nkubectl get backupschedule -n demo -l app.kubernetes.io/instance=redis-replication\n```\n\nExpected Output:\n```bash\nNAME                                    BACKUP-REPO   STATUS      AGE\nredis-replication-redis-backup-policy                 Available   17m\n\nNAME                                              STATUS      AGE\nredis-replication-redis-backup-schedule           Available   60m\n```\n\nView supported backup methods in the BackupPolicy CR 'redis-replication-redis-backup-policy':\n\n```bash\nkubectl get backuppolicy redis-replication-redis-backup-policy -n demo -oyaml | yq '.spec.backupMethods[].name'\n```\n**List of Backup methods**\n\nKubeBlocks Redis supports these backup methods:\n\n| Feature           | Method          | Description |\n|-------------|--------|------------|\n| Full Backup | datafile  | Uses `redis-cli BGSAVE` command to backup data |\n| Continuous Backup | aof | Continuously perform incremental backups by arc",
    "path": "docs/preview/kubeblocks-for-redis/05-backup-restore/02-create-full-backup",
    "description": "   # Create a Full Backup for Redis on KubeBlocks  This guide demonstrates how to create and validate full backups for Redis clusters on KubeBlocks using the `pg-basebackup` method through both: - The Backup API (direct backup operations) - The OpsRequest API (managed backup operations with enhanced"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_05-backup-restore_03-scheduled-full-backup",
    "title": "Setting Up a Redis Cluster with Scheduled Backups in KubeBlocks",
    "content": "\n\n# Setting Up a Redis Cluster with Scheduled Backups in KubeBlocks\n\nThis guide demonstrates how to deploy a Redis cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.\n\n## Prerequisites\n\n\n\n## Deploy a Redis Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Prerequisites for Backup\n\n1. Backup Repository Configured:\n   - Configured `BackupRepo`\n   - Network connectivity between cluster and repo, `BackupRepo` status is `Ready`\n\n2. Cluster is Running:\n   - Cluster must be in `Running` state\n   - No ongoing operations (scaling, upgrades etc.)\n\n## Configure Scheduled Backups\n\nKubeBlocks automatically creates a `BackupSchedule` resource when the cluster is created. Follow these steps to enable and configure scheduled backups:\n\n1. Verify the default backup schedule configuration:\n\n```bash\nkubectl get backupschedule redis-replication-redis-backup-schedule  -n demo -oyaml\n```\n\nExample Output:\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupSchedule\nspec:\n  backupPolicyName: redis-replication-redis-backup-policy\n  schedules:\n  - backupMethod: datafile\n    # ┌───────────── minute (0-59)\n    # │ ┌───────────── hour (0-23)\n    # │ │ ┌───────────── day of month (1-31)\n    # │ │ │ ┌───────────── month (1-12)\n    # │ │ │ │ ┌───────────── day of week (0-6) (Sunday=0)\n    # │ │ │ │ │\n    # 0 18 * * *\n    # schedule this job every day at 6:00 PM (18:00).\n    cronExpression: 0 18 * * * # update the cronExpression to your need\n    enabled: false # set to `true` to schedule base backup periodically\n    retentionPeriod: 7d # set the retention period to your need\n```\n\n2. Enable and customize the backup schedule:\n```bash\nkubectl edit backupschedule redis-replication-redis-backup-schedule -n demo\n```\n\nUpdate these key parameters:\n- `enabled`: Set to `true` to activate scheduled backups\n- `cronExpression`: Configure backup frequency using cron syntax\n- `retentionPeriod`: Set how long to keep backups (e.g., `7d`, `1mo`)\n\nExample configura",
    "path": "docs/preview/kubeblocks-for-redis/05-backup-restore/03-scheduled-full-backup",
    "description": "  # Setting Up a Redis Cluster with Scheduled Backups in KubeBlocks  This guide demonstrates how to deploy a Redis cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.  ## Prerequisites    ## Deploy a Redis Cluster    ## Verifying the Deployment    ## Prerequi"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_05-backup-restore_04-scheduled-continuous-backup",
    "title": "Setting Up a Redis Cluster with Scheduled Continuous Backup in KubeBlocks",
    "content": "\n# Setting Up a Redis Cluster with Scheduled Continuous Backup Enabled in KubeBlocks\n\nThis guide demonstrates how to configure a Redis cluster on KubeBlocks with:\n\n- Scheduled full backups (base backups)\n- Continuous WAL (Write-Ahead Log) archiving\n- Point-In-Time Recovery (PITR) capabilities\n\nThis combination provides comprehensive data protection with minimal recovery point objectives (RPO).\n\n## What is PITR?\nPoint-In-Time Recovery (PITR) allows you to restore a database to a specific moment in time by combining full backups with continuous binlog/wal/archive log backups.\n\nFor details on restoring data from both full backups and continuous binlog backups, refer to the [Restore From PITR](restore-with-pitr.mdx)  guide.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Prerequisites for Backup\n\n1. Backup Repository Configured:\n   - Configured `BackupRepo`\n   - Network connectivity between cluster and repo, `BackupRepo` status is `Ready`\n\n2. Cluster is Running:\n   - Cluster must be in `Running` state\n   - No ongoing operations (scaling, upgrades etc.)\n\n## List of Backup methods\n\nKubeBlocks Redis supports these backup methods:\n\n| Feature           | Method          | Description |\n|-------------|--------|------------|\n| Full Backup | datafile  | Uses `redis-cli BGSAVE` command to backup data |\n| Continuous Backup | aof | Continuously perform incremental backups by archiving Append-Only Files (AOF) |\n\n## Deploy a Redis Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Enabl",
    "path": "docs/preview/kubeblocks-for-redis/05-backup-restore/04-scheduled-continuous-backup",
    "description": " # Setting Up a Redis Cluster with Scheduled Continuous Backup Enabled in KubeBlocks  This guide demonstrates how to configure a Redis cluster on KubeBlocks with:  - Scheduled full backups (base backups) - Continuous WAL (Write-Ahead Log) archiving - Point-In-Time Recovery (PITR) capabilities  This "
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_05-backup-restore_05-restoring-from-full-backup",
    "title": "Restore a Redis Cluster from Backup",
    "content": "\n\n# Restore a Redis Cluster from Backup\n\nThis guide demonstrates two methods to restore a Redis cluster from backup in KubeBlocks:\n\n1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations\n2. **OpsRequest API Method** - Enhanced operational control with progress monitoring\n\n## Prerequisites\n\n\n\n## Preparing for Restoration: Locate one Full Backup\nBefore restoring, ensure that there is a full backup available. The restoration process will use this backup to create a new Redis cluster.\n\n- Backup repository accessible from new cluster\n- Valid full backup in `Completed` state\n- Adequate CPU/memory resources\n- Sufficient storage capacity\n\nFind available full backups:\n\n```bash\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Full,app.kubernetes.io/instance=redis-replication # get the list of full backups\n```\n\nPick ONE of the Backups whose status is `Completed`.\n\n## Option 1: Cluster Annotation Restoration\n\n### Step 1: Create Restored Cluster\nCreate a new cluster with restore configuration:\n\nKey parameters:\n- `kubeblocks.io/restore-from-backup` annotation\n- Backup name and namespace located from the previous steps\n\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: redis-replication-restore\n  namespace: demo\n  annotations:\n    kubeblocks.io/restore-from-backup: '}'\nspec:\n  terminationPolicy: Delete\n  clusterDef: redis\n  topology: replication\n  componentSpecs:\n    - name: redis\n      serviceVersion: \"7.2.4\"\n      disableExporter: false\n      replicas: 2\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n    - name: redis-sentinel\n      replicas: 3\n      resources:\n        limits:\n       ",
    "path": "docs/preview/kubeblocks-for-redis/05-backup-restore/05-restoring-from-full-backup",
    "description": "  # Restore a Redis Cluster from Backup  This guide demonstrates two methods to restore a Redis cluster from backup in KubeBlocks:  1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations 2. **OpsRequest API Method** - Enhanced operational control with progress monitori"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_05-backup-restore_06-restore-with-pitr",
    "title": "Restore a Redis Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks",
    "content": "\n\n# Restore a Redis Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks\n\nThis guide demonstrates how to perform Point-In-Time Recovery (PITR) for Redis clusters in KubeBlocks using:\n\n1. A full base backup\n2. Continuous WAL (Write-Ahead Log) backups\n3. Two restoration methods:\n   - Cluster Annotation (declarative approach)\n   - OpsRequest API (operational control)\n\nPITR enables recovery to any moment within the `timeRange` specified.\n\n## Prerequisites\n\n\n\n## Prepare for PITR Restoration\nTo perform a PITR restoration, both a full backup and continuous backup are required. Refer to the documentation to configure these backups if they are not already set up.\n\n- Completed full backup\n- Active continuous WAL backup\n- Backup repository accessible\n- Sufficient resources for new cluster\n\nTo identify the list of full and continuous backups, you may follow the steps:\n\n### 1. Verify Continuous Backup\nConfirm you have a continuous WAL backup, either running or completed:\n\n```bash\n# expect EXACTLY ONE continuous backup per cluster\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Continuous,app.kubernetes.io/instance=redis-replication\n```\n\n### 2. Check Backup Time Range\nGet the valid recovery window:\n\n```bash\nkubectl get backup  -n demo -o yaml | yq '.status.timeRange'\n```\n\nExpected Output:\n```text\nstart: \"2025-05-07T09:12:47Z\"\nend: \"2025-05-07T09:22:50Z\"\n```\n\n### 3. Identify Full Backup\nFind available full backups that meet:\n- Status: Completed\n- Completion time after continuous backup start time\n\n```bash\n# expect one or more Full backups\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Full,app.kubernetes.io/instance=redis-replication\n```\n\n:::tip\nKubeBlocks automatically selects the most recent qualifying full backup as the base.\nMake sure there is a full backup meets the condition: its `stopTime`/`completionTimestamp` must **AFTER** Continuous backup's `startTime`, otherwise PITR restoration will fail.\n:::\n\n## Option 1:",
    "path": "docs/preview/kubeblocks-for-redis/05-backup-restore/06-restore-with-pitr",
    "description": "  # Restore a Redis Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks  This guide demonstrates how to perform Point-In-Time Recovery (PITR) for Redis clusters in KubeBlocks using:  1. A full base backup 2. Continuous WAL (Write-Ahead Log) backups 3. Two restoration methods:    - Cl"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_06-custom-secret_01-custom-secret",
    "title": "Create a Redis Cluster with a Custom Root Password on KubeBlocks",
    "content": "\n# Create Redis Cluster With Custom Password on KubeBlocks\n\nThis guide demonstrates how to deploy a Redis cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.\n\n## Prerequisites\n\n\n\n## Deploying the Redis Replication Cluster\n\nKubeBlocks uses a declarative approach for managing Redis clusters. Below is an example configuration for deploying a Redis cluster with 2 nodes (1 primary, 1 replicas) and a custom root password.\n\n### Step 1: Create a Secret for the Defaults Account\n\nThe custom root password is stored in a Kubernetes Secret. Create the Secret by applying the following YAML:\n\n```yaml\napiVersion: v1\ndata:\n  password: Y3VzdG9tcGFzc3dvcmQ= # custompassword\n  username: cm9vdA== #root\nimmutable: true\nkind: Secret\nmetadata:\n  name: custom-secret\n  namespace: demo\n```\n- password: Replace custompassword with your desired password and encode it using Base64 (`echo -n \"custompassword\" | base64`).\n- username: The default Redis default user is 'default', encoded as 'cm9vdA=='.\n\n\n### Step 2: Deploy the Redis Cluster\n\nApply the following manifest to deploy the Redis cluster, referencing the Secret created in Step 1 for the root account:\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: redis-replication\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: redis\n  topology: replication\n  componentSpecs:\n    - name: redis\n      serviceVersion: \"7.2.4\"\n      disableExporter: false\n      replicas: 2\n      systemAccounts:  # override systemaccount password\n        - name: default\n          secretRef:\n            name: custom-secret\n            namespace: demo\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage:",
    "path": "docs/preview/kubeblocks-for-redis/06-custom-secret/01-custom-secret",
    "description": " # Create Redis Cluster With Custom Password on KubeBlocks  This guide demonstrates how to deploy a Redis cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.  ## Prerequisites    ## Deploying the Redis Replication Cluster  KubeBlocks uses a declarative approach for manag"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for Redis Clusters with the Prometheus Operator",
    "content": "\n\n# Redis Monitoring with Prometheus Operator\n\nThis guide demonstrates how to configure comprehensive monitoring for Redis clusters in KubeBlocks using:\n\n1. Prometheus Operator for metrics collection\n2. Built-in Redis exporter for metrics exposure\n3. Grafana for visualization\n\n## Prerequisites\n\n\n\n## Install Monitoring Stack\n\n### 1. Install Prometheus Operator\nDeploy the kube-prometheus-stack using Helm:\n\n```bash\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  -n monitoring \\\n  --create-namespace\n```\n\n### 2. Verify Installation\nCheck all components are running:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-jmxg8                1/1     Running   0          2m1s\n```\n\n\n## Deploy a Redis Cluster\n\n\n\n**Key Monitoring Configuration**\n- `disableExporter: false` enables the built-in metrics exporter\n- Exporter runs as sidecar container in each Redis pod\n- Scrapes Redis metrics on port 9187\n\n## Verifying the Deployment\nMonitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster ",
    "path": "docs/preview/kubeblocks-for-redis/08-monitoring/01-integrate-with-prometheus-operator",
    "description": "  # Redis Monitoring with Prometheus Operator  This guide demonstrates how to configure comprehensive monitoring for Redis clusters in KubeBlocks using:  1. Prometheus Operator for metrics collection 2. Built-in Redis exporter for metrics exposure 3. Grafana for visualization  ## Prerequisites    ##"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis__tpl__create-redis-replication-cluster",
    "title": "_create-redis-replication-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing Redis Replication Clusters.\nBelow is an example configuration for deploying a Redis Replication Cluster with two components, redis and redis sentinel.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: redis-replication\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: redis\n  topology: replication\n  componentSpecs:\n    - name: redis\n      serviceVersion: \"7.2.4\"\n      disableExporter: false\n      replicas: 2\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n    - name: redis-sentinel\n      replicas: 3\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n",
    "path": "docs/preview/kubeblocks-for-redis/_tpl/_create-redis-replication-cluster",
    "description": "KubeBlocks uses a declarative approach for managing Redis Replication Clusters. Below is an example configuration for deploying a Redis Replication Cluster with two components, redis and redis sentinel.  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps.kubebloc"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/preview/kubeblocks-for-redis/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_preview_kubeblocks-for-redis__tpl__verify-redis-replication-cluster",
    "title": "_verify-redis-replication-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster redis-replication -n demo -w\n```\n\nExpected Output:\n\n```bash\nNAME                CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS    AGE\nredis-replication   redis                Delete               Running   3m49s\n```\n\nCheck the pod status and roles:\n```bash\nkubectl get pods -l app.kubernetes.io/instance=redis-replication -L  kubeblocks.io/role -n demo\n```\n\nExpected Output:\n```bash\nNAME                                 READY   STATUS    RESTARTS   AGE     ROLE\nredis-replication-redis-0            3/3     Running   0          3m38s   primary\nredis-replication-redis-1            3/3     Running   0          3m16s   secondary\nredis-replication-redis-sentinel-0   2/2     Running   0          4m35s\nredis-replication-redis-sentinel-1   2/2     Running   0          4m17s\nredis-replication-redis-sentinel-2   2/2     Running   0          3m59s\n```\n\nOnce the cluster status becomes Running, your Redis cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::\n",
    "path": "docs/preview/kubeblocks-for-redis/_tpl/_verify-redis-replication-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster redis-replication -n demo -w ```  Expected Output:  ```bash NAME                CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS    AGE redis-replication   redis                Delete               Runn"
  },
  {
    "id": "docs_en_preview_user_docs_concepts_concept",
    "title": "Concepts",
    "content": "\n# Concepts\n\nYou've already seen the benefits of using unified APIs to represent various databases in the section [\"How Unified APIs Reduce Your Learning Curve\"](./../overview/introduction). If you take a closer look at those examples, you'll notice two key concepts in the sample YAML files: **Cluster** and **Component**. For instance, `test-mysql` is a Cluster that includes a Component called `mysql` (with a componentDef of `apecloud-mysql`). Similarly, `test-redis` is also a Cluster, and it includes two Components: one called `redis` (with a componentDef of `redis-7`), which has two replicas, and another called `redis-sentinel` (with a componentDef of `redis-sentinel`), which has three replicas.\n\nIn this document, we will explain the reasons behind these two concepts and provide a brief introduction to the underlying API (i.e., CRD).\n\n## Motivation of KubeBlocks’ Layered API\nIn KubeBlocks, to support the management of various databases through a unified API, we need to abstract the topologies and characteristics of different databases.\n\nWe’ve observed that database systems deployed in production environments often use a topology composed of multiple components. For example, a production MySQL cluster might include several Proxy nodes (such as ProxySQL, MaxScale, Vitess, WeScale, etc.) alongside multiple MySQL server nodes (like MySQL Community Edition, Percona, MariaDB, ApeCloud MySQL, etc.) to achieve higher availability and read-write separation. Similarly, Redis deployments typically consist of a primary node and multiple read replicas, managed for high availability via Sentinel. Some users even use twemproxy for horizontal sharding to achieve greater capacity and throughput.\n\nThis modular approach is even more pronounced in distributed database systems, where the entire system is divided into distinct components with clear and singular responsibilities, such as data storage, query processing, transaction management, logging, and metadata management. These comp",
    "path": "docs/preview/user_docs/concepts/concept",
    "description": " # Concepts  You've already seen the benefits of using unified APIs to represent various databases in the section [\"How Unified APIs Reduce Your Learning Curve\"](./../overview/introduction). If you take a closer look at those examples, you'll notice two key concepts in the sample YAML files: **Clust"
  },
  {
    "id": "docs_en_preview_user_docs_overview_install-kubeblocks",
    "title": "Installation",
    "content": "\n\n\n# KubeBlocks\n\nThis guide covers KubeBlocks deployment on existing Kubernetes clusters. Choose your preferred installation method:\n\n- **Helm** (recommended for production)\n- **kbcli** (simplified CLI experience)\n\n\n## Prerequisites\n\n### Resource Requirements\n| Component     | Database   | Recommendation |\n|--------------|------------|---------------|\n| **Control Plane** | - | 1 node (4 cores, 4GB RAM, 50GB storage) |\n| **Data Plane**    | MySQL | 2 nodes (2 cores, 4GB RAM, 50GB storage) |\n|                   | PostgreSQL | 2 nodes (2 cores, 4GB RAM, 50GB storage) |\n|                   | Redis | 2 nodes (2 cores, 4GB RAM, 50GB storage) |\n|                   | MongoDB | 3 nodes (2 cores, 4GB RAM, 50GB storage) |\n\n- **Control Plane**: Nodes running KubeBlocks components\n- **Data Plane**: Nodes hosting database instances\n\n\n### System Requirements\n\nBefore installation, verify your environment meets these requirements:\n\n- Kubernetes cluster (v1.21+ recommended) - [create test cluster](../references/prepare-a-local-k8s-cluster) if needed\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- Snapshot Controller installed ([installation guide](../references/install-snapshot-controller))\n\n## Install KubeBlocks\n\n\n\n```bash\n# Step 1: Install CRDs\nkubectl create -f https://github.com/apecloud/kubeblocks/releases/download/}/kubeblocks_crds.yaml\n\n# Step 2: Configure Helm Repository\nhelm repo add kubeblocks https://apecloud.github.io/helm-charts\nhelm repo update\n\n# Step 3: Deploy KubeBlocks\nhelm install kubeblocks kubeblocks/kubeblocks --namespace kb-system --create-namespace --version=}\n```\n\n\n:::note\n\nIf you are using K8s \\\n\n\n**Before You Begin**:\n- Install [KubeBlocks CLI](../../user_docs/references/install-kbcli)\n- Ensure kubectl is configured with cluster access\n\n```bash\nkbcli kubeblocks install --version=} --create-namespace\n```\n\n**Need a different version?**\n\nList available versions or fi",
    "path": "docs/preview/user_docs/overview/install-kubeblocks",
    "description": "   # KubeBlocks  This guide covers KubeBlocks deployment on existing Kubernetes clusters. Choose your preferred installation method:  - **Helm** (recommended for production) - **kbcli** (simplified CLI experience)   ## Prerequisites  ### Resource Requirements | Component     | Database   | Recommend"
  },
  {
    "id": "docs_en_preview_user_docs_overview_introduction",
    "title": "Introduction",
    "content": "\n\n# Introduction\n\n## What is KubeBlocks\n\nKubeBlocks is an open-source Kubernetes operator for databases (more specifically, for stateful applications, including databases and middleware like message queues), enabling users to run and manage multiple types of databases on Kubernetes. As far as we know, most database operators typically manage only one specific type of database. For example:\n- CloudNativePG, Zalando, CrunchyData, StackGres operator can manage PostgreSQL\n- Strimzi manages Kafka\n- Oracle and Percona MySQL operator manage MySQL\n\nIn contrast, KubeBlocks is designed to be a **general-purpose database operator**. This means that when designing the KubeBlocks API, we didn’t tie it to any specific database. Instead, we abstracted the common features of various databases, resulting in a universal, engine-agnostic API. Consequently, the operator implementation developed around this abstract API is also agnostic to the specific database engine.\n\n![Design of KubeBlocks, a general purpose database operator](/img/docs/en/kubeblocks_general_purpose_arch.png)\n\nIn above diagram, Cluster, Component, and InstanceSet are all CRDs provided by KubeBlocks. If you'd like to learn more about them, please refer to [concepts](../concepts/concept).\n\nKubeBlocks offers an Addon API to support the integration of various databases. For instance, we currently have the following KubeBlocks Addons for mainstream open-source database engines:\n- MySQL\n- PostgreSQL\n- Redis\n- MongoDB\n- Kafka\n- RabbitMQ\n- Minio\n- Elasticsearch\n- StarRocks\n- Qdrant\n- Milvus\n- ZooKeeper\n- etcd\n- ...\n\nFor a detailed list of Addons and their features, please refer to [supported addons](supported-addons.md).\n\nThe unified API makes KubeBlocks an excellent choice if you need to run multiple types of databases on Kubernetes. It can significantly reduce the learning curve associated with mastering multiple operators.\n\n## How unified APIs reduce your learning curve\n\nHere is an example of how to use KubeBlocks' Cluste",
    "path": "docs/preview/user_docs/overview/introduction",
    "description": "  # Introduction  ## What is KubeBlocks  KubeBlocks is an open-source Kubernetes operator for databases (more specifically, for stateful applications, including databases and middleware like message queues), enabling users to run and manage multiple types of databases on Kubernetes. As far as we kno"
  },
  {
    "id": "docs_en_preview_user_docs_overview_supported-addons",
    "title": "Supported Addons",
    "content": "\n# Supported Addons\n\nKubeBlocks uses Addons to extend support for various database engines. And there are currently over 30 Addons available in the KubeBlocks repository, which can be further categorized as follows sections.\n\nFor installing and enabling Addons, refer to [Addon installation tutorial](./../references/install-addons).\n\n## Relational Databases\n\nMySQL and PostgreSQL are the two most popular open-source relational databases in the world, and they have branches/variants.\n\n### MySQL and its variants\n\n**Addon List**\n\n| Addons          | Description                                                                                                                                                                                                                                                                                                                                                             |\n|:----------------|:---------------|\n| mysql           | This addon uses the community edition MySQL image officially released by Oracle.                                                    |\n| mariadb         | MariaDB is a high performance open source relational database management system that is widely used for web and application servers. |\n\n**Supported Features**\n\n:::note\n\nThe versions listed below may not be up-to-date, and some supported versions might be missing. For the latest addon versions, please refer to the [KubeBlocks addon GitHub repo](https://github.com/apecloud/kubeblocks-addons).\n\n:::\n\n| Addon (v0.9.0)     | Supported Versions                     | Vscale | Hscale | Volumeexpand | Stop/Start | Restart | Expose | Backup/Restore | Logs | Config | Upgrade (DB engine version) | Account | Failover | Switchover |\n|:------------------:|:--------------------------------------:|:------:|:------:|:------------:|:----------:|:-------:|:------:|:--------------:|:----:|:------:|:---------------------------:|:-------:|:--------:|:----------:|\n| mysql              | •",
    "path": "docs/preview/user_docs/overview/supported-addons",
    "description": " # Supported Addons  KubeBlocks uses Addons to extend support for various database engines. And there are currently over 30 Addons available in the KubeBlocks repository, which can be further categorized as follows sections.  For installing and enabling Addons, refer to [Addon installation tutorial]"
  },
  {
    "id": "docs_en_preview_user_docs_references_install-addons",
    "title": "Install Addons",
    "content": "\n\n# Install Addons\n\nWith the release of KubeBlocks v0.8.0, Addons are decoupled from KubeBlocks and some Addons are not installed by default. If you want to use these Addons, install Addons first by index. Or if you uninstalled some Addons, you can follow the steps in this tutorial to install them again.\n\nThis tutorial takes elasticsearch as an example. You can replace elasticsearch with the Addon you need.\n\nThe official index repo is [KubeBlocks index](https://github.com/apecloud/block-index). Addons are maintained in the [KubeBlocks Addon repo](https://github.com/apecloud/kubeblocks-addons).\n\n:::note\n\nMake sure the major version of Addons and KubeBlocks are the same.\n\nFor example, you can install an Addon v0.9.0 with KubeBlocks v0.9.2, but using mismatched major versions, such as an Addon v0.8.0 with KubeBlocks v0.9.2, may lead to errors.\n\n:::\n\n\n\n1. (Optional) Add the KubeBlocks repo. If you install KubeBlocks with Helm, just run `helm repo update`.\n\n   ```bash\n   helm repo add kubeblocks https://apecloud.github.io/helm-charts\n   helm repo update\n   ```\n\n2. View the Addon versions.\n\n   ```bash\n   helm search repo kubeblocks/elasticsearch --devel --versions\n   ```\n\n3. Install the Addon (take elasticsearch as example). Specify a version with `--version`.\n\n   ```bash\n   helm install kb-addon-es kubeblocks/elasticsearch --namespace kb-system --create-namespace --version \n   ```\n\n4. Verify whether this Addon is installed.\n\n   The STATUS is `deployed` and this Addon is installed successfully.\n\n   ```bash\n   helm list -A\n   >\n   NAME             NAMESPACE\tREVISION\tUPDATED                                STATUS  \t CHART                   APP VERSION\n   ...\n   kb-addon-es      kb-system\t1       \t2024-11-27 10:04:59.730127 +0800 CST   deployed\t elasticsearch-0.9.0     8.8.2\n   ```\n\n5. (Optional) You can run the command below to uninstall the Addon.\n\n   If you have created a related cluster, delete the cluster first.\n\n   ```bash\n   helm uninstall kb-addon-es --namespace kb-sy",
    "path": "docs/preview/user_docs/references/install-addons",
    "description": "  # Install Addons  With the release of KubeBlocks v0.8.0, Addons are decoupled from KubeBlocks and some Addons are not installed by default. If you want to use these Addons, install Addons first by index. Or if you uninstalled some Addons, you can follow the steps in this tutorial to install them a"
  },
  {
    "id": "docs_en_preview_user_docs_references_install-kbcli",
    "title": "Install and Configure the KubeBlocks CLI (kbcli)",
    "content": "\n\n# KubeBlocks Command Line (kbcli)\n\nkbcli is the official command-line tool for managing KubeBlocks clusters. It provides:\n- Cluster lifecycle management (create, scale, delete)\n- Configuration and troubleshooting tools\n- Version compatibility checks\n- Shell auto-completion support\n\n## Prerequisites\n\nBefore installing kbcli, ensure your system meets these requirements:\n\n- **All platforms**:\n  - Network access to download packages\n  - Administrator/sudo privileges\n- **Windows**:\n  - PowerShell 5.0 or later\n- **macOS/Linux**:\n  - curl or wget installed\n  - Homebrew (for macOS brew installation)\n\n## Install kbcli\n\n**Supported Platforms**\n\nkbcli is available for:\n- **macOS** (Intel and Apple Silicon)\n- **Windows** (x86-64)\n- **Linux** (x86-64 and ARM64)\n\n\nChoose your preferred installation method:\n\n- **curl** (recommended for most users)\n- **Homebrew** (macOS package manager)\n\n**Option 1: Install with curl**\n\nTo install the latest stable version:\n\n```bash\ncurl -fsSL https://kubeblocks.io/installer/install_cli.sh | bash\n```\n\nTo install a specific version:\n\n1. Check the available versions in [kbcli Release](https://github.com/apecloud/kbcli/releases/).\n2. Specify a version with `-s` and run the command below.\n\n   ```bash\n   curl -fsSL https://kubeblocks.io/installer/install_cli.sh | bash -s \n   ```\n\n:::note\n**Version Compatibility**\n\n- Always match kbcli version with your KubeBlocks deployment\n- Latest stable is recommended for new installations\n- Existing deployments require version matching:\n  - KubeBlocks v1.0.0 → kbcli v1.0.x\n  - KubeBlocks v0.9.x → kbcli v0.9.x\n- Mismatches may cause operational issues\n:::\n\n2. Run `kbcli version` to check the version of kbcli and ensure that it is successfully installed.\n\n:::tip\n**Troubleshooting**\nIf installation fails:\n1. Verify network connectivity\n2. Check firewall/proxy settings\n:::\n\n**Option 2: Install with Homebrew**\n\n1. Install ApeCloud tap, the Homebrew package of ApeCloud.\n\n   ```bash\n   brew tap apecloud/tap\n   ```\n\n2. In",
    "path": "docs/preview/user_docs/references/install-kbcli",
    "description": "  # KubeBlocks Command Line (kbcli)  kbcli is the official command-line tool for managing KubeBlocks clusters. It provides: - Cluster lifecycle management (create, scale, delete) - Configuration and troubleshooting tools - Version compatibility checks - Shell auto-completion support  ## Prerequisite"
  },
  {
    "id": "docs_en_preview_user_docs_references_install-snapshot-controller",
    "title": "Install Snapshot Controller",
    "content": "\n# Install Snapshot Controller\n\nThe Snapshot Controller manages CSI Volume Snapshots, enabling creation, restoration, and deletion of Persistent Volume (PV) snapshots. KubeBlocks' DataProtection Controller leverages this component for database snapshot operations.\n\n**Step 1: Check Prerequisites**\nVerify if required CRDs exist:\n\n```bash\nkubectl get crd volumesnapshotclasses.snapshot.storage.k8s.io\nkubectl get crd volumesnapshots.snapshot.storage.k8s.io\nkubectl get crd volumesnapshotcontents.snapshot.storage.k8s.io\n```\n\nIf your cluster lacks these CRDs, you'll need to install them first:\n\n```bash\n# v8.2.0 is the latest version of the external-snapshotter, you can replace it with the version you need.\nkubectl create -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml\nkubectl create -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml\nkubectl create -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml\n```\n\n\n:::note\n\n**Optional Installation**\n\nIf snapshot backups aren't required, you can install just the CRDs and skip following steps.\n\n:::\n\n\n**Step 2: Deploy Snapshot Controller**\n\nInstall using Helm with these steps:\n\n```bash\nhelm repo add piraeus-charts https://piraeus.io/helm-charts/\nhelm repo update\n# Update the namespace to an appropriate value for your environment (e.g. kb-system)\nhelm install snapshot-controller piraeus-charts/snapshot-controller -n kb-system --create-namespace\n```\n\nFor advanced configuration options, see the [Snapshot Controller documentation](https://artifacthub.io/packages/helm/piraeus-charts/snapshot-controller#configuration).\n\n**Step 3: Verify Deployment**\n\nCheck if the snapshot-controller Pod is running:\n\n```bash\nkubectl get pods -n kb-system | grep snapshot-contr",
    "path": "docs/preview/user_docs/references/install-snapshot-controller",
    "description": " # Install Snapshot Controller  The Snapshot Controller manages CSI Volume Snapshots, enabling creation, restoration, and deletion of Persistent Volume (PV) snapshots. KubeBlocks' DataProtection Controller leverages this component for database snapshot operations.  **Step 1: Check Prerequisites** Ve"
  },
  {
    "id": "docs_en_preview_user_docs_references_kubernetes_and_operator_101",
    "title": "Kubernetes and Operator 101",
    "content": "\n# Kubernetes and Operator 101\n\n## K8s\n\nWhat is Kubernetes? Some say it's a container orchestration system, others describe it as a distributed operating system, while some view it as a multi-cloud PaaS (Platform as a Service) platform, and others consider it a platform for building PaaS solutions.\n\nThis article will introduce the key concepts and building blocks within Kubernetes.\n\n## K8s Control Plane\n\nThe Kubernetes Control Plane is the brain and heart of Kubernetes. It manages the overall operation of the cluster, including processing API requests, storing configuration data, and ensuring the cluster's desired state. Key components include the API Server (which handles communication), etcd (which stores all cluster data), the Controller Manager (which enforces the desired state), the Scheduler (which assigns workloads to Nodes), and the Cloud Controller Manager (which manages cloud-specific integrations, such as load balancers, storage, and networking). Together, these components orchestrate the deployment, scaling, and management of containers across the cluster.\n\n## Node\n\nSome describe Kubernetes as a distributed operating system, capable of managing many Nodes. A Node is a physical or virtual machine that acts as a worker within the cluster. Each Node runs essential services, including the container runtime (such as Docker or containerd), the kubelet, and the kube-proxy. The kubelet ensures that containers are running as specified in a Pod, the smallest deployable unit in Kubernetes. The kube-proxy handles network routing, maintaining network rules, and enabling communication between Pods and services. Nodes provide the computational resources needed to run containerized applications and are managed by the Kubernetes Master, which distributes tasks, monitors Node health, and maintains the desired state of the cluster.\n\n:::note\n\nIn certain contexts, the term \"Node\" can be confusing when discussing Kubernetes (K8s) alongside databases. In Kubernetes, a \"Node\" r",
    "path": "docs/preview/user_docs/references/kubernetes_and_operator_101",
    "description": " # Kubernetes and Operator 101  ## K8s  What is Kubernetes? Some say it's a container orchestration system, others describe it as a distributed operating system, while some view it as a multi-cloud PaaS (Platform as a Service) platform, and others consider it a platform for building PaaS solutions. "
  },
  {
    "id": "docs_en_preview_user_docs_references_prepare-a-local-k8s-cluster",
    "title": "Create a test Kubernetes cluster",
    "content": "\n\n# Create a test Kubernetes cluster\n\nThis tutorial introduces how to create a local Kubernetes test cluster using Minikube, K3d, and Kind. These tools make it easy to try out KubeBlocks on your local host, offering a great solution for development, testing, and experimentation without the complexity of creating a full production-grade cluster.\n\n## Before you start\n\nMake sure you have the following tools installed on your local host:\n\n- Docker: All three tools rely on Docker to create containerized Kubernetes clusters.\n- kubectl: The Kubernetes command-line tool for interacting with clusters. Refer to the [kubectl installation guide](https://kubernetes.io/docs/tasks/tools/)\n\n\n\n## Create a Kubernetes cluster using Kind\n\nKind stands for Kubernetes IN Docker. It runs Kubernetes clusters within Docker containers, making it an ideal tool for local Kubernetes testing.\n\n1. Install Kind. For details, you can refer to [Kind Quick Start](https://kind.sigs.k8s.io/docs/user/quick-start/).\n\n\n\n   ```bash\n   brew install kind\n   ```\n\n   \n\n\n   ```bash\n   # For AMD64 / x86_64\n   [ $(uname -m) = x86_64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.24.0/kind-linux-amd64\n   # For ARM64\n   [ $(uname -m) = aarch64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.24.0/kind-linux-arm64\n   chmod +x ./kind\n   sudo cp ./kind /usr/local/bin/kind\n   rm -rf kind\n   ```\n\n   \n\n\n   You can use chocolatey to install Kind.\n\n   ```bash\n   choco install kind\n   ```\n\n   \n\n   \n\n2. Create a Kind cluster.\n\n   ```bash\n   kind create cluster --name mykindcluster\n   ```\n\n   This command creates a single-node Kubernetes cluster running in a Docker container.\n\n3. Check whether the cluster is started and running.\n\n   ```bash\n   kubectl get nodes\n   >\n   NAME                          STATUS   ROLES           AGE   VERSION\n   mykindcluster-control-plane   Ready    control-plane   25s   v1.31.0\n   ```\n\n   You can see a node named `mykindcluster-control-plane` from the output, which means the cluster is cre",
    "path": "docs/preview/user_docs/references/prepare-a-local-k8s-cluster",
    "description": "  # Create a test Kubernetes cluster  This tutorial introduces how to create a local Kubernetes test cluster using Minikube, K3d, and Kind. These tools make it easy to try out KubeBlocks on your local host, offering a great solution for development, testing, and experimentation without the complexit"
  },
  {
    "id": "docs_en_preview_user_docs_references_terminology",
    "title": "Terminology",
    "content": "\n# Terminology\n\n### Addon\n\nAn addon is an efficient and open extension mechanism. With the KubeBlocks addon, developers can quickly add a new database engine to KubeBlocks and obtain specific foundational management functionalities of that database engine, including but not limited to lifecycle management, data backup and recovery, metrics and log collection, etc.\n### ActionSet\n\nAn ActionSet declares a set of commands to perform backup and restore operations using specific tools, such as commands to backup MySQL using xtrabackup, as well as commands to restore data from the backup.\n\n### BackupPolicy\n\nA BackupPolicy represents a backup strategy for a Cluster, including details such as the backup repository (BackupRepo), backup targets, and backup methods. Multiple backup methods can be defined within a backup policy, with each method referring to a corresponding ActionSet. When creating a backup, the backup policy and backup method can be specified for the backup process.\n\n### BackupRepo\n\nBackupRepo is the storage repository for backup data. Its principle involves using a CSI driver to upload backup data to various storage systems, such as object storage systems like S3, GCS, as well as storage servers like FTP, NFS, and others.\n\n### BackupSchedule\n\nBackupSchedule declares the configuration for automatic backups in a Cluster, including backup frequency, retention period, backup policy, and backup method. The BackupSchedule Controller creates a CronJob to automatically backup the Cluster based on the configuration specified in the Custom Resource (CR).\n\n### Cluster\n\nCluster is composed by [components](#component-is-the-fundamental-assembly-component-used-to-build-a-data-storage-and-processing-system-a-component-utilizes-a-statefulset-either-native-to-kubernetes-or-specified-by-the-customer-such-as-openkruise-to-manage-one-to-multiple-pods).\n\n### Component\n\nA component is the fundamental assembly component used to build a data storage and processing system. A Component",
    "path": "docs/preview/user_docs/references/terminology",
    "description": " # Terminology  ### Addon  An addon is an efficient and open extension mechanism. With the KubeBlocks addon, developers can quickly add a new database engine to KubeBlocks and obtain specific foundational management functionalities of that database engine, including but not limited to lifecycle mana"
  },
  {
    "id": "docs_en_preview_user_docs_troubleshooting_handle-a-cluster-exception",
    "title": "FAQs",
    "content": "\n\n# FAQs\n\n### List of K8s Resources created by KubeBlocks when creating a Cluster\n\nTo get the full list of associated resources created by KubeBlocks for given cluster:\n\n```bash\nkubectl get cmp,its,po -l app.kubernetes.io/instance= -n demo # cluster and worload\nkubectl get backuppolicy,backupschedule,backup -l app.kubernetes.io/instance= -n demo # data protection resources\nkubectl get componentparameter,parameter -l app.kubernetes.io/instance= -n demo # configuration resources\nkubectl get opsrequest -l app.kubernetes.io/instance= -n demo # opsrequest resources\nkubectl get svc,secret,cm,pvc -l app.kubernetes.io/instance= -n demo # k8s native resources\n```\n\nFor troubleshooting,\n\n1. describe resource such as Cluster, Component, e.g.\n```bash\nkubectl describe TYPE NAME\n```\n\n2. check database instance logs\n```bash\nkubectl logs  -c \n```\n\n3. check KubeBlocks logs\n```bash\nkubectl -n kb-system logs deployments/kubeblocks -f\n```\n\n### How to get the detail of each backup method\n\nDetails of each backup method are defined in `ActionSet` in KubeBlocks.\n\nFor example, To get the `ActionSet` which defines the behavior of backup method named `wal-g-archive` in PostgreSQL, for instance:\n\n```bash\nkubectl -n demo get bp pg-cluster-postgresql-backup-policy -oyaml | yq '.spec.backupMethods[] | select(.name==\"wal-g-archive\") | .actionSetName'\n```\n\nActionSet defined:\n\n- backup type\n- both backup and restore procedures\n- environment variables used in procedures\n\nAnd you may check details of each ActionsSet to find out how backup and restore will be performed.\n\n\n### How to Check Compatible versions\n\nVersions and it compatibility rules are embedded in `ComponentVersion` CR in KubeBlocks.\nTo the the list of compatible versions:\n\n```bash\nkubectl get cmpv postgresql -ojson | jq '.spec.compatibilityRules'\n```\n\n\n\nExample Output\n\n```json\n[\n  ,\n  \n]\n```\n\n\n\nReleases are grouped by component definitions, and each group has a list of compatible releases.\nIn this example, it shows you can upgrade from ver",
    "path": "docs/preview/user_docs/troubleshooting/handle-a-cluster-exception",
    "description": "  # FAQs  ### List of K8s Resources created by KubeBlocks when creating a Cluster  To get the full list of associated resources created by KubeBlocks for given cluster:  ```bash kubectl get cmp,its,po -l app.kubernetes.io/instance= -n demo # cluster and worload kubectl get backuppolicy,backupschedul"
  },
  {
    "id": "docs_en_preview_user_docs_troubleshooting_known-issues",
    "title": "Known Issues",
    "content": "\n\n# Known Issues\n\n## Issue 1: KubeBlocks creates enormous number of secrets\n\n### Problem Description\nKubeBlocks keeps creating an enormous number of secrets for each cluster and never stops. You may see the following information in **KubeBlocks** logs:\n\n```bash\nINFO reconcile object *v1.ServiceAccount with action UPDATE OK\n```\n\n### Affected Version\n- KubeBlocks v1.0.0\n- Kubernetes versions \\≤ 1.24\n\n### Root Cause\nBefore Kubernetes version 1.24, Kubernetes automatically generated Secret-based tokens for ServiceAccounts, as documented in [Kubernetes Service Account Tokens](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/).\n\n### Solution\nUpgrade KubeBlocks to `v1.0.0-beta.3` or later.\n\n---\n\n## Issue 2: PostgreSQL fails to start with special characters in password\n\n### Problem Description\nPostgreSQL may fail to start when the password contains certain special characters. By checking POD logs:\n```bash\nFile \"/usr/lib/python3/dist-packages/yaml/scanner.py\", line 116, in check_token\n    self.fetch_more_tokens()\n  File \"/usr/lib/python3/dist-packages/yaml/scanner.py\", line 231, in fetch_more_tokens\n    return self.fetch_anchor()\n  File \"/usr/lib/python3/dist-packages/yaml/scanner.py\", line 621, in fetch_anchor\n    self.tokens.append(self.scan_anchor(AnchorToken))\n  File \"/usr/lib/python3/dist-packages/yaml/scanner.py\", line 929, in scan_anchor\n    raise ScannerError(\"while scanning an %s\" % name, start_mark,\nyaml.scanner.ScannerError: while scanning an anchor\n  in \"\", line 45, column 17:\n          password: &amp;JgE#F5x&amp;eNwis*2dW!7&amp ...\n                    ^\n```\n\n### Affected Version\n- All KubeBlocks versions with PostgreSQL clusters\n\n### Solution\nUse passwords that do not contain special characters that may cause parsing issues in PostgreSQL configuration files.",
    "path": "docs/preview/user_docs/troubleshooting/known-issues",
    "description": "  # Known Issues  ## Issue 1: KubeBlocks creates enormous number of secrets  ### Problem Description KubeBlocks keeps creating an enormous number of secrets for each cluster and never stops. You may see the following information in **KubeBlocks** logs:  ```bash INFO reconcile object *v1.ServiceAccou"
  },
  {
    "id": "docs_en_preview_user_docs_upgrade_upgrade-to-0_8",
    "title": "Upgrade to v0.8",
    "content": "\n\n# Upgrade to KubeBlocks v0.8\n\nIn this tutorial, you will learn how to upgrade to KubeBlocks v0.8.\n\n:::note\n\nExecute `helm -n kb-system list | grep kubeblocks` or `kbcli version` to check the current KubeBlocks version you are running, and then upgrade it.\n\n:::\n\n## Upgrade from KubeBlocks v0.7\n\n\n\n1. Set keepAddons.\n\n    KubeBlocks v0.8 streamlines the default installed engines and separates the addons from KubeBlocks operators to KubeBlocks-Addons repo, such as greptime, influxdb, neon, oracle-mysql, orioledb, tdengine, mariadb, nebula, risingwave, starrocks, tidb, and zookeeper. To avoid deleting addon resources that are already in use during the upgrade, execute the following commands:\n\n- Check the current KubeBlocks version.\n\n    ```bash\n    helm -n kb-system list | grep kubeblocks\n    ```\n\n- Set the value of keepAddons as true.\n\n    ```bash\n    helm repo add kubeblocks https://apecloud.github.io/helm-charts\n    helm repo update kubeblocks\n    helm -n kb-system upgrade kubeblocks kubeblocks/kubeblocks --version \\ --set keepAddons=true\n    ```\n\n    Replace \\ with your current KubeBlocks version, such as 0.7.2.\n\n- Check addons.\n\n    Execute the following command to ensure that the addon annotations contain `\"helm.sh/resource-policy\": \"keep\"`.\n\n    ```bash\n    kubectl get addon -o json | jq '.items[] | '\n    ```\n\n2. Install CRD.\n\n    To reduce the size of Helm chart, KubeBlocks v0.8 removes CRD from the Helm chart. Before upgrading, you need to install CRD.\n\n    ```bash\n    kubectl replace -f https://github.com/apecloud/kubeblocks/releases/download/v0.8.1/kubeblocks_crds.yaml\n    ```\n\n3. Upgrade KubeBlocks.\n\n    ```bash\n    helm -n kb-system upgrade kubeblocks kubeblocks/kubeblocks --version 0.8.1 --set dataProtection.image.datasafed.tag=0.1.0\n    ```\n\n:::note\n\nTo avoid affecting existing database clusters, when upgrading to KubeBlocks v0.8, the versions of already-installed addons will not be upgraded by default. If you want to upgrade the addons to the versions b",
    "path": "docs/preview/user_docs/upgrade/upgrade-to-0_8",
    "description": "  # Upgrade to KubeBlocks v0.8  In this tutorial, you will learn how to upgrade to KubeBlocks v0.8.  :::note  Execute `helm -n kb-system list | grep kubeblocks` or `kbcli version` to check the current KubeBlocks version you are running, and then upgrade it.  :::  ## Upgrade from KubeBlocks v0.7    1"
  },
  {
    "id": "docs_en_preview_user_docs_upgrade_upgrade-to-0_9_0",
    "title": "Upgrade to v0.9.0",
    "content": "\n\n# Upgrade to KubeBlocks v0.9.0\n\nIn this tutorial, you will learn how to upgrade to KubeBlocks v0.9.0.\n\n:::note\n\nExecute `helm -n kb-system list | grep kubeblocks` or `kbcli version` to check the current KubeBlocks version you are running, and then upgrade KubeBlocks.\n\n:::\n\n## Compatibility\n\nKubeBlocks 0.9.0 is compatible with KubeBlocks 0.8 APIs, but compatibility with APIs from versions prior to v0.8 is not guaranteed. If you are using Addons from KubeBlocks 0.7 or earlier (0.6, etc), DO [upgrade KubeBlocks and all Addons to v0.8 first](../upgrade/upgrade-to-0_8) to ensure service availability before upgrading to v0.9.0.\n\n## Upgrade from KubeBlocks v0.8\n\n\n\n1. Add the `\"helm.sh/resource-policy\": \"keep\"` for Addons.\n\n    KubeBlocks v0.8 streamlines the default installed engines. To avoid deleting Addon resources that are already in use during the upgrade, execute the following commands first.\n\n    - Add the `\"helm.sh/resource-policy\": \"keep\"` for Addons. You can replace `-l app.kubernetes.io/name=kubeblocks` with your actual filter name.\n\n         ```bash\n         kubectl annotate addons.extensions.kubeblocks.io -l app.kubernetes.io/name=kubeblocks helm.sh/resource-policy=keep\n         ```\n\n    - Check Addons.\n\n         Execute the following command to ensure that the Addon annotations contain `\"helm.sh/resource-policy\": \"keep\"`.\n\n         ```bash\n         kubectl get addon -o json | jq '.items[] | '\n         ```\n\n2. Delete the incompatible OpsDefinition.\n\n   ```bash\n   kubectl delete opsdefinitions.apps.kubeblocks.io kafka-quota kafka-topic kafka-user-acl switchover\n   ```\n\n3. Install the StorageProvider CRD before the upgrade.\n\n    If the network is slow, it's recommended to download the CRD YAML file on your localhost before further operations.\n\n    ```bash\n    kubectl create -f https://github.com/apecloud/kubeblocks/releases/download/v0.9.0/dataprotection.kubeblocks.io_storageproviders.yaml\n    ```\n\n4. Upgrade KubeBlocks.\n\n    ```bash\n    helm -n kb-system upgr",
    "path": "docs/preview/user_docs/upgrade/upgrade-to-0_9_0",
    "description": "  # Upgrade to KubeBlocks v0.9.0  In this tutorial, you will learn how to upgrade to KubeBlocks v0.9.0.  :::note  Execute `helm -n kb-system list | grep kubeblocks` or `kbcli version` to check the current KubeBlocks version you are running, and then upgrade KubeBlocks.  :::  ## Compatibility  KubeBl"
  },
  {
    "id": "docs_en_preview_user_docs_upgrade_upgrade-to-v09-version",
    "title": "Upgrade to v0.9.x",
    "content": "\n\n# Upgrade to KubeBlocks v0.9.x\n\n:::note\n\n- Before upgrading, check your current KubeBlocks version:\n\n   Run `helm -n kb-system list | grep kubeblocks` or `kbcli version`.\n- For upgrading to different versions:\n\n   - For v0.9.2 and v0.9.1, follow this upgrade tutorial, replacing the version number with v0.9.2 or v0.9.1 respectively.\n   - [v0.9.0 upgrade guide](./upgrade-to-0_9_0)\n   - [v0.8.x upgrade guide](./upgrade-to-0_8).\n\n   Installing the latest version is recommended for better performance and features.\n\n:::\n\n## Compatibility\n\nKubeBlocks v0.9.3 is compatible with KubeBlocks v0.8 APIs, but compatibility with APIs from versions prior to v0.8 is not guaranteed. If you are using Addons from KubeBlocks v0.7 or earlier (v0.6, etc), DO [upgrade KubeBlocks and all Addons to v0.8 first](./upgrade-to-0_8) to ensure service availability before upgrading to v0.9.\n\nIf you are upgrading from v0.8 to v0.9, it's recommended to enable webhook to ensure the availability.\n\n## Upgrade from KubeBlocks v0.9.x\n\n\n\n1. View Addon and check whether the `\"helm.sh/resource-policy\": \"keep\"` annotation exists.\n\n    KubeBlocks streamlines the default installed engines. Add the `\"helm.sh/resource-policy\": \"keep\"` annotation to avoid deleting Addon resources that are already in use during the upgrade.\n\n    Check whether the `\"helm.sh/resource-policy\": \"keep\"` annotation is added.\n\n    ```bash\n    kubectl get addon -o json | jq '.items[] | '\n    ```\n\n    If the annotation doesn't exist, run the command below to add it. You can replace `-l app.kubernetes.io/name=kubeblocks` with your actual filter name.\n\n    ```bash\n    kubectl annotate addons.extensions.kubeblocks.io -l app.kubernetes.io/name=kubeblocks helm.sh/resource-policy=keep\n    ```\n\n2. Install CRD.\n\n    To reduce the size of Helm chart, KubeBlocks v0.8 removes CRD from the Helm chart. Before upgrading, you need to install CRD.\n\n    ```bash\n    kubectl replace -f https://github.com/apecloud/kubeblocks/releases/download/v0.9.3/kubeblock",
    "path": "docs/preview/user_docs/upgrade/upgrade-to-v09-version",
    "description": "  # Upgrade to KubeBlocks v0.9.x  :::note  - Before upgrading, check your current KubeBlocks version:     Run `helm -n kb-system list | grep kubeblocks` or `kbcli version`. - For upgrading to different versions:     - For v0.9.2 and v0.9.1, follow this upgrade tutorial, replacing the version number "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-apecloud-mysql_apecloud-mysql-intro_apecloud-mysql-intro",
    "title": "ApeCloud MySQL introduction",
    "content": "\n# ApeCloud MySQL introduction\n\nMySQL is the world’s most popular open-source database and the second-most-popular database overall. It is used by many of the most accessed applications, such as Facebook, Twitter, Netflix, Uber, Airbnb, Shopify, and Booking.com. \nKubeBlocks adopts the MySQL distribution provided by ApeCloud, which includes data compression and high availability improvements.\n\n- When there are 3 or more replicas,  a strong consistent high-availability cluster is created with the consensus algorithm protocol to ensure that RPO=0 in the case of a single availability zone failure. Among them, the primary instance provides read/write capacity, and the remaining instances provide read-only services.\n- When there are 2 replicas, a Primary-Secondary replication cluster is created, in which the primary instance provides read/write capacity, and the secondary instance keeps in sync with the primary instance with asynchronous replication, providing read-only and fault tolerance capabilities.\n- When there is only 1 replica, a standalone cluster is created to provide read/write capacity. Automatic fault recovery capability is still provided, and RPO=0 remains ensured if the cloud disk is not damaged.\n\n## Instance Roles\n\nApeCloud MySQL supports four roles, **Leader**, **Follower**, **Candidate**, and **Learner**. The Leader and a Follower form a high-availability cluster and ensure RPO=0.\n\n- Leader: This role is the primary instance of the cluster, and supports R/W with forced consistency. It is voted by all the Candidates participating in the election. The Candidates with the majority of votes become the Leader, and the other Candidates become the Follower.\n- Follower: Follower supports data consistency with read-only capacity, and forms a high-availability cluster with Leader and other Followers.\n- Learner: This role is usually used for cross-regional consistent read-only data. Data synchronization is performed through the Paxos protocol, and the data source ca",
    "path": "docs/release-0_9/kubeblocks-for-apecloud-mysql/apecloud-mysql-intro/apecloud-mysql-intro",
    "description": " # ApeCloud MySQL introduction  MySQL is the world’s most popular open-source database and the second-most-popular database overall. It is used by many of the most accessed applications, such as Facebook, Twitter, Netflix, Uber, Airbnb, Shopify, and Booking.com.  KubeBlocks adopts the MySQL distribu"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-apecloud-mysql_cluster-management_create-and-connect-an-apecloud-mysql-cluster",
    "title": "Create and connect to an ApeCloud MySQL Cluster",
    "content": "\n\n# Create and connect to an ApeCloud MySQL cluster\n\nThis tutorial shows how to create and connect to an ApeCloud MySQL cluster.\n\n## Create a MySQL cluster\n\n### Before you start\n\n* [Install kbcli](./../../user_docs/installation/install-kbcli) if you want to create and connect a MySQL cluster by `kbcli`.\n* [Install KubeBlocks](./../../user_docs/installation/install-kubeblocks).\n* Check whether the ApeCloud MySQL Addon is enabled. The ApeCloud MySQL Addon is enabled by KubeBlocks by default. If you disable it when installing KubeBlocks,[enable it](./../../user_docs/installation/install-addons) first.\n\n\n\n  ```bash\n  kubectl get addons.extensions.kubeblocks.io apecloud-mysql\n  >\n  NAME             TYPE   VERSION   PROVIDER   STATUS    AGE\n  apecloud-mysql   Helm                        Enabled   61m\n  ```\n\n  \n\n\n  ```bash\n  kbcli addon list\n  >\n  NAME                           VERSION         PROVIDER    STATUS     AUTO-INSTALL\n  ...\n  apecloud-mysql                 0.9.0           apecloud    Enabled    true\n  ...\n  ```\n\n  \n\n  \n\n* View all the database types and versions available for creating a cluster.\n\n\n\n  Make sure the `apecloud-mysql` cluster definition is installed.\n\n  ```bash\n  kubectl get clusterdefinition apecloud-mysql\n  >\n  NAME             TOPOLOGIES   SERVICEREFS   STATUS      AGE\n  apecloud-mysql                              Available   85m\n  ```\n\n  View all available versions for creating a cluster.\n\n  ```bash\n  kubectl get clusterversions -l clusterdefinition.kubeblocks.io/name=apecloud-mysql\n  >\n  NAME                CLUSTER-DEFINITION   STATUS      AGE\n  ac-mysql-8.0.30     apecloud-mysql       Available   85m\n  ```\n\n  \n\n\n  ```bash\n  kbcli clusterdefinition list\n\n  kbcli clusterversion list\n  ```\n\n  \n\n  \n\n* To keep things isolated, create a separate namespace called `demo` throughout this tutorial.\n\n  ```bash\n  kubectl create namespace demo\n  ```\n\n### Create a cluster\n\nKubeBlocks supports creating two types of ApeCloud MySQL clusters: Standalone and Raf",
    "path": "docs/release-0_9/kubeblocks-for-apecloud-mysql/cluster-management/create-and-connect-an-apecloud-mysql-cluster",
    "description": "  # Create and connect to an ApeCloud MySQL cluster  This tutorial shows how to create and connect to an ApeCloud MySQL cluster.  ## Create a MySQL cluster  ### Before you start  * [Install kbcli](./../../user_docs/installation/install-kbcli) if you want to create and connect a MySQL cluster by `kbc"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-apecloud-mysql_cluster-management_delete-mysql-cluster",
    "title": "Delete an ApeCloud MySQL Cluster",
    "content": "\n\n# Delete an ApeCloud MySQL Cluster\n\n## Termination policy\n\n:::note\n\nThe termination policy determines how a cluster is deleted.\n\n:::\n\n| **terminationPolicy** | **Deleting Operation**                           |\n|:----------------------|:-------------------------------------------------|\n| `DoNotTerminate`      | `DoNotTerminate` blocks delete operation.        |\n| `Halt`                | `Halt` deletes Cluster resources like Pods and Services but retains Persistent Volume Claims (PVCs), allowing for data preservation while stopping other operations. Halt policy is deprecated in v0.9.1 and will have same meaning as DoNotTerminate. |\n| `Delete`              | `Delete` extends the Halt policy by also removing PVCs, leading to a thorough cleanup while removing all persistent data.   |\n| `WipeOut`             | `WipeOut` deletes all Cluster resources, including volume snapshots and backups in external storage. This results in complete data removal and should be used cautiously, especially in non-production environments, to avoid irreversible data loss.   |\n\nTo check the termination policy, execute the following command.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n>\nNAME        CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    AGE\nmycluster   apecloud-mysql       ac-mysql-8.0.30   Delete               Running   27m\n```\n\n\n\n\n```bash\nkbcli cluster list -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        apecloud-mysql       ac-mysql-8.0.30   Delete               Running   Sep 19,2024 16:01 UTC+0800\n```\n\n\n\n\n\n## Step\n\nRun the command below to delete a specified cluster.\n\n\n\nIf you want to delete a cluster and its all related resources, you can modify the termination policy to `WipeOut`, then delete the cluster.\n\n```bash\nkubectl patch -n demo cluster mycluster -p '}' --type=\"merge\"\n\nkubectl delete -n demo cluster mycluster\n```\n\n\n\n\n```bash\nkbcli cluster delete myclu",
    "path": "docs/release-0_9/kubeblocks-for-apecloud-mysql/cluster-management/delete-mysql-cluster",
    "description": "  # Delete an ApeCloud MySQL Cluster  ## Termination policy  :::note  The termination policy determines how a cluster is deleted.  :::  | **terminationPolicy** | **Deleting Operation**                           | |:----------------------|:-------------------------------------------------| | `DoNotTe"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-apecloud-mysql_cluster-management_expand-volume",
    "title": "Expand volume",
    "content": "\n\n# Expand volume\n\nYou can expand the storage volume size of each pod.\n\n:::note\n\nVolume expansion triggers pod restart, all pods restart in the order of learner -> follower -> leader and the leader pod may change after the operation.\n\n:::\n\n## Before you start\n\nCheck whether the cluster status is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n>\nNAME        CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    AGE\nmycluster   apecloud-mysql       ac-mysql-8.0.30   Delete               Running   27m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        apecloud-mysql       ac-mysql-8.0.30   Delete               Running   Sep 19,2024 16:01 UTC+0800\n```\n\n\n\n\n\n## Steps\n\n\n\n1. Apply an OpsRequest. Change the value of storage according to your need and run the command below to expand the volume of a cluster.\n\n   ```bash\n   kubectl apply -f - \n   NAME                              TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   mycluster-volumeexpansion-8257f   VolumeExpansion   mycluster   Succeed   3/3        4m40s\n   ```\n\n   If an error occurs, you can troubleshoot with `kubectl describe ops -n demo` command to view the events of this operation.\n\n3. After the cluster status is `Running` again, check whether the corresponding cluster resources change.\n\n   ```bash\n   kubectl describe cluster mycluster -n demo\n   ```\n\n\n\n\n1. Change the value of `spec.componentSpecs.volumeClaimTemplates.spec.resources` in the cluster YAML file.\n\n   `spec.componentSpecs.volumeClaimTemplates.spec.resources` is the storage resource information of the pod and changing this value triggers the volume expansion of a cluster.\n\n   ```bash\n   kubectl edit cluster mycluster -n demo\n   ```\n\n   Edit the value of `spec.componentSpecs.volumeClaimTemplates.spec.resources`.\n\n   ```yaml\n   apiVersion:",
    "path": "docs/release-0_9/kubeblocks-for-apecloud-mysql/cluster-management/expand-volume",
    "description": "  # Expand volume  You can expand the storage volume size of each pod.  :::note  Volume expansion triggers pod restart, all pods restart in the order of learner -> follower -> leader and the leader pod may change after the operation.  :::  ## Before you start  Check whether the cluster status is `Ru"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-apecloud-mysql_cluster-management_restart-mysql-cluster",
    "title": "Restart MySQL cluster",
    "content": "\n\n# Restart an ApeCloud MySQL cluster\n\nYou can restart all pods of the cluster. When an exception occurs in a database, you can try to restart it.\n\n## Steps\n\n\n\n1. Create an OpsRequest to restart a cluster.\n\n   ```bash\n   kubectl apply -f - \n   NAME                READY   STATUS        RESTARTS   AGE\n   mycluster-mysql-0   4/4     Running       0          5m32s\n   mycluster-mysql-1   4/4     Running       0          6m36s\n   mycluster-mysql-2   3/4     Terminating   0          7m37s\n\n   kubectl get ops ops-restart -n demo\n   >\n   NAME          TYPE      CLUSTER     STATUS    PROGRESS   AGE\n   ops-restart   Restart   mycluster   Succeed   1/1        3m26s\n   ```\n\n   During the restarting process, there are two status types for pods.\n\n   - STATUS=Terminating: it means the cluster restart is in progress.\n   - STATUS=Running: it means the cluster has been restarted.\n\n   If an error occurs, you can troubleshoot with `kubectl describe` command to view the events of this operation.\n\n\n\n\n1. Restart a cluster.  \n\n   Configure the values of `components` and `ttlSecondsAfterSucceed` and run the command below to restart a specified cluster.\n\n   ```bash\n   kbcli cluster restart mycluster --components=\"mysql\" --ttlSecondsAfterSucceed=30 -n demo\n   ```\n\n   - `components` describes the component name that needs to be restarted.\n   - `ttlSecondsAfterSucceed` describes the time to live of an OpsRequest job after the restarting succeeds.\n\n2. Check the cluster status to validate the restarting.\n\n   ```bash\n   kbcli cluster list mycluster -n demo\n   >\n   NAME        NAMESPACE   CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    CREATED-TIME\n   mycluster   demo        apecloud-mysql       ac-mysql-8.0.30   Delete               Running   Sep 19,2024 16:01 UTC+0800\n   ```\n\n   - STATUS=Updating: it means the cluster restart is in progress.\n   - STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n",
    "path": "docs/release-0_9/kubeblocks-for-apecloud-mysql/cluster-management/restart-mysql-cluster",
    "description": "  # Restart an ApeCloud MySQL cluster  You can restart all pods of the cluster. When an exception occurs in a database, you can try to restart it.  ## Steps    1. Create an OpsRequest to restart a cluster.     ```bash    kubectl apply -f -     NAME                READY   STATUS        RESTARTS   AGE"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-apecloud-mysql_cluster-management_scale-for-apecloud-mysql",
    "title": "Scale for an ApeCloudto scale a MySQL cluster, horizontal scaling, vertical scaling",
    "content": "\n\n# Scale an ApeCloud MySQL cluster\n\nYou can scale an ApeCloud MySQL cluster in two ways, vertical scaling and horizontal scaling.\n\n:::note\n\nAfter vertical scaling or horizontal scaling is performed, KubeBlocks automatically matches the appropriate configuration template based on the new specification. This is the KubeBlocks dynamic configuration feature. This feature simplifies the process of configuring parameters, saves time and effort and reduces performance issues caused by incorrect configuration. For detailed instructions, refer to [Configuration](./../configuration/configuration.md).\n\n:::\n\n## Vertical scaling\n\nYou can vertically scale a cluster by changing resource requirements and limits (CPU and storage). For example, you can change the resource class from 1C2G to 2C4G by performing vertical scaling.\n\n### Before you start\n\nCheck whether the cluster status is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n>\nNAME        CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    AGE\nmycluster   apecloud-mysql       ac-mysql-8.0.30   Delete               Running   27m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        apecloud-mysql       ac-mysql-8.0.30   Delete               Running   Sep 19,2024 16:01 UTC+0800\n```\n\n\n\n\n\n### Steps\n\n\n\n1. Apply an OpsRequest to the specified cluster. Configure the parameters according to your needs.\n\n   ```bash\n   kubectl apply -f - \n   NAMESPACE   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        ops-vertical-scaling   VerticalScaling   mycluster   Succeed   3/3        6m\n   ```\n\n   If an error occurs, you can troubleshoot with `kubectl describe ops -n demo` command to view the events of this operation.\n\n3. Check whether the cluster is running again and corresponding resources change",
    "path": "docs/release-0_9/kubeblocks-for-apecloud-mysql/cluster-management/scale-for-apecloud-mysql",
    "description": "  # Scale an ApeCloud MySQL cluster  You can scale an ApeCloud MySQL cluster in two ways, vertical scaling and horizontal scaling.  :::note  After vertical scaling or horizontal scaling is performed, KubeBlocks automatically matches the appropriate configuration template based on the new specificati"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-apecloud-mysql_cluster-management_stop-start-a-cluster",
    "title": "Stop/Start an ApeCloud MySQL cluster",
    "content": "\n\n# Stop/Start an ApeCloud MySQL cluster\n\nYou can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. You can start this cluster again to restore it to the state it was in before it was stopped.\n\n## Stop a cluster\n\n1. Configure the name of your cluster and run the command below to stop this cluster.\n\n\n\n    ```bash\n    kubectl apply -f - \n  \n\n    ```bash\n    kubectl edit cluster mycluster -n demo\n    ```\n\n    Configure the value of `spec.componentSpecs.replicas` as 0 to delete pods.\n\n    ```yaml\n    ...\n    spec:\n      clusterDefinitionRef: apecloud-mysql\n      clusterVersionRef: ac-mysql-8.0.30\n      terminationPolicy: Delete\n      componentSpecs:\n      - name: mysql\n        componentDefRef: mysql\n        disableExporter: true  \n        replicas: 0 # Change this value\n    ...\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster stop mycluster -n demo\n    ```\n\n    \n\n    \n2. Check the status of the cluster to see whether it is stopped.\n\n\n\n    ```bash\n    kubectl get cluster mycluster -n demo\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster list mycluster -n demo\n    ```\n\n    \n\n    \n\n## Start a cluster\n\n1. Configure the name of your cluster and run the command below to start this cluster.\n\n\n\n    ```bash\n    kubectl apply -f - \n  \n\n    ```bash\n    kubectl edit cluster mycluster -n demo\n    ```\n\n    Change the value of `spec.componentSpecs.replicas` back to the original amount to start this cluster again.\n\n    ```yaml\n    ...\n    spec:\n      clusterDefinitionRef: apecloud-mysql\n      clusterVersionRef: ac-mysql-8.0.30\n      terminationPolicy: Delete\n      componentSpecs:\n      - name: mysql\n        componentDefRef: mysql\n        disableExporter: true\n        replicas: 3 # Change this value\n    ...\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster start mycluster -n demo\n    ```\n\n    \n\n    \n\n2. Check the status of the cl",
    "path": "docs/release-0_9/kubeblocks-for-apecloud-mysql/cluster-management/stop-start-a-cluster",
    "description": "  # Stop/Start an ApeCloud MySQL cluster  You can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. You can start this cluster aga"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-apecloud-mysql_cluster-management_switchover",
    "title": "Switch over an ApeCloud MySQL cluster",
    "content": "\n\n# Switch over an ApeCloud MySQL cluster\n\nYou can initiate a switchover for an ApeCloud MySQL RaftGroup by executing the kbcli or kubectl command. Then KubeBlocks switches the instance roles.\n\n## Before you start\n\n* Make sure the cluster is running normally.\n  \n\n\n   ```bash\n   kubectl get cluster mycluster -n demo\n   >\n   NAME        CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    AGE\n   mycluster   apecloud-mysql       ac-mysql-8.0.30   Delete               Running   27m\n   ```\n\n   \n\n\n   ```bash\n   kbcli cluster list mycluster -n demo\n   >\n   NAME        NAMESPACE   CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    CREATED-TIME\n   mycluster   demo        apecloud-mysql       ac-mysql-8.0.30   Delete               Running   Sep 19,2024 16:01 UTC+0800\n   ```\n\n   \n\n   \n* Check whether the following role probe parameters exist to verify whether the role probe is enabled.\n\n   ```bash\n   kubectl get cd apecloud-mysql -o yaml\n   >\n   probes:\n     roleProbe:\n       failureThreshold: 2\n       periodSeconds: 1\n       timeoutSeconds: 1\n   ```\n\n## Initiate the switchover\n\nYou can switch over a follower of an ApeCloud MySQL RaftGroup to the leader role, and the former leader instance to a follower.\n\n\n\nThe value of `instanceName` decides whether a new leader instance is specified for the switchover.\n\n* Initiate a switchover with no leader instance specified.\n\n  ```yaml\n  kubectl apply -f -\n\n\n* Initiate a switchover with no leader instance specified.\n\n    ```bash\n    kbcli cluster promote mycluster -n demo\n    ```\n\n* Initiate a switchover with a specified new leader instance.\n\n    ```bash\n    kbcli cluster promote mycluster --instance='mycluster-mysql-2' -n demo\n    ```\n\n* If there are multiple components, you can use `--components` to specify a component.\n\n    ```bash\n    kbcli cluster promote mycluster --instance='mycluster-mysql-2' --components='apecloud-mysql' -n demo\n    ```\n\n\n\n\n\n## Verify the switchover\n\nCheck the instance status t",
    "path": "docs/release-0_9/kubeblocks-for-apecloud-mysql/cluster-management/switchover",
    "description": "  # Switch over an ApeCloud MySQL cluster  You can initiate a switchover for an ApeCloud MySQL RaftGroup by executing the kbcli or kubectl command. Then KubeBlocks switches the instance roles.  ## Before you start  * Make sure the cluster is running normally.         ```bash    kubectl get cluster m"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-apecloud-mysql_configuration_configuration",
    "title": "Configure cluster parameters",
    "content": "\n\n# Configure cluster parameters\n\nThis guide shows how to configure cluster parameters.\n\nFrom v0.9.0, KubeBlocks supports dynamic configuration. When the specification of a database instance changes (e.g. a user vertically scales a cluster), KubeBlocks automatically matches the appropriate configuration template based on the new specification. This is because different specifications of a database instance may require different optimal configurations to optimize performance and resource utilization. When you choose a different database instance specification, KubeBlocks automatically detects it and determines the best database configuration for the new specification, ensuring optimal performance and configuration of the database under the new specifications.\n\nThis feature simplifies the process of configuring parameters, which saves you from manually configuring database parameters as KubeBlocks handles the updates and configurations automatically to adapt to the new specifications. This saves time and effort and reduces performance issues caused by incorrect configuration.\n\nBut it's also important to note that the dynamic parameter configuration doesn't apply to all parameters. Some parameters may require manual configuration. Additionally, if you have manually modified database parameters before, KubeBlocks may overwrite your customized configurations when updating the database configuration template. Therefore, when using the dynamic configuration feature, it is recommended to back up and record your custom configuration so that you can restore them if needed.\n\n\n\nKubeBlocks supports configuring cluster parameters by editing the configuration file.\n\n1. Get the configuration file of this cluster.\n\n   ```bash\n   kubectl edit configurations.apps.kubeblocks.io mycluster-mysql -n demo\n   ```\n\n2. Configure parameters according to your needs. The example below adds the `spec.configFileParams` part to configure `max_connections`.\n\n   ```yaml\n   spec:\n     clusterRef: mycl",
    "path": "docs/release-0_9/kubeblocks-for-apecloud-mysql/configuration/configuration",
    "description": "  # Configure cluster parameters  This guide shows how to configure cluster parameters.  From v0.9.0, KubeBlocks supports dynamic configuration. When the specification of a database instance changes (e.g. a user vertically scales a cluster), KubeBlocks automatically matches the appropriate configura"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-apecloud-mysql_high-availability_high-availability",
    "title": "Failure simulation and automatic recovery",
    "content": "\n\n# Failure simulation and automatic recovery\n\nAs an open-source data management platform, Kubeblocks currently supports over thirty database engines and is continuously expanding. Due to the varying high availability capabilities of databases, KubeBlocks has designed and implemented a high availability (HA) system for database instances. The KubeBlocks HA system uses a unified HA framework to provide high availability for databases, allowing different databases on KubeBlocks to achieve similar high availability capabilities and experiences.\n\nThis tutorial uses ApeCloud MySQL RaftGroup Cluster as an example to demonstrate its fault simulation and recovery capabilities.\n\n## Recovery simulation\n\n:::note\n\nThe faults here are all simulated by deleting a pod. When there are sufficient resources, the fault can also be simulated by machine downtime or container deletion, and its automatic recovery is the same as described here.\n\n:::\n\n### Before you start\n\n* [Install KubeBlocks](./../../user_docs/installation/install-kubeblocks).\n* Create an ApeCloud MySQL RaftGroup, refer to [Create a MySQL cluster](./../cluster-management/create-and-connect-an-apecloud-mysql-cluster.md).\n* Run `kubectl get cd apecloud-mysql -o yaml` to check whether _rolechangedprobe_ is enabled in the ApeCloud MySQL RaftGroup (it is enabled by default). If the following configuration exists, it indicates that it is enabled:\n\n  ```bash\n  probes:\n    roleProbe:\n      failureThreshold: 2\n      periodSeconds: 1\n      timeoutSeconds: 1\n  ```\n\n### Leader pod fault\n\n***Steps:***\n\n\n\n1. View the pod role of the ApeCloud MySQL RaftGroup Cluster. In this example, the leader pod's name is `mycluster-mysql-1`.\n\n    ```bash\n    kubectl get pods --show-labels -n demo | grep role\n    ```\n\n    ![describe_pod](/img/docs/en/api-ha-grep-role.png)\n2. Delete the leader pod `mycluster-mysql-1` to simulate a pod fault.\n\n    ```bash\n    kubectl delete pod mycluster-mysql-1 -n demo\n    ```\n\n    ![delete_pod](/img/docs/en/api-ha-d",
    "path": "docs/release-0_9/kubeblocks-for-apecloud-mysql/high-availability/high-availability",
    "description": "  # Failure simulation and automatic recovery  As an open-source data management platform, Kubeblocks currently supports over thirty database engines and is continuously expanding. Due to the varying high availability capabilities of databases, KubeBlocks has designed and implemented a high availabi"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-apecloud-mysql_proxy_apecloud-mysql-proxy",
    "title": "How to use ApeCloud MySQL Proxy Cluster",
    "content": "\n\n# ApeCloud MySQL Proxy\n\n## Before you start\n\n1. [Install kbcli](./../../user_docs/installation/install-kbcli).\n2. [Install Helm](https://helm.sh/docs/intro/install/).\n3. Install KubeBlocks.\n\n   You can run `kbcli playground init` to install a k3d cluster and KubeBlocks. For details, refer to [Try KubeBlocks on your laptop](./../../try-out-on-playground/try-kubeblocks-on-your-laptop.md) or [Try KubeBlocks on cloud](./../../try-out-on-playground/try-kubeblocks-on-cloud.md).\n\n   ```bash\n   kbcli playground init\n\n   # Use --version to specify a version\n   kbcli playground init --version='x.y.z'\n   ```\n\n   Or if you already have a Kubernetes cluster, you can [install KubeBlocks](./../../user_docs/installation/install-kubeblocks) directly.\n4. Prepare an ApeCloud MySQL RaftGroup named `mycluster` for demonstrating how to enable the proxy function for an existing cluster. Refer to [Create a MySQL cluster](./../cluster-management/create-and-connect-an-apecloud-mysql-cluster.md) for details.\n\n## Create a Proxy Cluster\n\nIt is recommended to use kbcli to create an ApeCloud MySQL Proxy Cluster.\n\n\n\n1. Add the KubeBlocks repository.\n\n   ```bash\n   helm repo add kubeblocks https://apecloud.github.io/helm-charts\n   ```\n\n2. View the repository list to verify whether the KubeBlocks repository is added successfully.\n\n   ```bash\n   helm repo list\n   ```\n\n3. Run the update command to make sure you have added the latest version.\n\n   ```bash\n   helm repo update\n   ```\n\n4. Install etcd to create the external service reference.\n\n   1. View all versions of etcd.\n\n       ```bash\n       helm search repo kubeblocks/etcd --devel --versions\n       ```\n\n   2. Install the etcd Addon.\n\n       ```bash\n       helm install etcd kubeblocks/etcd --version=v0.6.5\n       ```\n\n   3. Install the etcd cluster.\n\n       ```bash\n       helm install etcd-cluster kubeblocks/etcd-cluster\n       ```\n\n   4. view the status of the etcd cluster and make sure it is running.\n\n       ```bash\n       kubectl get cluster\n  ",
    "path": "docs/release-0_9/kubeblocks-for-apecloud-mysql/proxy/apecloud-mysql-proxy",
    "description": "  # ApeCloud MySQL Proxy  ## Before you start  1. [Install kbcli](./../../user_docs/installation/install-kbcli). 2. [Install Helm](https://helm.sh/docs/intro/install/). 3. Install KubeBlocks.     You can run `kbcli playground init` to install a k3d cluster and KubeBlocks. For details, refer to [Try "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-apecloud-mysql_proxy_introduction",
    "title": "Introduction",
    "content": "\n# Introduction\n\nA database proxy is an essential tool for developers and database administrators to improve the scalability, performance, security, and resilience of their applications.\n\nApeCloud MySQL Proxy is a database proxy designed to be highly compatible with MySQL. It supports the MySQL wire protocol, read-write splitting without stale reads, connection pooling, and transparent failover. This section introduces ApeCloud MySQL Proxy, explaining its architecture, key features, and benefits.\n\n## Architecture\n\nApeCloud MySQL Proxy is a fork of the Vitess project and the support for sharding is removed in exchange for better SQL compatibility, for example, better support for subqueries, Common Table Expressions (CTE) and expression evaluation. The below graph displays the architecture of a proxy cluster.\n\n**VTGate**: Client application usually connects to VTGate via standard MySQL wire protocol. VTGate is stateless, which means it can be easily and effectively scaled in terms of size and performance. It acts like a MySQL and is responsible for parsing SQL queries, as well as planning and routing queries to VTTables.\n\n**VTTablet**: Typically, VTTablet is implemented as a sidecar for MySQL. If ApeCloud MySQL Proxy is deployed in Kubernetes, VTTablet should be in the same pod as MySQL. VTTablet accepts gRPC requests from VTGate and then sends those queries to be executed on MySQL. The VTTablet takes care of a few tasks such as permission checking and logging, but its most critical role is to ensure proper connection pooling.\n\n**VTController**: The VTController component facilitates service discovery between VTGate and VTTablet, while also enabling them to store metadata of the cluster. If the role of MySQL changes, for example, from leaders to followers, the corresponding role of VTTablet should change accordingly. VTController checks the status of the MySQL cluster and sends commands to VTTablet to request that it changes roles.\n\n![ApeCloud MySQL Proxy architecture",
    "path": "docs/release-0_9/kubeblocks-for-apecloud-mysql/proxy/introduction",
    "description": " # Introduction  A database proxy is an essential tool for developers and database administrators to improve the scalability, performance, security, and resilience of their applications.  ApeCloud MySQL Proxy is a database proxy designed to be highly compatible with MySQL. It supports the MySQL wire"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-kafka_cluster-management_connect-to-a-cluster",
    "title": "Connect to a Kafka cluster",
    "content": "\n\n# Connect to a Kafka cluster\n\nBefore you connect to the Kafka cluster, you must check your network environment, and from which network you would like to connect to the cluster.\nThere are three scenarios of connecting.\n\n* Connect to the cluster within the same Kubernetes cluster.\n* Connect to a kafka cluster from outside of the Kubernetes cluster but in the same VPC.\n* Connect to a kafka cluster from public internet.\n\n## Connect to a kafka cluster within the Kubernetes cluster\n\nWithin the same Kubernetes cluster, you can directly access the Kafka cluster with ClusterIp service:9092.\n\n***Steps:***\n\n1. Get the address of the Kafka ClusterIP service port No..\n\n   ```bash\n   kubectl get svc -n demo\n   > \n   NAME                                      TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                               AGE\n   kubernetes                                ClusterIP   10.43.0.1             443/TCP                               9d\n   mycluster-kafka-cluster-broker-headless   ClusterIP   None                  9092/TCP,9093/TCP,9094/TCP,5556/TCP   7d16h\n   mycluster-kafka-cluster-broker            ClusterIP   10.43.8.124           9093/TCP,9092/TCP,5556/TCP            7d16h\n   ```\n\n2. Connect to the Kafka cluster with the port No..\n\n   Below is an example of connecting with the official client script.\n\n   1. Start client pod.\n\n       ```bash\n       kubectl run kafka-producer --restart='Never' --image docker.io/bitnami/kafka:3.3.2-debian-11-r54 --command -- sleep infinity\n       kubectl run kafka-consumer --restart='Never' --image docker.io/bitnami/kafka:3.3.2-debian-11-r54 --command -- sleep infinity\n       ```\n\n   2. Log in to kafka-producer.\n\n       ```bash\n       kubectl exec -ti kafka-producer -- bash\n       ```\n\n   3. Create topic.\n\n       ```bash\n       kafka-topics.sh --create --topic quickstart-events --bootstrap-server xxx-broker:9092\n       ```\n\n   4. Create producer.\n\n       ```bash\n       kafka-console-producer.sh --topic quickstart-events --b",
    "path": "docs/release-0_9/kubeblocks-for-kafka/cluster-management/connect-to-a-cluster",
    "description": "  # Connect to a Kafka cluster  Before you connect to the Kafka cluster, you must check your network environment, and from which network you would like to connect to the cluster. There are three scenarios of connecting.  * Connect to the cluster within the same Kubernetes cluster. * Connect to a kaf"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-kafka_cluster-management_create-a-kafka-cluster",
    "title": "Create a cluster for Kafka",
    "content": "\n\n# Create a Kafka cluster\n\nThis document shows how to create a Kafka cluster.\n\n## Before you start\n\n* [Install kbcli](./../../user_docs/installation/install-kbcli) if you want to create a Kafka cluster by `kbcli`.\n* [Install KubeBlocks](./../../user_docs/installation/install-kubeblocks).\n* Make sure Kafka Addon is enabled with `kbcli addon list`. If this Addon is not enabled, [enable it](./../../user_docs/installation/install-addons) first.\n\n\n\n  ```bash\n  kubectl get addons.extensions.kubeblocks.io kafka\n  >\n  NAME    TYPE   VERSION   PROVIDER   STATUS    AGE\n  kafka   Helm                        Enabled   13m\n  ```\n\n  \n\n\n  ```bash\n  kbcli addon list\n  >\n  NAME                           TYPE   STATUS     EXTRAS         AUTO-INSTALL\n  ...\n  kafka                          Helm   Enabled                   true\n  ...\n  ```\n\n  \n\n  \n\n* To keep things isolated, create a separate namespace called `demo` throughout this tutorial.\n\n  ```bash\n  kubectl create namespace demo\n  ```\n\n:::note\n\n* KubeBlocks integrates Kafka v3.3.2, running it in KRaft mode.\n* You are not recommended to use kraft cluster in combined mode in a production environment.\n* The controller number suggested ranges from 3 to 5, out of complexity and availability.\n\n:::\n\n## Create a Kafka cluster\n\n\n\n1. Create a Kafka cluster. If you only have one node for deploying a cluster with multiple replicas, set `spec.affinity.topologyKeys` as `null`. But for a production environment, it is not recommended to deploy all replicas on one node, which may decrease the cluster availability.\n\n   * Create a Kafka cluster in combined mode.\n\n     ```yaml\n     # create kafka in combined mode\n     kubectl apply -f - \n   NAME        CLUSTER-DEFINITION   VERSION       TERMINATION-POLICY   STATUS    AGE\n   mycluster   kafka                kafka-3.3.2   Delete               Running   2m2s\n   ```\n\n\n\n\n1. Create a Kafka cluster.\n\n   The cluster creation command is simply `kbcli cluster create`. Further, you can customize your cluster re",
    "path": "docs/release-0_9/kubeblocks-for-kafka/cluster-management/create-a-kafka-cluster",
    "description": "  # Create a Kafka cluster  This document shows how to create a Kafka cluster.  ## Before you start  * [Install kbcli](./../../user_docs/installation/install-kbcli) if you want to create a Kafka cluster by `kbcli`. * [Install KubeBlocks](./../../user_docs/installation/install-kubeblocks). * Make sur"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-kafka_cluster-management_delete-kafka-cluster",
    "title": "Delete a kafka Cluster",
    "content": "\n\n# Delete a Kafka cluster\n\n## Termination policy\n\n:::note\n\nThe termination policy determines how a cluster is deleted. Set the policy when creating a cluster.\n\n:::\n\n| **terminationPolicy** | **Deleting Operation**                           |\n|:----------------------|:-------------------------------------------------|\n| `DoNotTerminate`      | `DoNotTerminate` blocks delete operation.        |\n| `Halt`                | `Halt` deletes Cluster resources like Pods and Services but retains Persistent Volume Claims (PVCs), allowing for data preservation while stopping other operations. Halt policy is deprecated in v0.9.1 and will have same meaning as DoNotTerminate. |\n| `Delete`              | `Delete` extends the Halt policy by also removing PVCs, leading to a thorough cleanup while removing all persistent data.   |\n| `WipeOut`             | `WipeOut` deletes all Cluster resources, including volume snapshots and backups in external storage. This results in complete data removal and should be used cautiously, especially in non-production environments, to avoid irreversible data loss.   |\n\nTo check the termination policy, execute the following command.\n\n\n\n```bash\nkubectl -n demo get cluster mycluster\n>\nNAME           CLUSTER-DEFINITION   VERSION        TERMINATION-POLICY   STATUS     AGE\nmycluster      kafka                kafka-3.3.2    Delete               Running    19m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION       TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        kafka                kafka-3.3.2   Delete               Running   Sep 27,2024 15:15 UTC+0800\n```\n\n\n\n\n\n## Steps\n\nRun the command below to delete a specified cluster.\n\n\n\n```bash\nkubectl delete -n demo cluster mycluster\n```\n\nIf you want to delete a cluster and its all related resources, you can set the termination policy to `WipeOut`, then delete the cluster.\n\n```bash\nkubectl patch -n demo cluster mycluster -p '}' --type=\"merge",
    "path": "docs/release-0_9/kubeblocks-for-kafka/cluster-management/delete-kafka-cluster",
    "description": "  # Delete a Kafka cluster  ## Termination policy  :::note  The termination policy determines how a cluster is deleted. Set the policy when creating a cluster.  :::  | **terminationPolicy** | **Deleting Operation**                           | |:----------------------|:-------------------------------"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-kafka_cluster-management_expand-volume",
    "title": "Expand volume",
    "content": "\n\n# Expand volume\n\nYou can expand the storage volume size of each pod.\n\n## Before you start\n\nRun the command below to check whether the cluster STATUS is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl -n demo get cluster mycluster\n>\nNAME           CLUSTER-DEFINITION   VERSION        TERMINATION-POLICY   STATUS     AGE\nmycluster      kafka                kafka-3.3.2    Delete               Running    19m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION       TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        kafka                kafka-3.3.2   Delete               Running   Sep 27,2024 15:15 UTC+0800\n```\n\n\n\n\n\n## Steps\n\n\n\n1. Apply an OpsRequest. Change the value of storage according to your need and run the command below to expand the volume of a cluster.\n\n   ```bash\n   kubectl apply -f - \n   NAMESPACE   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        ops-volume-expansion   VolumeExpansion   mycluster   Succeed   3/3        6m\n   ```\n\n3. Check whether the corresponding cluster resources change.\n\n   ```bash\n   kubectl describe cluster mycluster -n demo\n   >\n   ...\n   Volume Claim Templates:\n     Name:  data\n     Spec:\n       Access Modes:\n         ReadWriteOnce\n       Resources:\n         Requests:\n           Storage:   40Gi\n   ```\n\n\n\n\n1. Change the value of `spec.componentSpecs.volumeClaimTemplates.spec.resources` in the cluster YAML file.\n\n   `spec.componentSpecs.volumeClaimTemplates.spec.resources` is the storage resource information of the pod and changing this value triggers the volume expansion of a cluster.\n\n   ```yaml\n   apiVersion: apps.kubeblocks.io/v1alpha1\n   kind: Cluster\n   metadata:\n     name: mycluster\n     namespace: demo \n   spec:\n     clusterDefinitionRef: kafka\n     clusterVersionRef: kafka-3.3.2\n     componentSpecs:\n     - name: kafka \n       componentDefRef: kafka\n       volumeClaimTemplates:\n       - ",
    "path": "docs/release-0_9/kubeblocks-for-kafka/cluster-management/expand-volume",
    "description": "  # Expand volume  You can expand the storage volume size of each pod.  ## Before you start  Run the command below to check whether the cluster STATUS is `Running`. Otherwise, the following operations may fail.    ```bash kubectl -n demo get cluster mycluster > NAME           CLUSTER-DEFINITION   VE"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-kafka_cluster-management_restart-a-kafka-cluster",
    "title": "Restart Kafka cluster",
    "content": "\n\n# Restart a Kafka cluster\n\nYou can restart all pods of the cluster. When an exception occurs in a database, you can try to restart it.\n\n:::note\n\nThe pod role may change after the cluster restarts.\n\n:::\n\n## Steps\n\n\n\n1. Create an OpsRequest to restart a cluster.\n\n    ```bash\n    kubectl apply -f - \n   NAME          TYPE      CLUSTER     STATUS    PROGRESS   AGE\n   ops-restart   Restart   mycluster   Succeed   1/1        3m26s\n   ```\n\n   During the restarting process, there are two status types for pods.\n\n   - STATUS=Terminating: it means the cluster restart is in progress.\n   - STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n1. Restart a cluster.\n  \n   Configure the values of `components` and `ttlSecondsAfterSucceed` and run the command below to restart a specified cluster.\n\n   ```bash\n   kbcli cluster restart mycluster -n demo --components=\"kafka\" --ttlSecondsAfterSucceed=30\n   ```\n\n   - `components` describes the component name that needs to be restarted.\n   - `ttlSecondsAfterSucceed` describes the time to live of an OpsRequest job after the restarting succeeds.\n\n2. Validate the restarting.\n\n   Run the command below to check the cluster status to check the restarting status.\n\n   ```bash\n   kbcli cluster list cluster-name\n   >\n   NAME    CLUSTER-DEFINITION   VERSION        TERMINATION-POLICY   STATUS     AGE\n   kafka   kafka                kafka-3.3.2    Delete               Running    19m\n   ```\n\n   * STATUS=Restarting: it means the cluster restart is in progress.\n   * STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n",
    "path": "docs/release-0_9/kubeblocks-for-kafka/cluster-management/restart-a-kafka-cluster",
    "description": "  # Restart a Kafka cluster  You can restart all pods of the cluster. When an exception occurs in a database, you can try to restart it.  :::note  The pod role may change after the cluster restarts.  :::  ## Steps    1. Create an OpsRequest to restart a cluster.      ```bash     kubectl apply -f -  "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-kafka_cluster-management_scale",
    "title": "Scale for a Kafka cluster",
    "content": "\n\n# Scale a Kafka cluster\n\nYou can scale a Kafka cluster in two ways, vertical scaling and horizontal scaling.\n\n## Vertical scaling\n\nYou can vertically scale a cluster by changing resource requirements and limits (CPU and storage). For example, you can change the resource class from 1C2G to 2C4G by performing vertical scaling.\n\n### Before you start\n\nCheck whether the cluster status is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl -n demo get cluster mycluster\n>\nNAME           CLUSTER-DEFINITION   VERSION        TERMINATION-POLICY   STATUS     AGE\nmycluster      kafka                kafka-3.3.2    Delete               Running    19m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION       TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        kafka                kafka-3.3.2   Delete               Running   Sep 27,2024 15:15 UTC+0800\n```\n\n\n\n\n\n### Steps\n\n\n\n1. Apply an OpsRequest to the specified cluster. Configure the parameters according to your needs.\n\n   ```yaml\n   kubectl apply -f - \n   NAMESPACE   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        ops-vertical-scaling   VerticalScaling   mycluster   Succeed   3/3        6m\n   ```\n\n   If an error occurs, you can troubleshoot with `kubectl describe ops -n demo` command to view the events of this operation.\n\n3. Check whether the corresponding resources change.\n\n   ```bash\n   kubectl describe cluster mycluster -n demo\n   >\n   ...\n   Component Specs:\n    Component Def Ref:  kafka\n    Enabled Logs:\n      running\n    DisableExporter:   true\n    Name:      kafka\n    Replicas:  2\n    Resources:\n      Limits:\n        Cpu:     2\n        Memory:  4Gi\n      Requests:\n        Cpu:     1\n        Memory:  2Gi\n   ```\n\n\n\n\n1. Change the configuration of `spec.componentSpecs.resources` in the YAML file. `spec.componentSpecs.resources` controls the requirement and limit of resources and changin",
    "path": "docs/release-0_9/kubeblocks-for-kafka/cluster-management/scale",
    "description": "  # Scale a Kafka cluster  You can scale a Kafka cluster in two ways, vertical scaling and horizontal scaling.  ## Vertical scaling  You can vertically scale a cluster by changing resource requirements and limits (CPU and storage). For example, you can change the resource class from 1C2G to 2C4G by "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-kafka_cluster-management_start-stop-a-cluster",
    "title": "Stop/Start a Kafka cluster",
    "content": "\n\n# Stop/Start a Kafka Cluster\n\nYou can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. Start this cluster again if you want to restore the cluster resources from the original storage by snapshots.\n\n## Stop a cluster\n\n***Steps:***\n\n1. Configure the name of your cluster and run the command below to stop this cluster.\n\n\n\n   Run the command below to stop a cluster.\n\n   ```bash\n   kubectl apply -f - \n\n\n   ```bash\n   kubectl edit cluster mycluster -n demo\n   ```\n\n   Configure the value of `spec.componentSpecs.replicas` as 0 to delete pods.\n\n   ```yaml\n   ...\n   spec:\n     clusterDefinitionRef: kafka\n     clusterVersionRef: kafka-3.3.2\n     terminationPolicy: Delete\n     componentSpecs:\n     - name: kafka\n       componentDefRef: kafka\n       disableExporter: true  \n       replicas: 0 # Change this value\n   ...\n   ```\n\n   \n\n\n   ```bash\n   kbcli cluster stop mycluster -n demo\n   ```\n\n   \n\n   \n\n2. Check the status of the cluster to see whether it is stopped.\n\n\n\n   ```bash\n   kubectl get cluster mycluster -n demo\n   ```\n\n   \n\n\n   ```bash\n   kbcli cluster list -n demo\n   ```\n\n   \n\n   \n\n## Start a cluster\n  \n1. Configure the name of your cluster and run the command below to start this cluster.\n\n\n\n   Apply an OpsRequest to start the cluster.\n\n   ```bash\n   kubectl apply -f - \n\n\n   ```bash\n   kubectl edit cluster mycluster -n demo\n   ```\n\n   Change the value of `spec.componentSpecs.replicas` back to the original amount to start this cluster again.\n\n   ```yaml\n   ...\n   spec:\n     clusterDefinitionRef: kafka\n     clusterVersionRef: kafka-3.3.2\n     terminationPolicy: Delete\n     componentSpecs:\n     - name: kafka\n       componentDefRef: kafka\n       disableExporter: true   \n       replicas: 1 # Change this value\n   ...\n   ```\n\n   \n\n\n   ```bash\n   kbcli cluster start mycluster -n demo\n   ```\n\n   \n\n   \n\n2. Check t",
    "path": "docs/release-0_9/kubeblocks-for-kafka/cluster-management/start-stop-a-cluster",
    "description": "  # Stop/Start a Kafka Cluster  You can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. Start this cluster again if you want to "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-kafka_configuration_configuration",
    "title": "Configure cluster parameters",
    "content": "\n\n# Configure cluster parameters\n\nThe KubeBlocks configuration function provides a set of consistent default configuration generation strategies for all the databases running on KubeBlocks and also provides a unified parameter configuration interface to facilitate managing parameter configuration, searching the parameter user guide, and validating parameter effectiveness.\n\nFrom v0.6.0, KubeBlocks supports `kbcli cluster configure` and `kbcli cluster edit-config` to configure parameters. The difference is that KubeBlocks configures parameters automatically with `kbcli cluster configure` but `kbcli cluster edit-config` provides a visualized way for you to edit parameters directly.\n\nKubeBlocks also supports configuring a cluster by editing the configuration file or applying an OpsRequest.\n\n\n\nKubeBlocks supports configuring cluster parameters by editing its configuration file.\n\n1. Get the configuration file of this cluster.\n\n   ```bash\n   kubectl get configurations.apps.kubeblocks.io -n demo\n\n   kubectl edit configurations.apps.kubeblocks.io mycluster-kafka-combine -n demo\n   ```\n\n2. Configure parameters according to your needs. The example below adds the `spec.configFileParams` part to configure `log.cleanup.policy`.\n\n   ```yaml\n   spec:\n     clusterRef: mycluster\n     componentName: kafka\n     configItemDetails:\n     - configFileParams:\n         server.properties:\n           parameters:\n             log.cleanup.policy: \"compact\"\n       configSpec:\n         constraintRef: kafka-cc\n         name: kafka-configuration-tpl\n         namespace: kb-system\n         templateRef: kafka-configuration-tpl\n         volumeName: kafka-config\n       name: kafka-configuration-tpl\n   ```\n\n3. Connect to this cluster to verify whether the configuration takes effect as expected.\n\n   ```bash\n   kbcli cluster describe-config mycluster -n demo --show-detail | grep log.cleanup.policy\n   >\n   log.cleanup.policy = compact\n   ```\n\n:::note\n\nJust in case you cannot find the configuration file of yo",
    "path": "docs/release-0_9/kubeblocks-for-kafka/configuration/configuration",
    "description": "  # Configure cluster parameters  The KubeBlocks configuration function provides a set of consistent default configuration generation strategies for all the databases running on KubeBlocks and also provides a unified parameter configuration interface to facilitate managing parameter configuration, s"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mongodb_cluster-management_create-and-connect-to-a-mongodb-cluster",
    "title": "Create and connect to a MongoDB Cluster",
    "content": "\n\n# Create and connect to a MongoDB cluster\n\nThis tutorial shows how to create and connect to a MongoDB cluster.\n\n## Create a MongoDB cluster\n\n### Before you start\n\n* [Install kbcli](./../../user_docs/installation/install-kbcli) if you want to create and connect a MySQL cluster by `kbcli`.\n* [Install KubeBlocks](./../../user_docs/installation/install-kubeblocks).\n* Make sure the MongoDB Addon is enabled. If this addon is not enabled, [enable it](./../../user_docs/installation/install-addons) first.\n\n\n\n  ```bash\n  kubectl get addons.extensions.kubeblocks.io mongodb\n  >\n  NAME      TYPE   VERSION   PROVIDER   STATUS    AGE\n  mongodb   Helm                        Enabled   23m\n  ```\n\n  \n\n\n  ```bash\n  kbcli addon list\n  >\n  NAME                           TYPE   STATUS     EXTRAS         AUTO-INSTALL\n  ...\n  mongodb                        Helm   Enabled                   true\n  ...\n  ```\n\n  \n\n  \n\n* View all the database types and versions available for creating a cluster.\n\n\n\n  ```bash\n  kubectl get clusterdefinition mongodb\n  >\n  NAME      TOPOLOGIES   SERVICEREFS   STATUS      AGE\n  mongodb                              Available   23m\n  ```\n\n  ```bash\n  kubectl get clusterversions -l clusterdefinition.kubeblocks.io/name=mongodb\n  ```\n\n  \n\n\n  ```bash\n  kbcli clusterdefinition list\n\n  kbcli clusterversion list\n  ```\n\n  \n\n  \n\n* To keep things isolated, create a separate namespace called `demo` throughout this tutorial.\n\n  ```bash\n  kubectl create namespace demo\n  >\n  namespace/demo created\n  ```\n\n### Create a cluster\n\nKubeBlocks supports creating two types of MongoDB clusters: Standalone and ReplicaSet. Standalone only supports one replica and can be used in scenarios with lower requirements for availability. For scenarios with high availability requirements, it is recommended to create a ReplicaSet, which creates a cluster with two replicas to support automatic failover. To ensure high availability, all replicas are distributed on different nodes by default.\n\n\n\n1. Create ",
    "path": "docs/release-0_9/kubeblocks-for-mongodb/cluster-management/create-and-connect-to-a-mongodb-cluster",
    "description": "  # Create and connect to a MongoDB cluster  This tutorial shows how to create and connect to a MongoDB cluster.  ## Create a MongoDB cluster  ### Before you start  * [Install kbcli](./../../user_docs/installation/install-kbcli) if you want to create and connect a MySQL cluster by `kbcli`. * [Instal"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mongodb_cluster-management_delete-mongodb-cluster",
    "title": "Delete a MongoDB Cluster",
    "content": "\n\n# Delete a MongoDB cluster\n\n## Termination policy\n\n:::note\n\nThe termination policy determines how a cluster is deleted. Set the policy when creating a cluster.\n\n:::\n\n| **terminationPolicy** | **Deleting Operation**                           |\n|:----------------------|:-------------------------------------------------|\n| `DoNotTerminate`      | `DoNotTerminate` blocks delete operation.        |\n| `Halt`                | `Halt` deletes Cluster resources like Pods and Services but retains Persistent Volume Claims (PVCs), allowing for data preservation while stopping other operations. Halt policy is deprecated in v0.9.1 and will have same meaning as DoNotTerminate. |\n| `Delete`              | `Delete` extends the Halt policy by also removing PVCs, leading to a thorough cleanup while removing all persistent data.   |\n| `WipeOut`             | `WipeOut` deletes all Cluster resources, including volume snapshots and backups in external storage. This results in complete data removal and should be used cautiously, especially in non-production environments, to avoid irreversible data loss.   |\n\nTo check the termination policy, execute the following command.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n>\nNAME        CLUSTER-DEFINITION   VERSION       TERMINATION-POLICY   STATUS    AGE\nmycluster   mongodb              mongodb-5.0   Delete               Running   17m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n```\n\n\n\n\n\n## Steps\n\nRun the command below to delete a specified cluster.\n\n\n\n```bash\nkubectl delete -n demo cluster mycluster\n```\n\nIf you want to delete a cluster and its all related resources, you can set the termination policy to `WipeOut`, and then delete the cluster.\n\n```bash\nkubectl patch -n demo cluster mycluster -p '}' --type=\"merge\"\n\nkubectl delete -n demo cluster mycluster\n```\n\n\n\n\n```bash\nkbcli cluster delete mycluster -n demo\n```\n\n\n\n\n",
    "path": "docs/release-0_9/kubeblocks-for-mongodb/cluster-management/delete-mongodb-cluster",
    "description": "  # Delete a MongoDB cluster  ## Termination policy  :::note  The termination policy determines how a cluster is deleted. Set the policy when creating a cluster.  :::  | **terminationPolicy** | **Deleting Operation**                           | |:----------------------|:-----------------------------"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mongodb_cluster-management_expand-volume",
    "title": "Expand volume",
    "content": "\n\n# Expand volume\n\nYou can expand the storage volume size of each pod.\n\n## Before you start\n\nCheck whether the cluster STATUS is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n>\nNAME        CLUSTER-DEFINITION   VERSION       TERMINATION-POLICY   STATUS    AGE\nmycluster   mongodb              mongodb-5.0   Delete               Running   27m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME             NAMESPACE        CLUSTER-DEFINITION    VERSION            TERMINATION-POLICY        STATUS         CREATED-TIME\nmycluster        demo             mongodb               mongodb-5.0        Delete                    Running        Apr 10,2023 16:20 UTC+0800\n```\n\n\n\n\n\n## Steps\n\n\n\n1. Apply an OpsRequest. Change the value of storage according to your need and run the command below to expand the volume of a cluster.\n\n   ```bash\n   kubectl apply -f - \n   NAMESPACE   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        ops-volume-expansion   VolumeExpansion   mycluster   Succeed   3/3        6m\n   ```\n\n3. Check whether the corresponding cluster resources change.\n\n   ```bash\n   kubectl describe cluster mycluster -n demo\n   >\n   ...\n   Volume Claim Templates:\n      Name:  data\n      Spec:\n        Access Modes:\n          ReadWriteOnce\n        Resources:\n          Requests:\n            Storage:   40Gi\n   ```\n\n\n\n\n1. Change the value of `spec.components.volumeClaimTemplates.spec.resources` in the cluster YAML file. `spec.components.volumeClaimTemplates.spec.resources` is the storage resource information of the pod and changing this value triggers the volume expansion of a cluster.\n\n   ```yaml\n   apiVersion: apps.kubeblocks.io/v1alpha1\n   kind: Cluster\n   metadata:\n     name: mycluster\n     namespace: demo\n   spec:\n     clusterDefinitionRef: mongodb\n     clusterVersionRef: mongodb-5.0\n     componentSpecs:\n     - name: mongodb \n       componentDefRef: mongodb\n       replicas: 1\n  ",
    "path": "docs/release-0_9/kubeblocks-for-mongodb/cluster-management/expand-volume",
    "description": "  # Expand volume  You can expand the storage volume size of each pod.  ## Before you start  Check whether the cluster STATUS is `Running`. Otherwise, the following operations may fail.    ```bash kubectl get cluster mycluster -n demo > NAME        CLUSTER-DEFINITION   VERSION       TERMINATION-POLI"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mongodb_cluster-management_restart-mongodb-cluster",
    "title": "Restart a MongoDB cluster",
    "content": "\n\n# Restart MongoDB cluster\n\nYou can restart all pods of the cluster. When an exception occurs in a database, you can try to restart it.\n\n## Steps\n\n\n\n1. Create an OpsRequest to restart a cluster.\n\n   ```bash\n   kubectl apply -f - \n   NAME                  READY   STATUS            RESTARTS   AGE\n   mycluster-mongodb-0   3/4     Terminating       0          5m32s\n\n   kubectl get ops ops-restart -n demo\n   >\n   NAME          TYPE      CLUSTER     STATUS    PROGRESS   AGE\n   ops-restart   Restart   mycluster   Succeed   1/1        3m26s\n   ```\n\n   During the restarting process, there are two status types for pods.\n\n   - STATUS=Terminating: it means the cluster restart is in progress.\n   - STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n1. Restart a cluster with `kbcli cluster restart` command and enter the cluster name again.\n\n   ```bash\n   kbcli cluster restart mycluster -n demo\n   >\n   OpsRequest mongodb-cluster-restart-pzsbj created successfully, you can view the progress:\n         kbcli cluster describe-ops mongodb-cluster-restart-pzsbj -n demo\n   ```\n\n2. Validate the restart operation.\n\n   Check the cluster status to identify the restart status.\n\n   ```bash\n   kbcli cluster list mycluster -n demo\n   >\n   NAME                   NAMESPACE        CLUSTER-DEFINITION        VERSION            TERMINATION-POLICY        STATUS         CREATED-TIME\n   mongodb-cluster        demo             mongodb                   mongodb-5.0        Delete                    Running        Apr 26,2023 12:50 UTC+0800\n   ```\n\n   - STATUS=Updating: it means the cluster restart is in progress.\n   - STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n",
    "path": "docs/release-0_9/kubeblocks-for-mongodb/cluster-management/restart-mongodb-cluster",
    "description": "  # Restart MongoDB cluster  You can restart all pods of the cluster. When an exception occurs in a database, you can try to restart it.  ## Steps    1. Create an OpsRequest to restart a cluster.     ```bash    kubectl apply -f -     NAME                  READY   STATUS            RESTARTS   AGE    "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mongodb_cluster-management_scale-for-mongodb",
    "title": "Scale for MongoDB cluster",
    "content": "\n\n# Scale a MongoDB cluster\n\nYou can scale a MongoDB cluster in two ways, vertical scaling and horizontal scaling.\n\n## Vertical scaling\n\nYou can vertically scale a cluster by changing resource requirements and limits (CPU and storage). For example, you can change the resource class from 1C2G to 2C4G by performing vertical scaling.\n\n### Before you start\n\nCheck whether the cluster status is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n>\nNAME        CLUSTER-DEFINITION   VERSION       TERMINATION-POLICY   STATUS    AGE\nmycluster   mongodb              mongodb-5.0   Delete               Running   27m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME             NAMESPACE        CLUSTER-DEFINITION    VERSION            TERMINATION-POLICY        STATUS         CREATED-TIME\nmycluster        demo             mongodb               mongodb-5.0        Delete                    Running        Apr 10,2023 16:20 UTC+0800\n```\n\n\n\n\n\n### Steps\n\n\n\n1. Apply an OpsRequest to the specified cluster. Configure the parameters according to your needs.\n\n   ```bash\n   kubectl apply -f - \n   NAMESPACE   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        ops-vertical-scaling   VerticalScaling   mycluster   Succeed   3/3        6m\n   ```\n\n   If an error occurs to the vertical scaling operation, you can troubleshoot with `kubectl describe ops -n demo` command to view the events of this operation.\n\n3. Check whether the corresponding resources change.\n\n   ```bash\n   kubectl describe cluster mycluster -n demo\n   >\n   ...\n   Component Specs:\n    Component Def Ref:  mongodb\n    Enabled Logs:\n      running\n    DisableExporter:   true\n    Name:      mongodb\n    Replicas:  1\n    Resources:\n      Limits:\n        Cpu:     2\n        Memory:  4Gi\n      Requests:\n        Cpu:     1\n        Memory:  2Gi\n   ```\n\n\n\n\n1. Change the configuration of `spec.componentSpecs.resources` in the YAML file.\n\n   `sp",
    "path": "docs/release-0_9/kubeblocks-for-mongodb/cluster-management/scale-for-mongodb",
    "description": "  # Scale a MongoDB cluster  You can scale a MongoDB cluster in two ways, vertical scaling and horizontal scaling.  ## Vertical scaling  You can vertically scale a cluster by changing resource requirements and limits (CPU and storage). For example, you can change the resource class from 1C2G to 2C4G"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mongodb_cluster-management_start-stop-a-cluster",
    "title": "Stop/Start a MongoDB cluster",
    "content": "\n\n# Stop/Start a MongoDB Cluster\n\nYou can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. You can start this cluster again to restore it to the state it was in before it was stopped.\n\n## Stop a cluster\n\n***Steps:***\n\n1. Configure the name of your cluster and run the command below to stop this cluster.\n\n\n\n    Apply an OpsRequest to stop a cluster.\n\n    ```bash\n    kubectl apply -f - \n\n\n    Configure replicas as 0 to delete pods.\n\n    ```yaml\n    apiVersion: apps.kubeblocks.io/v1alpha1\n    kind: Cluster\n    metadata:\n      name: mycluster\n      namespace: demo\n    spec:\n      clusterDefinitionRef: mongodb\n      clusterVersionRef: mongodb-5.0\n      terminationPolicy: Delete\n      componentSpecs:\n      - name: mongodb\n        componentDefRef: mongodb\n        disableExporter: true  \n        replicas: 0\n        volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: standard\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster stop mycluster -n demo\n    ```\n\n    \n\n    \n\n2. Check the status of the cluster to see whether it is stopped.\n\n\n\n    ```bash\n    kubectl get cluster mycluster -n demo\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster list mycluster -n demo\n    ```\n\n    \n\n    \n\n## Start a cluster\n  \n1. Configure the name of your cluster and run the command below to start this cluster.\n\n\n\n    Apply an OpsRequest to start a cluster.\n\n    ```bash\n    kubectl apply -f - \n\n\n    Change replicas back to the original amount to start this cluster again.\n\n    ```yaml\n    apiVersion: apps.kubeblocks.io/v1alpha1\n    kind: Cluster\n    metadata:\n      name: mycluster\n      namespace: demo\n    spec:\n      clusterDefinitionRef: mongodb\n      clusterVersionRef: mon",
    "path": "docs/release-0_9/kubeblocks-for-mongodb/cluster-management/start-stop-a-cluster",
    "description": "  # Stop/Start a MongoDB Cluster  You can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. You can start this cluster again to re"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mongodb_cluster-management_switchover",
    "title": "Switch over a MongoDB cluster",
    "content": "\n\n# Switch over a MongoDB cluster\n\nYou can initiate a switchover for a MongoDB ReplicaSet. Then KubeBlocks modifies the instance roles.\n\n## Before you start\n\n* Make sure the cluster is running normally.\n* Check whether the following role probe parameters exist to verify whether the role probe is enabled.\n\n   ```bash\n   kubectl get cd mongodb -o yaml\n   >\n   probes:\n     roleProbe:\n       failureThreshold: 3\n       periodSeconds: 2\n       timeoutSeconds: 2\n   ```\n\n## Initiate the switchover\n\nYou can switch over a secondary of a MongoDB ReplicaSet to the primary role, and the former primary instance to a secondary.\n\n\n\nThe value of `instanceName` decides whether a new primary instance is specified for the switchover.\n\n* Switchover with no specified primary instance\n\n  ```yaml\n  kubectl apply -f ->\n  ```\n\n* Switchover with a specified new primary instance\n\n  ```yaml\n  kubectl apply -f ->\n  ```\n\n\n\n\n* Switchover with no primary instance specified\n\n    ```bash\n    kbcli cluster promote mycluster -n demo\n    ```\n\n* Switchover with a specified new primary instance\n\n    ```bash\n    kbcli cluster promote mycluster --instance='mycluster-mongodb-2' -n demo\n    ```\n\n* If there are multiple components, you can use `--components` to specify a component.\n\n    ```bash\n    kbcli cluster promote mycluster --instance='mycluster-mongodb-2' --components='mongodb' -n demo\n    ```\n\n\n\n\n\n## Verify the switchover\n\nCheck the instance status to verify whether the switchover is performed successfully.\n\n\n\n```bash\nkubectl get pods -n demo\n```\n\n\n\n\n```bash\nkbcli cluster list-instances -n demo\n```\n\n\n\n\n\n## Handle an exception\n\nIf an error occurs, refer to [Handle an exaception](./../../handle-an-exception/handle-a-cluster-exception.md) to troubleshoot the operation.\n",
    "path": "docs/release-0_9/kubeblocks-for-mongodb/cluster-management/switchover",
    "description": "  # Switch over a MongoDB cluster  You can initiate a switchover for a MongoDB ReplicaSet. Then KubeBlocks modifies the instance roles.  ## Before you start  * Make sure the cluster is running normally. * Check whether the following role probe parameters exist to verify whether the role probe is ena"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mongodb_configuration_configuration",
    "title": "Configure cluster parameters",
    "content": "\n\n# Configure cluster parameters\n\nThe KubeBlocks configuration function provides a set of consistent default configuration generation strategies for all the databases running on KubeBlocks and also provides a unified parameter configuration interface to facilitate managing parameter configuration, searching the parameter user guide, and validating parameter effectiveness.\n\nFrom v0.6.0, KubeBlocks supports `kbcli cluster configure` and `kbcli cluster edit-config` to configure parameters. The difference is that KubeBlocks configures parameters automatically with `kbcli cluster configure` but `kbcli cluster edit-config` provides a visualized way for you to edit parameters directly.\n\n\n\nKubeBlocks supports configuring cluster parameters by editing its configuration file.\n\n1. Get the configuration file of this cluster.\n\n   ```bash\n   kubectl edit configurations.apps.kubeblocks.io mycluster-mongodb -n demo\n   ```\n\n2. Configure parameters according to your needs. The example below adds the `spec.configFileParams` part to configure `systemLog.verbosity`.\n\n   ```yaml\n   spec:\n     clusterRef: mycluster\n     componentName: mongodb\n     configItemDetails:\n     - configFileParams:\n         mongodb.cnf:\n           parameters:\n             systemLog.verbosity: \"1\"\n       configSpec:\n         constraintRef: mongodb-config-constraints\n         name: mongodb-configuration\n         namespace: kb-system\n         templateRef: mongodb5.0-config-template\n         volumeName: mongodb-config\n       name: mongodb-config\n     - configSpec:\n         defaultMode: 292\n   ```\n\n3. Connect to this cluster to verify whether the configuration takes effect as expected.\n\n      ```bash\n      kubectl exec -ti -n demo mycluster-mongodb-0 -- bash\n\n      root@mycluster-mongodb-0:/# cat etc/mongodb/mongodb.conf |grep verbosity\n      >\n        verbosity: 1\n      ```\n\n:::note\n\nJust in case you cannot find the configuration file of your cluster, you can switch to the `kbcli` tab to view the current configuratio",
    "path": "docs/release-0_9/kubeblocks-for-mongodb/configuration/configuration",
    "description": "  # Configure cluster parameters  The KubeBlocks configuration function provides a set of consistent default configuration generation strategies for all the databases running on KubeBlocks and also provides a unified parameter configuration interface to facilitate managing parameter configuration, s"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mysql_configuration_configuration",
    "title": "Configure cluster parameters",
    "content": "\n\n# Configure cluster parameters\n\nThis guide shows how to configure cluster parameters.\n\nFrom v0.9.0, KubeBlocks supports dynamic configuration. When the specification of a database instance changes (e.g. a user vertically scales a cluster), KubeBlocks automatically matches the appropriate configuration template based on the new specification. This is because different specifications of a database instance may require different optimal configurations to optimize performance and resource utilization. When you choose a different database instance specification, KubeBlocks automatically detects it and determines the best database configuration for the new specification, ensuring optimal performance and configuration of the database under the new specifications.\n\nThis feature simplifies the process of configuring parameters, which saves you from manually configuring database parameters as KubeBlocks handles the updates and configurations automatically to adapt to the new specifications. This saves time and effort and reduces performance issues caused by incorrect configuration.\n\nBut it's also important to note that the dynamic parameter configuration doesn't apply to all parameters. Some parameters may require manual configuration. Additionally, if you have manually modified database parameters before, KubeBlocks may overwrite your customized configurations when updating the database configuration template. Therefore, when using the dynamic configuration feature, it is recommended to back up and record your custom configuration so that you can restore them if needed.\n\n\n\nKubeBlocks supports configuring cluster parameters by editing its configuration file.\n\n1. Get the configuration file of this cluster.\n\n   ```bash\n   kubectl edit configurations.apps.kubeblocks.io mycluster-mysql -n demo\n   ```\n\n2. Configure parameters according to your needs. The example below adds the `spec.configFileParams` part to configure `max_connections`.\n\n   ```yaml\n   spec:\n     clusterRef: mycl",
    "path": "docs/release-0_9/kubeblocks-for-mysql/configuration/configuration",
    "description": "  # Configure cluster parameters  This guide shows how to configure cluster parameters.  From v0.9.0, KubeBlocks supports dynamic configuration. When the specification of a database instance changes (e.g. a user vertically scales a cluster), KubeBlocks automatically matches the appropriate configura"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mysql_cluster-management_create-and-connect-a-mysql-cluster",
    "title": "Create and connect to a MySQL Cluster",
    "content": "\n\n# Create and connect to a MySQL cluster\n\nThis tutorial shows how to create and connect to a MySQL cluster.\n\n## Create a MySQL cluster\n\n### Before you start\n\n* [Install kbcli](./../../user_docs/installation/install-kbcli).\n* [Install KubeBlocks](./../../user_docs/installation/install-kubeblocks).\n* Make sure the MySQL Addon is enabled. The MySQL Addon is installed and enabled by KubeBlocks by default. If you disable it when installing KubeBlocks, [enable it](./../../user_docs/installation/install-addons) first.\n\n\n\n  ```bash\n  kubectl get addons.extensions.kubeblocks.io mysql\n  >\n  NAME    TYPE   VERSION   PROVIDER   STATUS    AGE\n  mysql   Helm                        Enabled   27h\n  ```\n\n  \n\n\n  ```bash\n  kbcli addon list\n  >\n  NAME                           VERSION         PROVIDER    STATUS     AUTO-INSTALL\n  ...\n  mysql                          0.9.1           community   Enabled    true\n  ...\n  ```\n\n  \n\n  \n\n* View all the database types and versions available for creating a cluster.\n\n\n\n  Make sure the `mysql` cluster definition is installed.\n\n  ```bash\n  kubectl get clusterdefinition mysql\n  >\n  NAME             TOPOLOGIES   SERVICEREFS   STATUS      AGE\n  mysql                                       Available   85m\n  ```\n\n  View all available versions for creating a cluster.\n\n  ```bash\n  kubectl get clusterversions -l clusterdefinition.kubeblocks.io/name=mysql\n  >\n  NAME           CLUSTER-DEFINITION   STATUS      AGE\n  mysql-5.7.44   mysql                Available   27h\n  mysql-8.0.33   mysql                Available   27h\n  mysql-8.4.2    mysql                Available   27h\n  ```\n\n  \n\n\n  ```bash\n  kbcli clusterdefinition list\n  kbcli clusterversion list\n  ```\n\n  \n\n  \n\n* To keep things isolated, create a separate namespace called `demo` throughout this tutorial.\n\n  ```bash\n  kubectl create namespace demo\n  ```\n\n### Create a cluster\n\nKubeBlocks supports creating two types of MySQL clusters: Standalone and Replication Cluster. Standalone only supports one replica",
    "path": "docs/release-0_9/kubeblocks-for-mysql/cluster-management/create-and-connect-a-mysql-cluster",
    "description": "  # Create and connect to a MySQL cluster  This tutorial shows how to create and connect to a MySQL cluster.  ## Create a MySQL cluster  ### Before you start  * [Install kbcli](./../../user_docs/installation/install-kbcli). * [Install KubeBlocks](./../../user_docs/installation/install-kubeblocks). *"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mysql_cluster-management_delete-mysql-cluster",
    "title": "Delete a MySQL Cluster",
    "content": "\n\n# Delete a MySQL Cluster\n\n## Termination policy\n\n:::note\n\nThe termination policy determines how you delete a cluster.\n\n:::\n\n| **terminationPolicy** | **Deleting Operation**                           |\n|:----------------------|:-------------------------------------------------|\n| `DoNotTerminate`      | `DoNotTerminate` blocks delete operation.        |\n| `Halt`                | `Halt` deletes Cluster resources like Pods and Services but retains Persistent Volume Claims (PVCs), allowing for data preservation while stopping other operations. Halt policy is deprecated in v0.9.1 and will have same meaning as DoNotTerminate. |\n| `Delete`              | `Delete` extends the Halt policy by also removing PVCs, leading to a thorough cleanup while removing all persistent data.   |\n| `WipeOut`             | `WipeOut` deletes all Cluster resources, including volume snapshots and backups in external storage. This results in complete data removal and should be used cautiously, especially in non-production environments, to avoid irreversible data loss.   |\n\nTo check the termination policy, execute the following command.\n\n\n\n```bash\nkubectl -n demo get cluster mycluster\n>\nNAME        CLUSTER-DEFINITION   VERSION        TERMINATION-POLICY   STATUS    AGE\nmycluster   mysql                mysql-8.0.30   Delete               Running   67m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION        TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        mysql                mysql-8.0.33   Delete               Running   Jul 05,2024 18:46 UTC+0800\n```\n\n\n\n\n\n## Step\n\nRun the command below to delete a specified cluster.\n\n\n\n```bash\nkubectl delete cluster mycluster -n demo\n```\n\nIf you want to delete a cluster and its all related resources, you can modify the termination policy to `WipeOut`, then delete the cluster.\n\n```bash\nkubectl patch -n demo cluster mycluster -p '}' --type=\"merge\"\n\nkubectl delete -n demo cluster mycluster\n",
    "path": "docs/release-0_9/kubeblocks-for-mysql/cluster-management/delete-mysql-cluster",
    "description": "  # Delete a MySQL Cluster  ## Termination policy  :::note  The termination policy determines how you delete a cluster.  :::  | **terminationPolicy** | **Deleting Operation**                           | |:----------------------|:-------------------------------------------------| | `DoNotTerminate`  "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mysql_cluster-management_expand-volume",
    "title": "Expand volume",
    "content": "\n\n# Expand volume\n\nYou can expand the storage volume size of each pod.\n\n## Before you start\n\nCheck whether the cluster status is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n>\nNAME        CLUSTER-DEFINITION   VERSION        TERMINATION-POLICY   STATUS    AGE\nmycluster   mysql                mysql-8.0.33   Delete               Running   4d18h\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION        TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        mysql                mysql-8.0.33   Delete               Running   Jul 05,2024 18:46 UTC+0800\n```\n\n\n\n\n\n## Steps\n\n\n\n1. Apply an OpsRequest. Change the value of storage according to your need and run the command below to expand the volume of a cluster.\n\n   ```bash\n   kubectl apply -f - \n   NAMESPACE   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        ops-volume-expansion   VolumeExpansion   mycluster   Succeed   3/3        6m\n   ```\n\n3. Check whether the corresponding cluster resources change.\n\n   ```bash\n   kubectl describe cluster mycluster -n demo\n   >\n   ...\n   Volume Claim Templates:\n      Name:  data\n      Spec:\n        Access Modes:\n          ReadWriteOnce\n        Resources:\n          Requests:\n            Storage: 40Gi\n   ...\n   ```\n\n\n\n\n1. Change the value of `spec.componentSpecs.volumeClaimTemplates.spec.resources` in the cluster YAML file.\n\n   `spec.componentSpecs.volumeClaimTemplates.spec.resources` is the storage resource information of the pod and changing this value triggers the volume expansion of a cluster.\n\n   ```yaml\n   apiVersion: apps.kubeblocks.io/v1alpha1\n   kind: Cluster\n   metadata:\n     name: mycluster\n     namespace: demo\n   spec:\n     clusterDefinitionRef: mysql\n     clusterVersionRef: mysql-8.0.33\n     componentSpecs:\n     - name: mysql\n       componentDefRef: mysql\n       replicas: 2\n       volumeClaimTemplates:\n       - name: d",
    "path": "docs/release-0_9/kubeblocks-for-mysql/cluster-management/expand-volume",
    "description": "  # Expand volume  You can expand the storage volume size of each pod.  ## Before you start  Check whether the cluster status is `Running`. Otherwise, the following operations may fail.    ```bash kubectl get cluster mycluster -n demo > NAME        CLUSTER-DEFINITION   VERSION        TERMINATION-POL"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mysql_cluster-management_restart-mysql-cluster",
    "title": "Restart MySQL cluster",
    "content": "\n\n# Restart MySQL cluster\n\nYou can restart all pods of the cluster. When an exception occurs in a database, you can try to restart it.\n\n:::note\n\nThe pod role may change after the cluster restarts.\n\n:::\n\n## Steps\n\n\n\n1. Restart a cluster.\n\n   ```bash\n   kubectl apply -f - \n   NAME                READY   STATUS           RESTARTS   AGE\n   mycluster-mysql-0   4/4     Running          0          5m32s\n   mycluster-mysql-1   3/4     Terminating      0          6m36s\n\n   kubectl get ops ops-restart -n demo\n   >\n   NAME          TYPE      CLUSTER     STATUS    PROGRESS   AGE\n   ops-restart   Restart   mycluster   Succeed   1/1        3m26s\n   ```\n\n   During the restarting process, there are two status types for pods.\n\n   - STATUS=Terminating: it means the cluster restart is in progress.\n   - STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n1. Restart a cluster.  \n\n   Configure the values of `components` and `ttlSecondsAfterSucceed` and run the command below to restart a specified cluster.\n\n   ```bash\n   kbcli cluster restart mycluster -n demo --components=\"mysql\" --ttlSecondsAfterSucceed=30\n   ```\n\n   - `components` describes the component name that needs to be restarted.\n   - `ttlSecondsAfterSucceed` describes the time to live of an OpsRequest job after the restarting succeeds.\n\n2. Check the cluster status to validate the restarting.\n\n   ```bash\n   kbcli cluster list mycluster -n demo\n   >\n   NAME        NAMESPACE   CLUSTER-DEFINITION   VERSION        TERMINATION-POLICY   STATUS     CREATED-TIME\n   mycluster   demo        mysql                mysql-8.0.33   Delete               Updating   Jul 05,2024 19:01 UTC+0800\n   ```\n\n   - STATUS=Updating: it means the cluster restart is in progress.\n   - STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n",
    "path": "docs/release-0_9/kubeblocks-for-mysql/cluster-management/restart-mysql-cluster",
    "description": "  # Restart MySQL cluster  You can restart all pods of the cluster. When an exception occurs in a database, you can try to restart it.  :::note  The pod role may change after the cluster restarts.  :::  ## Steps    1. Restart a cluster.     ```bash    kubectl apply -f -     NAME                READY"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mysql_cluster-management_scale-for-mysql",
    "title": "Scale for a MySQL cluster",
    "content": "\n\n# Scale a MySQL cluster\n\nYou can scale a MySQL cluster in two ways, vertical scaling and horizontal scaling.\n\n:::note\n\nAfter vertical scaling or horizontal scaling is performed, KubeBlocks automatically matches the appropriate configuration template based on the new specification. This is the KubeBlocks dynamic configuration feature. This feature simplifies the process of configuring parameters, saves time and effort and reduces performance issues caused by incorrect configuration. For detailed instructions, refer to [Configuration](./../configuration/configuration.md).\n\n:::\n\n## Vertical scaling\n\nYou can vertically scale a cluster by changing resource requirements and limits (CPU and storage). For example, you can change the resource class from 1C2G to 2C4G by performing vertical scaling.\n\n### Before you start\n\nCheck whether the cluster status is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl get cluster mycluster\n>\nNAME        CLUSTER-DEFINITION   VERSION        TERMINATION-POLICY   STATUS    AGE\nmycluster   mysql                mysql-8.0.33   Delete               Running   18m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION        TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        mysql                mysql-8.0.33   Delete               Running   Jul 05,2024 18:46 UTC+0800\n```\n\n\n\n\n\n### Steps\n\n\n\n1. Apply an OpsRequest to the specified cluster. Configure the parameters according to your needs.\n\n   ```bash\n   kubectl apply -f - \n   NAMESPACE   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        ops-vertical-scaling   VerticalScaling   mycluster   Succeed   3/3        6m\n   ```\n\n   If an error occurs, you can troubleshoot with `kubectl describe ops -n demo` command to view the events of this operation.\n\n3. Check whether the corresponding resources change.\n\n    ```bash\n    kubectl describe cluster mycluster -n demo\n    ```",
    "path": "docs/release-0_9/kubeblocks-for-mysql/cluster-management/scale-for-mysql",
    "description": "  # Scale a MySQL cluster  You can scale a MySQL cluster in two ways, vertical scaling and horizontal scaling.  :::note  After vertical scaling or horizontal scaling is performed, KubeBlocks automatically matches the appropriate configuration template based on the new specification. This is the Kube"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mysql_cluster-management_stop-start-a-cluster",
    "title": "Stop/Start a MySQL cluster",
    "content": "\n\n# Stop/Start a MySQL cluster\n\nYou can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. You can start this cluster again to restore it to the state it was in before it was stopped.\n\n## Stop a cluster\n\n1. Configure the name of your cluster and run the command below to stop this cluster.\n\n\n\n    Apply an OpsRequest to stop a cluster.\n\n    ```bash\n    kubectl apply -f - \n\n\n    ```bash\n    kubectl edit cluster mycluster -n demo\n    ```\n\n    Configure replicas as 0 to delete pods.\n\n    ```yaml\n    ...\n    spec:\n      clusterDefinitionRef: mysql\n      clusterVersionRef: mysql-8.0.33\n      terminationPolicy: Delete\n      componentSpecs:\n      - name: mysql\n        componentDefRef: mysql\n        disableExporter: true  \n        replicas: 0 # Change this value\n    ...\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster stop mycluster -n demo\n    ```\n\n    \n\n    \n\n2. Check the status of the cluster to see whether it is stopped.\n\n\n\n    ```bash\n    kubectl get cluster mycluster -n demo\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster list mycluster -n demo\n    ```\n\n    \n\n    \n\n## Start a cluster\n\n1. Configure the name of your cluster and run the command below to start this cluster.\n\n\n\n    Apply an OpsRequest to start a cluster.\n\n    ```bash\n    kubectl apply -f - \n\n\n    ```bash\n    kubectl edit cluster mycluster -n demo\n    ```\n\n    Change replicas back to the original amount to start this cluster again.\n\n    ```yaml\n    ...\n    spec:\n      clusterDefinitionRef: mysql\n      clusterVersionRef: mysql-8.0.33\n      terminationPolicy: Delete\n      componentSpecs:\n      - name: mysql\n        componentDefRef: mysql\n        disableExporter: true  \n        replicas: 2 # Change this value\n    ...\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster start mycluster -n demo\n    ```\n\n    \n\n    \n\n2. Check the status of the cluster to see whe",
    "path": "docs/release-0_9/kubeblocks-for-mysql/cluster-management/stop-start-a-cluster",
    "description": "  # Stop/Start a MySQL cluster  You can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. You can start this cluster again to rest"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mysql_cluster-management_switchover",
    "title": "Switch over a MySQL cluster",
    "content": "\n\n# Switch over a MySQL cluster\n\nYou can initiate a switchover for a MySQL Replication Cluster. Then KubeBlocks switches the instance roles.\n\n## Before you start\n\n* Make sure the cluster is running normally.\n* Check whether the following role probe parameters exist to verify whether the role probe is enabled.\n\n   ```bash\n   kubectl get cd apecloud-mysql -o yaml\n   >\n   probes:\n     roleProbe:\n       failureThreshold: 2\n       periodSeconds: 1\n       timeoutSeconds: 1\n   ```\n\n## Initiate the switchover\n\nYou can switch over a secondary of a MySQL Replication to the primary role, and the former primary instance to a secondary one.\n\n\n\nThe value of `instanceName` decides whether a new primary instance is specified for the switchover.\n\n* Initiate a switchover with no specified primary instance.\n\n  ```yaml\n  kubectl apply -f ->\n  ```\n\n* Initiate a switchover with a specified new primary instance.\n\n  ```yaml\n  kubectl apply -f ->\n  ```\n\n\n\n\n* Initiate a switchover with a specified new primary instance.\n\n    ```bash\n    kbcli cluster promote mycluster --instance='mycluster-mysql-1' -n demo\n    ```\n\n* If there are multiple components, you can use `--components` to specify a component.\n\n    ```bash\n    kbcli cluster promote mycluster --instance='mycluster-mysql-1' --components='apecloud-mysql' -n demo\n    ```\n\n\n\n\n\n## Verify the switchover\n\nCheck the instance status to verify whether the switchover is performed successfully.\n\n\n\n```bash\nkubectl get pods -n demo\n```\n\n\n\n\n```bash\nkbcli cluster list-instances -n demo\n```\n\n\n\n\n\n## Handle an exception\n\nIf an error occurs, refer to [Handle an exception](./../../handle-an-exception/handle-a-cluster-exception.md) to troubleshoot the operation.\n",
    "path": "docs/release-0_9/kubeblocks-for-mysql/cluster-management/switchover",
    "description": "  # Switch over a MySQL cluster  You can initiate a switchover for a MySQL Replication Cluster. Then KubeBlocks switches the instance roles.  ## Before you start  * Make sure the cluster is running normally. * Check whether the following role probe parameters exist to verify whether the role probe i"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-mysql_high-availability_high-availability",
    "title": "Failure simulation and automatic recovery",
    "content": "\n\n# Failure simulation and automatic recovery\n\nAs an open-source data management platform, Kubeblocks currently supports over thirty database engines and is continuously expanding. Due to the varying high availability capabilities of databases, KubeBlocks has designed and implemented a high availability (HA) system for database instances. The KubeBlocks HA system uses a unified HA framework to provide high availability for databases, allowing different databases on KubeBlocks to achieve similar high availability capabilities and experiences.\n\nThis tutorial uses MySQL Community edition as an example to demonstrate its fault simulation and recovery capabilities.\n\n## Recovery simulation\n\n:::note\n\nThe faults here are all simulated by deleting a pod. When there are sufficient resources, the fault can also be simulated by machine downtime or container deletion, and its automatic recovery is the same as described here.\n\n:::\n\n### Before you start\n\n* [Install KubeBlocks](./../../user_docs/installation/install-kubeblocks).\n* Create a MySQL Replication Cluster, refer to [Create a MySQL cluster](./../cluster-management/create-and-connect-a-mysql-cluster.md).\n* Run `kubectl get cd mysql -o yaml` to check whether _rolechangedprobe_ is enabled in the MySQL Replication (it is enabled by default). If the following configuration exists, it indicates that it is enabled:\n\n  ```bash\n  probes:\n    roleProbe:\n      failureThreshold: 2\n      periodSeconds: 1\n      timeoutSeconds: 1\n  ```\n\n### Primary pod fault\n\n***Steps:***\n\n\n\n1. View the pod role of the MySQL Replication Cluster. In this example, the primary pod's name is `mycluster-mysql-0`.\n\n    ```bash\n    kubectl get pods --show-labels -n demo | grep role\n    ```\n\n    ![describe_pod](/img/docs/en/api-mysql-ha-grep-role.png)\n2. Delete the primary pod `mycluster-mysql-0` to simulate a pod fault.\n\n    ```bash\n    kubectl delete pod mycluster-mysql-0 -n demo\n    ```\n\n    ![delete_pod](/img/docs/en/api-mysql-ha-delete-primary-pod.png)\n3. C",
    "path": "docs/release-0_9/kubeblocks-for-mysql/high-availability/high-availability",
    "description": "  # Failure simulation and automatic recovery  As an open-source data management platform, Kubeblocks currently supports over thirty database engines and is continuously expanding. Due to the varying high availability capabilities of databases, KubeBlocks has designed and implemented a high availabi"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-postgresql_configuration_configuration",
    "title": "Configure cluster parameters",
    "content": "\n\n# Configure cluster parameters\n\nThis guide shows how to configure cluster parameters.\n\nFrom v0.9.0, KubeBlocks supports dynamic configuration. When the specification of a database instance changes (e.g. a user vertically scales a cluster), KubeBlocks automatically matches the appropriate configuration template based on the new specification. This is because different specifications of a database instance may require different optimal configurations to optimize performance and resource utilization. When you choose a different database instance specification, KubeBlocks automatically detects it and determines the best database configuration for the new specification, ensuring optimal performance and configuration of the database under the new specifications.\n\nThis feature simplifies the process of configuring parameters, which saves you from manually configuring database parameters as KubeBlocks handles the updates and configurations automatically to adapt to the new specifications. This saves time and effort and reduces performance issues caused by incorrect configuration.\n\nBut it's also important to note that the dynamic parameter configuration doesn't apply to all parameters. Some parameters may require manual configuration. Additionally, if you have manually modified database parameters before, KubeBlocks may overwrite your customized configurations when updating the database configuration template. Therefore, when using the dynamic configuration feature, it is recommended to back up and record your custom configuration so that you can restore them if needed.\n\n\n\nKubeBlocks supports configuring cluster parameters by editing its configuration file.\n\n1. Get the configuration file of this cluster.\n\n   ```bash\n   kubectl edit configurations.apps.kubeblocks.io mycluster-postgresql -n demo\n   ```\n\n2. Configure parameters according to your needs. The example below adds the `spec.configFileParams` part to configure `max_connections`.\n\n   ```yaml\n   spec:\n     clusterRef:",
    "path": "docs/release-0_9/kubeblocks-for-postgresql/configuration/configuration",
    "description": "  # Configure cluster parameters  This guide shows how to configure cluster parameters.  From v0.9.0, KubeBlocks supports dynamic configuration. When the specification of a database instance changes (e.g. a user vertically scales a cluster), KubeBlocks automatically matches the appropriate configura"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-postgresql_cluster-management_create-and-connect-a-postgresql-cluster",
    "title": "Create and connect to a PostgreSQL Cluster",
    "content": "\n\n# Create and connect to a PostgreSQL cluster\n\nThis tutorial shows how to create and connect to a PostgreSQL cluster.\n\n## Create a PostgreSQL cluster\n\n### Before you start\n\n* [Install kbcli](./../../user_docs/installation/install-kbcli) if you want to manage the PostgreSQL cluster by `kbcli`.\n* [Install KubeBlocks](./../../user_docs/installation/install-kubeblocks).\n* Make sure the PostgreSQL Addon is enabled. The PostgreSQL Addon is installed and enabled  by KubeBlocks by default. But if you disable it when installing KubeBlocks, [enable it](./../../user_docs/installation/install-addons) first.\n\n\n\n  ```bash\n  kubectl get addons.extensions.kubeblocks.io postgresql\n  >\n  NAME         TOPOLOGIES   SERVICEREFS   STATUS      AGE\n  postgresql                              Available   30m\n  ```\n\n  \n\n\n  ```bash\n  kbcli addon list\n  >\n  NAME                       TYPE   STATUS     EXTRAS         AUTO-INSTALL\n  ...\n  postgresql                 Helm   Enabled                   true\n  ...\n  ```\n\n  \n\n  \n\n* View all the database types and versions available for creating a cluster.\n\n\n\n  ```bash\n  kubectl get clusterdefinition postgresql\n  >\n  NAME         TOPOLOGIES   SERVICEREFS   STATUS      AGE\n  postgresql                              Available   30m\n  ```\n\n  View all available versions for creating a cluster.\n\n  ```bash\n  kubectl get clusterversions -l clusterdefinition.kubeblocks.io/name=postgresql\n  >\n  NAME                 CLUSTER-DEFINITION   STATUS      AGE\n  postgresql-12.14.0   postgresql           Available   30m\n  postgresql-12.14.1   postgresql           Available   30m\n  postgresql-12.15.0   postgresql           Available   30m\n  postgresql-14.7.2    postgresql           Available   30m\n  postgresql-14.8.0    postgresql           Available   30m\n  postgresql-15.7.0    postgresql           Available   30m\n  postgresql-16.4.0    postgresql           Available   30m\n  ```\n\n  \n\n\n  ```bash\n  kbcli clusterdefinition list\n  kbcli clusterversion list\n  ```\n\n  \n\n  \n\n* To k",
    "path": "docs/release-0_9/kubeblocks-for-postgresql/cluster-management/create-and-connect-a-postgresql-cluster",
    "description": "  # Create and connect to a PostgreSQL cluster  This tutorial shows how to create and connect to a PostgreSQL cluster.  ## Create a PostgreSQL cluster  ### Before you start  * [Install kbcli](./../../user_docs/installation/install-kbcli) if you want to manage the PostgreSQL cluster by `kbcli`. * [In"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-postgresql_cluster-management_delete-a-postgresql-cluster",
    "title": "Delete a PostgreSQL Cluster",
    "content": "\n\n# Delete a PostgreSQL Cluster\n\n:::note\n\nThe termination policy determines how a cluster is deleted.\n\n:::\n\n## Termination policy\n\n| **terminationPolicy** | **Deleting Operation**                           |\n|:----------------------|:-------------------------------------------------|\n| `DoNotTerminate`      | `DoNotTerminate` blocks delete operation.        |\n| `Halt`                | `Halt` deletes Cluster resources like Pods and Services but retains Persistent Volume Claims (PVCs), allowing for data preservation while stopping other operations. Halt policy is deprecated in v0.9.1 and will have same meaning as DoNotTerminate. |\n| `Delete`              | `Delete` extends the Halt policy by also removing PVCs, leading to a thorough cleanup while removing all persistent data.   |\n| `WipeOut`             | `WipeOut` deletes all Cluster resources, including volume snapshots and backups in external storage. This results in complete data removal and should be used cautiously, especially in non-production environments, to avoid irreversible data loss.   |\n\nTo check the termination policy, execute the following command.\n\n\n\n```bash\nkubectl -n demo get cluster mycluster\n>\nNAME        CLUSTER-DEFINITION   VERSION             TERMINATION-POLICY   STATUS    AGE\nmycluster   postgresql           postgresql-14.8.0   Delete               Running   29m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION             TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        postgresql           postgresql-14.8.0   Delete               Running   Sep 28,2024 16:47 UTC+0800\n```\n\n\n\n\n\n## Step\n\nRun the command below to delete a specified cluster.\n\n\n\n```bash\nkubectl delete cluster mycluster -n demo\n```\n\nIf you want to delete a cluster and its all related resources, you can modify the termination policy to `WipeOut`, and then delete the cluster.\n\n```bash\nkubectl patch -n demo cluster mycluster -p '}' --type=\"merge\"\n\nkubectl dele",
    "path": "docs/release-0_9/kubeblocks-for-postgresql/cluster-management/delete-a-postgresql-cluster",
    "description": "  # Delete a PostgreSQL Cluster  :::note  The termination policy determines how a cluster is deleted.  :::  ## Termination policy  | **terminationPolicy** | **Deleting Operation**                           | |:----------------------|:-------------------------------------------------| | `DoNotTermina"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-postgresql_cluster-management_expand-volume",
    "title": "Expand volume",
    "content": "\n\n# Expand volume\n\nYou can expand the storage volume size of each pod.\n\n## Before you start\n\nCheck whether the cluster STATUS is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl -n demo get cluster mycluster\n>\nNAME        CLUSTER-DEFINITION   VERSION             TERMINATION-POLICY   STATUS    AGE\nmycluster   postgresql           postgresql-14.8.0   Delete               Running   29m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION             TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        postgresql           postgresql-14.8.0   Delete               Running   Sep 28,2024 16:47 UTC+0800\n```\n\n\n\n\n\n## Steps\n\n\n\n1. Apply an OpsRequest. Change the value of storage according to your need and run the command below to expand the volume of a cluster.\n\n   ```bash\n   kubectl apply -f - \n   NAMESPACE   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        ops-volume-expansion   VolumeExpansion   mycluster   Succeed   3/3        6m\n   ```\n\n3. Check whether the corresponding cluster resources change.\n\n   ```bash\n   kubectl describe cluster mycluster -n demo\n   >\n   ...\n   Volume Claim Templates:\n      Name:  data\n      Spec:\n        Access Modes:\n          ReadWriteOnce\n        Resources:\n          Requests:\n            Storage:   40Gi\n   ```\n\n\n\n\n1. Change the value of `spec.components.volumeClaimTemplates.spec.resources` in the cluster YAML file. \n\n   `spec.components.volumeClaimTemplates.spec.resources` is the storage resource information of the pod and changing this value triggers the volume expansion of a cluster.\n\n   ```yaml\n   apiVersion: apps.kubeblocks.io/v1alpha1\n   kind: Cluster\n   metadata:\n     name: mycluster\n     namespace: demo\n   spec:\n     clusterDefinitionRef: postgresql\n     clusterVersionRef: postgresql-14.8.0\n     componentSpecs:\n     - name: postgresql\n       componentDefRef: postgresql\n       replicas: 1\n       volu",
    "path": "docs/release-0_9/kubeblocks-for-postgresql/cluster-management/expand-volume",
    "description": "  # Expand volume  You can expand the storage volume size of each pod.  ## Before you start  Check whether the cluster STATUS is `Running`. Otherwise, the following operations may fail.    ```bash kubectl -n demo get cluster mycluster > NAME        CLUSTER-DEFINITION   VERSION             TERMINATIO"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-postgresql_cluster-management_restart-a-postgresql-cluster",
    "title": "Restart PostgreSQL cluster",
    "content": "\n\n# Restart PostgreSQL cluster\n\nYou can restart all pods of the cluster. When an exception occurs in a database, you can try to restart it.\n\n:::note\n\nThe pod role may change after the cluster restarts.\n\n:::\n\n## Steps\n\n\n\n1. Restart a cluster.\n\n   ```bash\n   kubectl apply -f - \n   NAME                     READY   STATUS            RESTARTS   AGE\n   mycluster-postgresql-0   3/4     Terminating       0          5m32s\n   mycluster-postgresql-1   4/4     Running           0          6m36s\n\n   kubectl get ops ops-restart -n demo\n   >\n   NAME          TYPE      CLUSTER     STATUS    PROGRESS   AGE\n   ops-restart   Restart   mycluster   Succeed   1/1        3m26s\n   ```\n\n   During the restarting process, there are two status types for pods.\n\n   - STATUS=Terminating: it means the cluster restart is in progress.\n   - STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n1. Configure the values of `components` and `ttlSecondsAfterSucceed` and run the command below to restart a specified cluster.\n\n   ```bash\n   kbcli cluster restart mycluster -n demo --components=\"postgresql\" --ttlSecondsAfterSucceed=30\n   ```\n\n   - `components` describes the component name that needs to be restarted.\n   - `ttlSecondsAfterSucceed` describes the time to live of an OpsRequest job after the restarting succeeds.\n\n2. Validate the restarting.\n\n   Run the command below to check the cluster status to check the restarting status.\n\n   ```bash\n   kbcli cluster list mycluster -n demo\n   >\n   NAME       NAMESPACE   CLUSTER-DEFINITION          VERSION             TERMINATION-POLICY   STATUS    CREATED-TIME\n   mycluster   demo       postgresql                  postgresql-14.8.0   Delete               Running   Sep 28,2024 16:57 UTC+0800\n   ```\n\n   * STATUS=Updating: it means the cluster restart is in progress.\n   * STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n",
    "path": "docs/release-0_9/kubeblocks-for-postgresql/cluster-management/restart-a-postgresql-cluster",
    "description": "  # Restart PostgreSQL cluster  You can restart all pods of the cluster. When an exception occurs in a database, you can try to restart it.  :::note  The pod role may change after the cluster restarts.  :::  ## Steps    1. Restart a cluster.     ```bash    kubectl apply -f -     NAME                "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-postgresql_cluster-management_scale-for-a-postgresql-cluster",
    "title": "Scale for a PostgreSQL cluster",
    "content": "\n\n# Scale a PostgreSQL cluster\n\nYou can scale a PostgreSQL cluster in two ways, vertical scaling and horizontal scaling.\n\n:::note\n\nAfter vertical scaling or horizontal scaling is performed, KubeBlocks automatically matches the appropriate configuration template based on the new specification. This is the KubeBlocks dynamic configuration feature. This feature simplifies the process of configuring parameters, saves time and effort and reduces performance issues caused by incorrect configuration. For detailed instructions, refer to [Configuration](./../configuration/configuration.md).\n\n:::\n\n## Vertical scaling\n\nYou can vertically scale a cluster by changing resource requirements and limits (CPU and storage). For example, you can change the resource class from 1C2G to 2C4G by performing vertical scaling.\n\n### Before you start\n\nCheck whether the cluster status is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl -n demo get cluster mycluster\n>\nNAME        CLUSTER-DEFINITION   VERSION             TERMINATION-POLICY   STATUS    AGE\nmycluster   postgresql           postgresql-14.8.0   Delete               Running   29m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION             TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        postgresql           postgresql-14.8.0   Delete               Running   Sep 28,2024 16:47 UTC+0800\n```\n\n\n\n\n\n### Steps\n\n\n\n1. Apply an OpsRequest to the specified cluster. Configure the parameters according to your needs.\n\n   ```yaml\n   kubectl apply -f - \n   NAMESPACE   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        ops-vertical-scaling   VerticalScaling   mycluster   Succeed   3/3        6m\n   ```\n\n   If an error occurs, you can troubleshoot with `kubectl describe ops -n demo` command to view the events of this operation.\n\n3. Check whether the corresponding resources change.\n\n   ```bash\n   kubectl descri",
    "path": "docs/release-0_9/kubeblocks-for-postgresql/cluster-management/scale-for-a-postgresql-cluster",
    "description": "  # Scale a PostgreSQL cluster  You can scale a PostgreSQL cluster in two ways, vertical scaling and horizontal scaling.  :::note  After vertical scaling or horizontal scaling is performed, KubeBlocks automatically matches the appropriate configuration template based on the new specification. This i"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-postgresql_cluster-management_start-stop-a-cluster",
    "title": "Stop/Start a PostgreSQL cluster",
    "content": "\n\n# Stop/Start PostgreSQL Cluster\n\nYou can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. You can start this cluster again to restore it to the state it was in before it was stopped.\n\n## Stop a cluster\n\n1. Configure the name of your cluster and run the command below to stop this cluster.\n\n\n\n   Apply an OpsRequest to stop a cluster.\n\n   ```bash\n   kubectl apply -f - \n\n\n   Configure replicas as 0 to delete pods.\n\n   ```bash\n   kubectl edit cluster mycluster -n demo\n   ```\n\n   Edit the value of `spec.componentSpecs.replicas`.\n\n   ```yaml\n   ...\n   spec:\n     clusterDefinitionRef: postgresql\n     clusterVersionRef: postgresql-14.8.0\n     terminationPolicy: Delete\n     componentSpecs:\n     - name: postgresql\n       componentDefRef: postgresql\n       disableExporter: true  \n       replicas: 0 # Change this value\n   ...\n   ```\n\n   \n\n\n   ```bash\n   kbcli cluster stop mycluster -n demo\n   ```\n\n   \n\n   \n\n2. Check the status of the cluster to see whether it is stopped.\n\n\n\n   ```bash\n   kubectl get cluster mycluster -n demo\n   ```\n\n   \n\n\n   ```bash\n   kbcli cluster list -n demo\n   ```\n\n   \n\n   \n\n## Start a cluster\n\n1. Configure the name of your cluster and run the command below to start this cluster.\n\n\n\n   ```bash\n   kubectl apply -f - \n  \n\n   Change replicas back to the original amount to start this cluster again.\n\n   ```bash\n   kubectl edit cluster mycluster -n demo\n   ```\n\n   Edit the value of `spec.componentSpecs.replicas`.\n\n   ```yaml\n   ...\n   spec:\n     clusterDefinitionRef: postgresql\n     clusterVersionRef: postgresql-14.8.0\n     terminationPolicy: Delete\n     componentSpecs:\n     - name: mysql\n       componentDefRef: mysql\n       disableExporter: true\n       replicas: 1 # Change this value\n   ...\n   ```\n\n   \n\n\n   ```bash\n   kbcli cluster start mycluster -n demo\n   ```\n\n   \n\n   \n\n2. Check the status",
    "path": "docs/release-0_9/kubeblocks-for-postgresql/cluster-management/start-stop-a-cluster",
    "description": "  # Stop/Start PostgreSQL Cluster  You can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. You can start this cluster again to r"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-postgresql_cluster-management_switchover",
    "title": "Switch over a PostgreSQL cluster",
    "content": "\n\n# Switch over a PostgreSQL cluster\n\nYou can initiate a switchover for a PostgreSQL Replication Cluster by executing the kbcli or kubectl command. Then KubeBlocks modifies the instance roles.\n\n## Before you start\n\n* Make sure the cluster is running normally.\n* Check whether the following role probe parameters exist to verify whether the role probe is enabled.\n\n   ```bash\n   kubectl get cd postgresql -o yaml\n   >\n   probes:\n     roleProbe:\n       failureThreshold: 2\n       periodSeconds: 1\n       timeoutSeconds: 1\n   ```\n\n## Initiate the switchover\n\nYou can switch over a secondary of a PostgreSQL Replication Cluster to the primary role, and the former primary instance to a secondary.\n\n\n\nThe value of `instanceName` decides whether a new primary instance is specified for the switchover.\n\n* Switchover with no specified primary instance\n\n  ```yaml\n  kubectl apply -f ->\n  ```\n\n* Switchover with a specified new primary instance\n\n  ```yaml\n  kubectl apply -f ->\n  ```\n\n\n\n\n* Switchover with no primary instance specified\n\n    ```bash\n    kbcli cluster promote mycluster -n demo\n    ```\n\n* Switchover with a specified new primary instance\n\n    ```bash\n    kbcli cluster promote mycluster -n demo --instance='mycluster-postgresql-2'\n    ```\n\n* If there are multiple components, you can use `--components` to specify a component.\n\n    ```bash\n    kbcli cluster promote mycluster -n demo --instance='mycluster-postgresql-2' --components='postgresql'\n    ```\n\n\n\n\n\n## Verify the switchover\n\nCheck the instance status to verify whether the switchover is performed successfully.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n\nkubectl -n demo get po -L kubeblocks.io/role \n```\n\n\n\n\n```bash\nkbcli cluster list-instances -n demo\n```\n\n\n\n\n\n## Handle an exception\n\nIf an error occurs, refer to [Handle an exception](./../../handle-an-exception/handle-a-cluster-exception.md) to troubleshoot the operation.\n",
    "path": "docs/release-0_9/kubeblocks-for-postgresql/cluster-management/switchover",
    "description": "  # Switch over a PostgreSQL cluster  You can initiate a switchover for a PostgreSQL Replication Cluster by executing the kbcli or kubectl command. Then KubeBlocks modifies the instance roles.  ## Before you start  * Make sure the cluster is running normally. * Check whether the following role probe"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-postgresql_high-availability_high-availability",
    "title": "High Availability for PostgreSQL",
    "content": "\n\n# High availability\n\nKubeBlocks integrates [the open-source Patroni solution](https://patroni.readthedocs.io/en/latest/) to realize high availability and adopts Noop as the switch policy.\n\n## Before you start\n\n* [Install kbcli](./../../user_docs/installation/install-kbcli) if you want to manage the PostreSQL cluster with `kbcli`.\n* [Install KubeBlocks](./../../user_docs/installation/install-kubeblocks).\n* [Create a PostgreSQL Replication Cluster](./../cluster-management/create-and-connect-a-postgresql-cluster.md#create-a-postgresql-cluster).\n* Check whether the following role probe parameters exist to verify the role probe is enabled.\n\n    ```bash\n    kubectl get cd postgresql -o yaml\n    >\n    probes:\n      roleProbe:\n        failureThreshold: 2\n        periodSeconds: 1\n        timeoutSeconds: 1\n    ```\n\n## Steps\n\n\n\n1. View the initial status of the PostgreSQL cluster and pods.\n\n   ```bash\n   kubectl get cluster mycluster -n demo\n\n   kubectl -n demo get pod -L kubeblocks.io/role\n   ```\n\n   ![PostgreSQL cluster original status](/img/docs/en/api-ha-pg-original-status.png)\n\n   Currently, `mycluster-postgresql-0` is the primary pod and `mycluster-postgresql-1` is the secondary pod.\n\n2. Simulate a primary pod exception.\n\n   ```bash\n   # Enter the primary pod\n   kubectl exec -it mycluster-postgresql-0 -n demo -- bash\n\n   # Delete the data directory of PostgreSQL to simulate an exception\n   root@mycluster-postgresql-0:/home/postgres# rm -fr /home/postgres/pgdata/pgroot/data\n   ```\n\n3. View logs to observe how the roles of pods switch  when an exception occurs.\n\n   ```bash\n   # View the primary pod logs\n   kubectl logs mycluster-postgresql-0 -n demo\n   ```\n\n   In the logs, the leader lock is released from the primary pod and an HA switch occurs.\n\n   ```bash\n   2024-05-17 02:41:23,523 INFO: Lock owner: mycluster-postgresql-0; I am mycluster-postgresql-0\n   2024-05-17 02:41:23,702 INFO: Leader key released\n   2024-05-17 02:41:23,904 INFO: released leader key voluntarily as",
    "path": "docs/release-0_9/kubeblocks-for-postgresql/high-availability/high-availability",
    "description": "  # High availability  KubeBlocks integrates [the open-source Patroni solution](https://patroni.readthedocs.io/en/latest/) to realize high availability and adopts Noop as the switch policy.  ## Before you start  * [Install kbcli](./../../user_docs/installation/install-kbcli) if you want to manage th"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-postgresql_introduction_introduction",
    "title": "PostgreSQL introduction",
    "content": "\n# PostgreSQL introduction\n\nPostgreSQL is a powerful, scalable, secure, and customizable open-source relational database management system that is suitable for various applications and environments. In addition to the PostgreSQL core features, the KubeBlocks PostgreSQL cluster supports a number of extensions and users can also add the extensions by themselves.\n\n* PostGIS: PostGIS is an open-source spatial database extension that adds geographic information system (GIS) functionality to the PostgreSQL database. It provides a set of spatial functions and types that can be used for storing, querying, and analyzing geospatial data, allowing you to run location queries with SQL.\n* pgvector: Pgvector is a PostgreSQL extension that supports vector data types and vector operations. It provides an efficient way to store and query vector data, supporting various vector operations such as similarity search, clustering, classification, and recommendation systems. Pgvector can be used to store AI model embeddings, adding persistent memory to AI.\n* pg_trgm: The pg_trgm module provides functions and operators for determining the similarity of alphanumeric text based on trigram matching, as well as index operator classes that support fast searching for similar strings.\n* postgres_fdw: The postgres_fdw extension can map tables from a remote database to a local table in PostgreSQL, allowing users to query and manipulate the data through the local database.\n\n**Reference**\n\n* [PostgreSQL features](https://www.postgresql.org/about/featurematrix/)\n* [PostGIS](https://postgis.net/)\n* [pgvector](https://github.com/pgvector/pgvector)\n",
    "path": "docs/release-0_9/kubeblocks-for-postgresql/introduction/introduction",
    "description": " # PostgreSQL introduction  PostgreSQL is a powerful, scalable, secure, and customizable open-source relational database management system that is suitable for various applications and environments. In addition to the PostgreSQL core features, the KubeBlocks PostgreSQL cluster supports a number of e"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-postgresql_postgresql-connection-pool_postgresql-connection-pool",
    "title": "PostgreSQL connection pool",
    "content": "\n\n# PostgreSQL connection pool\n\nPostgreSQL adopts a multi-process architecture, which creates a separate backend process for each user connection. When there are too many user connections, it occupies a large amount of memory, which reduces the throughput and stability of the database. To solve these problems, KubeBlocks introduces a connection pool, PgBouncer, for PostgreSQL database clusters.\n\nWhen creating a PostgreSQL cluster with KubeBlocks, PgBouncer is installed by default.\n\n## Steps\n\n\n\n1. View the status of the created PostgreSQL cluster and ensure this cluster is `Running`.\n\n   ```bash\n   kubectl get cluster mycluster -n demo\n   ```\n\n2. Describe the services and there are two connection links in Endpoints.\n\n    Port `5432` is used to connect to the primary pod of this database and port `6432` is used to connect to PgBouncer.\n\n    ```bash\n    kubectl get services mycluster-postgresql -n demo\n    >\n    NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE\n    mycluster-postgresql   ClusterIP   10.97.123.178           5432/TCP,6432/TCP   39m       \n    ```\n\n3. Run the command below to get the `username` and `password` for the `kubectl exec` command.\n\n    ```bash\n    kubectl get secrets -n demo mycluster-conn-credential -o jsonpath='' | base64 -d\n    >\n    postgres\n\n    kubectl get secrets -n demo mycluster-conn-credential -o jsonpath='' | base64 -d\n    >\n    shgkz4z9\n   ```\n\n4. Connect the cluster with PgBouncer. The default example uses port `6432` and you can replace it with port `5432`.\n\n    ```bash\n    kubectl -n demo port-forward service/mycluster-postgresql 6432:6432\n    ```\n\n5. Open a new terminal window and run the `psql` command to connect to PgBouncer.\n\n    Fill the password obtained from step 3 into the `PGPASSWORD`.\n\n    ```bash\n    PGPASSWORD=shgkz4z9 psql -h127.0.0.1 -p 6432 -U postgres postgres\n    ```\n\n6. Run the following command in `psgl` to verify the connection.\n\n   If you can connect to `pgbouncer` and ex",
    "path": "docs/release-0_9/kubeblocks-for-postgresql/postgresql-connection-pool/postgresql-connection-pool",
    "description": "  # PostgreSQL connection pool  PostgreSQL adopts a multi-process architecture, which creates a separate backend process for each user connection. When there are too many user connections, it occupies a large amount of memory, which reduces the throughput and stability of the database. To solve thes"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-pulsar_cluster-management_create-pulsar-cluster-on-kubeblocks",
    "title": "Create a Pulsar Cluster",
    "content": "\n\n## Introduction\n\nKubeBlocks can quickly integrate new engines through good abstraction. The functions tested in KubeBlocks include Pulsar cluster creation and deletion, vertical and horizontal scaling of Pulsar cluster components, storage expansion, restart, and configuration changes.\n\nKubeBlocks supports Pulsar's daily operations, including basic lifecycle operations such as cluster creation, deletion, and restart, as well as advanced operations such as horizontal and vertical scaling, storage expansion, and configuration changes.\n\n## Environment Recommendation\n\nRefer to the [Pulsar official document](https://pulsar.apache.org/docs/3.1.x/) for the configuration, such as memory, cpu, and storage, of each component.\n\n|      Components        |                                 Replicas                                  |\n| :--------------------  | :------------------------------------------------------------------------ |\n|       zookeeper        |          1 for test environment or 3 for production environment           |\n|        bookies         |  at least 3 for test environment, at lease 4 for production environment   |\n|        broker          |      at least 1, for production environment, 3 replicas recommended       |\n| recovery (Optional)    | 1; if autoRecovery is not enabled for bookie, at least 3 replicas needed  |\n|   proxy (Optional)     |           1; and for production environment, 3 replicas needed            |\n\n## Before you start\n\n* [Install kbcli](./../../user_docs/installation/install-kbcli) if you want to manage the StarRocks cluster with `kbcli`.\n* [Install KubeBlocks](./../../user_docs/installation/install-kubeblocks).\n* Check whether the Pulsar Addon is enabled. If this Addon is disabled, [enable it](./../../user_docs/installation/install-addons) first.\n* View all the database types and versions available for creating a cluster.\n\n\n\n  ```bash\n  kubectl get clusterdefinition pulsar\n  >\n  NAME    TOPOLOGIES                                        S",
    "path": "docs/release-0_9/kubeblocks-for-pulsar/cluster-management/create-pulsar-cluster-on-kubeblocks",
    "description": "  ## Introduction  KubeBlocks can quickly integrate new engines through good abstraction. The functions tested in KubeBlocks include Pulsar cluster creation and deletion, vertical and horizontal scaling of Pulsar cluster components, storage expansion, restart, and configuration changes.  KubeBlocks "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-pulsar_cluster-management_delete-a-pulsar-cluster",
    "title": "Delete a PostgreSQL Cluster",
    "content": "\n\n# Delete a Pulsar Cluster\n\n:::note\n\nThe termination policy determines how a cluster is deleted.\n\n:::\n\n## Termination policy\n\n| **terminationPolicy** | **Deleting Operation**                           |\n|:----------------------|:-------------------------------------------------|\n| `DoNotTerminate`      | `DoNotTerminate` blocks delete operation.        |\n| `Halt`                | `Halt` deletes Cluster resources like Pods and Services but retains Persistent Volume Claims (PVCs), allowing for data preservation while stopping other operations. Halt policy is deprecated in v0.9.1 and will have same meaning as DoNotTerminate. |\n| `Delete`              | `Delete` extends the Halt policy by also removing PVCs, leading to a thorough cleanup while removing all persistent data.   |\n| `WipeOut`             | `WipeOut` deletes all Cluster resources, including volume snapshots and backups in external storage. This results in complete data removal and should be used cautiously, especially in non-production environments, to avoid irreversible data loss.   |\n\nTo check the termination policy, execute the following command.\n\n\n\n```bash\nkubectl -n demo get cluster mycluster\n>\nNAME           CLUSTER-DEFINITION   VERSION          TERMINATION-POLICY   STATUS     AGE\nmycluster      pulsar               pulsar-3.0.2     Delete               Running    19m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION             TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        pulsar               pulsar-3.0.2        Delete               Running   Sep 28,2024 16:47 UTC+0800\n```\n\n\n\n\n\n## Step\n\nRun the command below to delete a specified cluster.\n\n\n\n```bash\nkubectl delete cluster mycluster -n demo\n```\n\nIf you want to delete a cluster and its all related resources, you can modify the termination policy to `WipeOut`, and then delete the cluster.\n\n```bash\nkubectl patch -n demo cluster mycluster -p '}' --type=\"merge\"\n\nkubectl delete",
    "path": "docs/release-0_9/kubeblocks-for-pulsar/cluster-management/delete-a-pulsar-cluster",
    "description": "  # Delete a Pulsar Cluster  :::note  The termination policy determines how a cluster is deleted.  :::  ## Termination policy  | **terminationPolicy** | **Deleting Operation**                           | |:----------------------|:-------------------------------------------------| | `DoNotTerminate` "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-pulsar_cluster-management_expand-volume",
    "title": "Expand volume",
    "content": "\n\n# Expand volume\n\nYou can expand the storage volume size of each pod.\n\n## Before you start\n\nCheck whether the cluster status is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n```\n\n\n\n\n\n## Steps\n\n\n\n1. Apply an OpsRequest. Change the value of storage according to your need and run the command below to expand the volume of a cluster.\n\n   ```bash\n   kubectl apply -f - \n   NAMESPACE   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        ops-volume-expansion   VolumeExpansion   mycluster   Succeed   3/3        6m\n   ```\n\n3. Check whether the corresponding cluster resources change.\n\n    ```bash\n    kubectl describe cluster mycluster -n demo\n    ```\n\n\n\n\n1. Change the value of `spec.components.volumeClaimTemplates.spec.resources` in the cluster YAML file. \n\n   `spec.components.volumeClaimTemplates.spec.resources` is the storage resource information of the pod and changing this value triggers the volume expansion of a cluster.\n\n   ```yaml\n   apiVersion: apps.kubeblocks.io/v1alpha1\n   kind: Cluster\n   metadata:\n     name: mycluster\n     namespace: demo\n   spec:\n     clusterDefinitionRef: pulsar\n     clusterVersionRef: pulsar-3.0.2\n     componentSpecs:\n     - name: pulsar\n       componentDefRef: pulsar\n       replicas: 1\n       volumeClaimTemplates:\n       - name: data\n         spec:\n           accessModes:\n             - ReadWriteOnce\n           resources:\n             requests:\n               storage: 40Gi # Change the volume storage size\n     terminationPolicy: Delete\n   ```\n\n2. Check whether the corresponding cluster resources change.\n\n    ```bash\n    kubectl describe cluster mycluster -n demo\n    ```\n\n\n\n\n1. Configure the values of `--components`, `--volume-claim-templates`, and `--storage`, and run the command below to expand the volume.\n\n    :::note\n\n    Expand volume for `journal` first. `ledger` volume expansion must be ",
    "path": "docs/release-0_9/kubeblocks-for-pulsar/cluster-management/expand-volume",
    "description": "  # Expand volume  You can expand the storage volume size of each pod.  ## Before you start  Check whether the cluster status is `Running`. Otherwise, the following operations may fail.    ```bash kubectl get cluster mycluster -n demo ```     ```bash kbcli cluster list mycluster -n demo ```      ## "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-pulsar_cluster-management_restart-a-pulsar-cluster",
    "title": "Restart Pulsar cluster",
    "content": "\n\n# Restart a Pulsar cluster\n\nYou can restart all pods of the cluster. When an exception occurs in a database, you can try to restart it.\n\n:::note\n\nThe pod role may change after the cluster restarts.\n\n:::\n\n## Steps\n\n\n\n1. Restart a cluster.\n\n   ```bash\n   kubectl apply -f - \n   NAME          TYPE      CLUSTER     STATUS    PROGRESS   AGE\n   ops-restart   Restart   mycluster   Succeed   1/1        3m26s\n   ```\n\n   During the restarting process, there are two status types for pods.\n\n   - STATUS=Terminating: it means the cluster restart is in progress.\n   - STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n1. Configure the values of `components` and `ttlSecondsAfterSucceed` and run the command below to restart a specified cluster.\n\n   ```bash\n   kbcli cluster restart mycluster -n demo --components=\"pulsar\" --ttlSecondsAfterSucceed=30\n   ```\n\n   - `components` describes the component name that needs to be restarted.\n   - `ttlSecondsAfterSucceed` describes the time to live of an OpsRequest job after the restarting succeeds.\n\n2. Validate the restarting.\n\n   Run the command below to check the cluster status to check the restarting status.\n\n   ```bash\n   kbcli cluster list mycluster -n demo\n   >\n   NAME           CLUSTER-DEFINITION   VERSION          TERMINATION-POLICY   STATUS     AGE\n   mycluster      pulsar               pulsar-3.0.2     Delete               Running    19m\n   ```\n\n   * STATUS=Updating: it means the cluster restart is in progress.\n   * STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n",
    "path": "docs/release-0_9/kubeblocks-for-pulsar/cluster-management/restart-a-pulsar-cluster",
    "description": "  # Restart a Pulsar cluster  You can restart all pods of the cluster. When an exception occurs in a database, you can try to restart it.  :::note  The pod role may change after the cluster restarts.  :::  ## Steps    1. Restart a cluster.     ```bash    kubectl apply -f -     NAME          TYPE    "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-pulsar_cluster-management_scale-for-pulsar",
    "title": "Scale for a Pulsar",
    "content": "\n\n# Scale for a Pulsar cluster\n\n## Vertical scaling\n\nYou can vertically scale a cluster by changing resource requirements and limits (CPU and storage). For example, you can change the resource class from 1C2G to 2C4G by performing vertical scaling.\n\n### Before you start\n\nCheck whether the cluster status is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl get cluster mycluster -n demo\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n```\n\n\n\n\n\n### Steps\n\n\n\n1. Apply an OpsRequest to the specified cluster. Configure the parameters according to your needs.\n\n   ```bash\n   kubectl create -f -\n   NAMESPACE   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        ops-vertical-scaling   VerticalScaling   mycluster   Succeed   3/3        6m\n   ```\n\n   If an error occurs, you can troubleshoot with `kubectl describe ops -n demo` command to view the events of this operation.\n\n3. Check whether the corresponding resources change.\n\n    ```bash\n    kubectl describe cluster mycluster -n demo\n    ```\n\n\n\n\n1. Change the configuration of `spec.componentSpecs.resources` in the YAML file.\n\n   `spec.componentSpecs.resources` controls the requirement and limit of resources and changing them triggers a vertical scaling.\n\n   ```bash\n   kubectl edit cluster mycluster -n demo\n   ```\n\n   Edit the value of `spec.componentSpecs.resources`.\n\n   ```yaml\n   ...\n   spec:\n     affinity:\n       podAntiAffinity: Preferred\n       topologyKeys:\n       - kubernetes.io/hostname\n     clusterDefinitionRef: pulsar\n     clusterVersionRef: pulsar-3.0.2\n     componentSpecs:\n     - componentDefRef: pulsar\n       enabledLogs:\n       - running\n       disableExporter: true\n       name: pulsar\n       replicas: 1\n       resources: # Change values of resources\n         limits:\n           cpu: \"2\"\n           memory: 4Gi\n         requests:\n           cpu: \"1\"\n           memory: 2Gi\n   ```\n\n2. Check whether the corresponding resources change.\n\n   ```bash\n   k",
    "path": "docs/release-0_9/kubeblocks-for-pulsar/cluster-management/scale-for-pulsar",
    "description": "  # Scale for a Pulsar cluster  ## Vertical scaling  You can vertically scale a cluster by changing resource requirements and limits (CPU and storage). For example, you can change the resource class from 1C2G to 2C4G by performing vertical scaling.  ### Before you start  Check whether the cluster st"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-pulsar_cluster-management_stop-start-a-pulsar-cluster",
    "title": "Stop/Start a Pulsar cluster",
    "content": "\n\n# Stop/Start a Pulsar Cluster\n\nYou can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. Start this cluster again if you want to restore the cluster resources from the original storage by snapshots.\n\n## Stop a cluster\n\n1. Configure the name of your cluster and run the command below to stop this cluster.\n\n\n\n    Apply an OpsRequest to stop a cluster.\n\n    ```bash\n    kubectl apply -f - \n\n\n    ```bash\n    kubectl edit cluster mycluster -n demo\n    ```\n\n    Configure replicas as 0 to delete pods.\n\n    ```yaml\n    ...\n    spec:\n      clusterDefinitionRef: pulsar\n      clusterVersionRef: pulsar-3.0.2\n      terminationPolicy: Delete\n      componentSpecs:\n      - name: pulsar\n        componentDefRef: pulsar\n        disableExporter: true \n        replicas: 0\n    ...\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster stop mycluster -n demo\n    ```\n\n    \n\n    \n\n2. Check the status of the cluster to see whether it is stopped.\n\n\n\n    ```bash\n    kubectl get cluster mycluster -n demo\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster list mycluster -n demo\n    ```\n\n    \n\n    \n\n## Start a cluster\n  \n1. Configure the name of your cluster and run the command below to start this cluster.\n\n\n\n    Apply an OpsRequest to start a cluster.\n\n    ```bash\n    kubectl apply -f - \n\n\n    ```bash\n    kubectl edit cluster mycluster \n    ```\n\n    Change replicas back to the original amount to start this cluster again.\n\n    ```yaml\n    ...\n    spec:\n      clusterDefinitionRef: pulsar\n      clusterVersionRef: pulsar-3.0.2\n      terminationPolicy: Delete\n      componentSpecs:\n      - name: pulsar\n        componentDefRef: pulsar\n        disableExporter: true  \n        replicas: 1\n    ...\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster start mycluster -n demo\n    ```\n\n    \n\n    \n\n2. Check the status of the cluster to see whether it is running aga",
    "path": "docs/release-0_9/kubeblocks-for-pulsar/cluster-management/stop-start-a-pulsar-cluster",
    "description": "  # Stop/Start a Pulsar Cluster  You can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. Start this cluster again if you want to"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-pulsar_configuration_configuration",
    "title": "Configure cluster parameters",
    "content": "\n\n# Configure cluster parameters\n\nFrom v0.6.0, KubeBlocks supports `kbcli cluster configure` and `kbcli cluster edit-config` to configure parameters. The difference is that KubeBlocks configures parameters automatically with `kbcli cluster configure` but `kbcli cluster edit-config` provides a visualized way for you to edit parameters directly.\n\nThere are 3 types of parameters:\n\n1. Environment parameters, such as GC-related parameters, `PULSAR_MEM`, and `PULSAR_GC`, changes will apply to all components;\n2. Configuration parameters, such as `zookeeper` or `bookies.conf` configuration files, can be changed through `env` and changes restart the pod;\n3. Dynamic parameters, such as configuration files in `brokers.conf`, `broker` supports two types of change modes:\n    a. Parameter change requires a restart, such as `zookeeperSessionExpiredPolicy`;\n    b. For parameters that support dynamic parameters, you can obtain a list of all dynamic parameters with `pulsar-admin brokers list-dynamic-config`.\n\n:::note\n\n`pulsar-admin` is a management tool built in the Pulsar cluster. You can log in to the corresponding pod with `kubectl exec -it  -- bash` (pod-name can be checked by `kubectl get pods` command, and you can choose any pod with the word `broker` in its name ), and there are corresponding commands in the `/pulsar/bin path` of the pod. For more information about pulsar-admin, please refer to the [official documentation](https://pulsar.apache.org/docs/3.0.x/admin-api-tools/\n).\n:::\n\n\n\nKubeBlocks supports configuring cluster parameters by configuration file.\n\n1. Modify the Pulsar `broker.conf` file, in this case, it is `pulsar-broker-broker-config`.\n\n   ```bash\n   kubectl edit cm pulsar-broker-broker-config -n demo\n   ```\n\n2. Check whether the configuration is done.\n\n   ```bash\n   kubectl get pod -l app.kubernetes.io/name=pulsar-broker -n dmo\n   ```\n\n:::note\n\nJust in case you cannot find the configuration file of your cluster, you can use switch to the `kbcli` tab to view the ",
    "path": "docs/release-0_9/kubeblocks-for-pulsar/configuration/configuration",
    "description": "  # Configure cluster parameters  From v0.6.0, KubeBlocks supports `kbcli cluster configure` and `kbcli cluster edit-config` to configure parameters. The difference is that KubeBlocks configures parameters automatically with `kbcli cluster configure` but `kbcli cluster edit-config` provides a visual"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-redis_cluster-management_create-and-connect-a-redis-cluster",
    "title": "Create and connect to a Redis Cluster",
    "content": "\n\n# Create and Connect to a Redis cluster\n\nThis tutorial shows how to create and connect to a Redis cluster.\n\n## Create a Redis cluster\n\n### Before you start\n\n* [Install kbcli](./../../user_docs/installation/install-kbcli) if you want to create a Redis cluster by `kbcli`.\n* [Install KubeBlocks](./../../user_docs/installation/install-kubeblocks).\n* Make sure the Redis Addon is enabled. The Redis Addon is enabled by KubeBlocks by default. If you disable it when installing KubeBlocks, [enable it](./../../user_docs/installation/install-addons) first.\n\n\n\n  ```bash\n  kubectl get addons.extensions.kubeblocks.io redis\n  >\n  NAME      TYPE   VERSION   PROVIDER   STATUS    AGE\n  redis     Helm                        Enabled   61m\n  ```\n\n  \n\n\n  ```bash\n  kbcli addon list\n  >\n  NAME                      TYPE   STATUS     EXTRAS         AUTO-INSTALL\n  ...\n  redis                     Helm   Enabled                   true\n  ...\n  ```\n\n  \n\n  \n\n* View all the database types and versions available for creating a cluster.\n\n\n\n  ```bash\n  kubectl get clusterdefinition redis\n  >\n  NAME    TOPOLOGIES                                              SERVICEREFS   STATUS      AGE\n  redis   replication,replication-twemproxy,standalone                          Available   16m\n  ```\n\n  ```bash\n  kubectl get clusterversions -l clusterdefinition.kubeblocks.io/name=redis\n  >\n  NAME          CLUSTER-DEFINITION   STATUS      AGE\n  redis-7.0.6   redis                Available   16m\n  redis-7.2.4   redis                Available   16m\n  ```\n\n  \n\n\n  ```bash\n  kbcli clusterdefinition list\n  >\n  NAME               TOPOLOGIES                                              SERVICEREFS   STATUS      AGE\n  redis              replication,replication-twemproxy,standalone                          Available   16m\n\n  kbcli clusterversion list\n  >\n  NAME                 CLUSTER-DEFINITION   STATUS      IS-DEFAULT   CREATED-TIME\n  redis-7.0.6          redis                Available   false        Sep 27,2024 11:36 UTC+0",
    "path": "docs/release-0_9/kubeblocks-for-redis/cluster-management/create-and-connect-a-redis-cluster",
    "description": "  # Create and Connect to a Redis cluster  This tutorial shows how to create and connect to a Redis cluster.  ## Create a Redis cluster  ### Before you start  * [Install kbcli](./../../user_docs/installation/install-kbcli) if you want to create a Redis cluster by `kbcli`. * [Install KubeBlocks](./.."
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-redis_cluster-management_delete-a-redis-cluster",
    "title": "Delete a Redis Cluster",
    "content": "\n\n# Delete a Redis Cluster\n\n## Termination policy\n\n:::note\n\nThe termination policy determines how a cluster is deleted.\n\n:::\n\n| **terminationPolicy** | **Deleting Operation**                           |\n|:----------------------|:-------------------------------------------------|\n| `DoNotTerminate`      | `DoNotTerminate` blocks delete operation.        |\n| `Halt`                | `Halt` deletes Cluster resources like Pods and Services but retains Persistent Volume Claims (PVCs), allowing for data preservation while stopping other operations. Halt policy is deprecated in v0.9.1 and will have same meaning as DoNotTerminate. |\n| `Delete`              | `Delete` extends the Halt policy by also removing PVCs, leading to a thorough cleanup while removing all persistent data.   |\n| `WipeOut`             | `WipeOut` deletes all Cluster resources, including volume snapshots and backups in external storage. This results in complete data removal and should be used cautiously, especially in non-production environments, to avoid irreversible data loss.   |\n\nTo check the termination policy, execute the following command.\n\n\n\n```bash\nkubectl -n demo get cluster mycluster\n>\nNAME        CLUSTER-DEFINITION   VERSION       TERMINATION-POLICY   STATUS    AGE\nmycluster   redis                              Delete               Running   10m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION   TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        redis                          Delete               Running   Apr 10,2023 20:27 UTC+0800\n```\n\n\n\n\n\n## Step\n\nRun the command below to delete a specified cluster.\n\n\n\n```bash\nkubectl delete cluster mycluster -n demo\n```\n\nIf you want to delete a cluster and its all related resources, you can modify the termination policy to `WipeOut`, then delete the cluster.\n\n```bash\nkubectl patch -n demo cluster mycluster -p '}' --type=\"merge\"\n\nkubectl delete -n demo cluster mycluster\n```\n\n\n\n\n```bas",
    "path": "docs/release-0_9/kubeblocks-for-redis/cluster-management/delete-a-redis-cluster",
    "description": "  # Delete a Redis Cluster  ## Termination policy  :::note  The termination policy determines how a cluster is deleted.  :::  | **terminationPolicy** | **Deleting Operation**                           | |:----------------------|:-------------------------------------------------| | `DoNotTerminate`  "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-redis_cluster-management_expand-volume",
    "title": "Expand volume",
    "content": "\n\n# Expand volume\n\nYou can expand the storage volume size of each pod.\n\n:::note\n\nVolume expansion triggers a concurrent restart and the pod role may change after the operation.\n\n:::\n\n## Before you start\n\nCheck whether the cluster STATUS is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl -n demo get cluster mycluster\n>\nNAME           CLUSTER-DEFINITION   VERSION        TERMINATION-POLICY   STATUS     AGE\nmycluster      redis                               Delete               Running    19m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION   TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        redis                          Delete               Running   Sep 29,2024 09:46 UTC+0800\n```\n\n\n\n\n\n## Steps\n\n\n\n1. Apply an OpsRequest. Change the value of storage according to your need and run the command below to expand the volume of a cluster.\n\n   ```bash\n   kubectl apply -f - \n   NAMESPACE   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        ops-volume-expansion   VolumeExpansion   mycluster   Succeed   3/3        6m\n   ```\n\n3. Check whether the corresponding cluster resources change.\n\n   ```bash\n   kubectl describe cluster mycluster -n demo\n   >\n   ...\n   Volume Claim Templates:\n      Name:  data\n      Spec:\n        Access Modes:\n          ReadWriteOnce\n        Resources:\n          Requests:\n            Storage:   40Gi\n   ```\n\n\n\n\n1. Change the value of `spec.componentSpecs.volumeClaimTemplates.spec.resources` in the cluster YAML file.\n\n   `spec.componentSpecs.volumeClaimTemplates.spec.resources` is the storage resource information of the pod and changing this value triggers the volume expansion of a cluster.\n\n    ```bash\n    kubectl edit cluster mycluster -n demo\n    ```\n\n    Edit the value of `spec.componentSpecs.volumeClaimTemplates.spec.resources.requests.storage`.\n\n    ```yaml\n    ...\n    spec:\n      affinity:\n        podAntiAffin",
    "path": "docs/release-0_9/kubeblocks-for-redis/cluster-management/expand-volume",
    "description": "  # Expand volume  You can expand the storage volume size of each pod.  :::note  Volume expansion triggers a concurrent restart and the pod role may change after the operation.  :::  ## Before you start  Check whether the cluster STATUS is `Running`. Otherwise, the following operations may fail.    "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-redis_cluster-management_restart-a-redis-cluster",
    "title": "Restart a Redis cluster",
    "content": "\n\n# Restart a Redis cluster\n\nYou can restart all pods of the cluster. When an exception occurs in a database, you can try to restart it.\n\n:::note\n\nThe pod role may change after the cluster restarts.\n\n:::\n\n## Steps\n\n\n\n1. Create an OpsRequest to restart a cluster.\n\n   ```bash\n   kubectl apply -f - \n   NAME                READY   STATUS            RESTARTS   AGE\n   mycluster-redis-0   3/4     Terminating       0          5m32s\n\n   kubectl get ops ops-restart -n demo\n   >\n   NAME          TYPE      CLUSTER     STATUS    PROGRESS   AGE\n   ops-restart   Restart   mycluster   Succeed   1/1        3m26s\n   ```\n\n   During the restarting process, there are two status types for pods.\n\n   - STATUS=Terminating: it means the cluster restart is in progress.\n   - STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n1. Restart a cluster.\n  \n   Configure the values of `components` and `ttlSecondsAfterSucceed` and run the command below to restart a specified cluster.\n\n   ```bash\n   kbcli cluster restart mycluster --components=\"redis\" --ttlSecondsAfterSucceed=30\n   ```\n\n   - `components` describes the component name that needs to be restarted.\n   - `ttlSecondsAfterSucceed` describes the time to live of an OpsRequest job after the restarting succeeds.\n\n2. Validate the restart operation.\n\n   Check the cluster status to identify the restart status.\n\n   ```bash\n   kbcli cluster list mycluster -n demo\n   >\n   NAME        NAMESPACE   CLUSTER-DEFINITION   VERSION   TERMINATION-POLICY   STATUS    CREATED-TIME\n   mycluster   demo        redis                          Delete               Running   Sep 29,2024 09:46 UTC+0800\n   ```\n\n   - STATUS=Updating: it means the cluster restart is in progress.\n   - STATUS=Running: it means the cluster has been restarted.\n\n\n\n\n",
    "path": "docs/release-0_9/kubeblocks-for-redis/cluster-management/restart-a-redis-cluster",
    "description": "  # Restart a Redis cluster  You can restart all pods of the cluster. When an exception occurs in a database, you can try to restart it.  :::note  The pod role may change after the cluster restarts.  :::  ## Steps    1. Create an OpsRequest to restart a cluster.     ```bash    kubectl apply -f -    "
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-redis_cluster-management_scale-for-a-redis-cluster",
    "title": "Scale for a Redis cluster",
    "content": "\n\n# Scale for a Redis cluster\n\nYou can scale Redis DB instances in two ways, vertical scaling and horizontal scaling.\n\n## Vertical scaling\n\nYou can vertically scale a cluster by changing resource requirements and limits (CPU and storage). For example, you can change the resource class from 1C2G to 2C4G by performing vertical scaling.\n\n### Before you start\n\nRun the command below to check whether the cluster STATUS is `Running`. Otherwise, the following operations may fail.\n\n\n\n```bash\nkubectl -n demo get cluster mycluster\n>\nNAME           CLUSTER-DEFINITION   VERSION        TERMINATION-POLICY   STATUS     AGE\nmycluster      redis                               Delete               Running    19m\n```\n\n\n\n\n```bash\nkbcli cluster list mycluster -n demo\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   VERSION   TERMINATION-POLICY   STATUS    CREATED-TIME\nmycluster   demo        redis                          Delete               Running   Sep 29,2024 09:46 UTC+0800\n```\n\n\n\n\n\n### Steps\n\n\n\n1. Apply an OpsRequest to the specified cluster. Configure the parameters according to your needs.\n\n    ```bash\n    kubectl apply -f - \n    NAMESPACE   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n    demo        ops-vertical-scaling   VerticalScaling   mycluster   Succeed   3/3        6m\n    ```\n\n   If an error occurs to the vertical scaling operation, you can troubleshoot with `kubectl describe ops -n demo` command to view the events of this operation.\n\n3. Check whether the corresponding resources change.\n\n    ```bash\n    kubectl describe cluster mycluster -n demo\n    ```\n\n\n\n\n1. Change the configuration of `spec.componentSpecs.resources` in the YAML file.\n\n   `spec.componentSpecs.resources` controls the requests and limits of resources and changing them triggers a vertical scaling.\n\n   ```bash\n   kubectl edit cluster mycluster -n demo\n   ```\n\n   Edit the values of `spec.componentSpecs.resources`.\n\n   ```yaml\n   ...\n   spec:\n     clusterDefinitionRef: redis\n   ",
    "path": "docs/release-0_9/kubeblocks-for-redis/cluster-management/scale-for-a-redis-cluster",
    "description": "  # Scale for a Redis cluster  You can scale Redis DB instances in two ways, vertical scaling and horizontal scaling.  ## Vertical scaling  You can vertically scale a cluster by changing resource requirements and limits (CPU and storage). For example, you can change the resource class from 1C2G to 2"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-redis_cluster-management_stop-start-a-redis-cluster",
    "title": "Stop/Start a Redis cluster",
    "content": "\n\n# Stop/Start a Redis Cluster\n\nYou can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. You can start this cluster again to restore it to the state it was in before it was stopped.\n\n## Stop a cluster\n\n1. Configure the name of your cluster and run the command below to stop this cluster.\n\n\n\n    Run the command below to stop a cluster.\n\n    ```bash\n    kubectl apply -f - \n\n\n    ```bash\n    kubectl edit cluster mycluster -n demo\n    ```\n\n    Configure the values of `replicas` as 0 to delete pods.\n\n    ```yaml\n    ...\n    spec:\n      affinity:\n        podAntiAffinity: Preferred\n        topologyKeys:\n        - kubernetes.io/hostname\n      clusterDefinitionDef: redis\n      componentSpecs:\n      - componentDef: redis-7\n        enabledLogs:\n        - running\n        disableExporter: true\n        name: redis\n        replicas: 0 # Change this value\n        ...\n      - componentDef: redis-sentinel-7\n        name: redis-sentinel\n        replicas: 0 # Change this value\n        ...\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster stop mycluster -n demo\n    ```\n\n    \n\n    \n\n2. Check the status of the cluster to see whether it is stopped.\n\n\n\n    ```bash\n    kubectl get cluster mycluster -n demo\n    ```\n\n    \n\n\n    ```bash\n    kbcli cluster list -n demo\n    ```\n\n    \n\n    \n\n## Start a cluster\n  \n1. Configure the name of your cluster and run the command below to start this cluster.\n\n\n\n    Apply an OpsRequest to start the cluster.\n\n    ```bash\n    kubectl apply -f - \n\n\n    ```bash\n    kubectl edit cluster mycluster -n demo\n    ```\n\n    Change the values of `replicas` back to the original amount to start this cluster again.\n\n    ```yaml\n    ...\n    spec:\n      affinity:\n        podAntiAffinity: Preferred\n        topologyKeys:\n        - kubernetes.io/hostname\n      clusterDefinitionDef: redis\n      componentSpecs:\n      -",
    "path": "docs/release-0_9/kubeblocks-for-redis/cluster-management/stop-start-a-redis-cluster",
    "description": "  # Stop/Start a Redis Cluster  You can stop/start a cluster to save computing resources. When a cluster is stopped, the computing resources of this cluster are released, which means the pods of Kubernetes are released, but the storage resources are reserved. You can start this cluster again to rest"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-redis_configuration_configuration",
    "title": "Configure cluster parameters",
    "content": "\n\n# Configure cluster parameters\n\nThe KubeBlocks configuration function provides a set of consistent default configuration generation strategies for all the databases running on KubeBlocks and also provides a unified parameter configuration interface to facilitate managing parameter configuration, searching the parameter user guide, and validating parameter effectiveness.\n\nFrom v0.6.0, KubeBlocks supports `kbcli cluster configure` and `kbcli cluster edit-config` to configure parameters. The difference is that KubeBlocks configures parameters automatically with `kbcli cluster configure` but `kbcli cluster edit-config` provides a visualized way for you to edit parameters directly.\n\n\n\n1. Get the configuration file of this cluster.\n\n   ```bash\n   kubectl edit configurations.apps.kubeblocks.io mycluster-redis -n demo\n   ```\n\n2. Configure parameters according to your needs. The example below adds the `- configFileParams` part to configure `acllog-max-len`.\n\n    ```yaml\n    spec:\n      clusterRef: mycluster\n      componentName: redis\n      configItemDetails:\n      - configSpec:\n          constraintRef: redis7-config-constraints\n          name: redis-replication-config\n          namespace: demo\n          reRenderResourceTypes:\n          - vscale\n          templateRef: redis7-config-template\n          volumeName: redis-config\n      - configFileParams:\n          redis.conf:\n            parameters:\n              acllog-max-len: \"256\"\n        name: mycluster-redis-redis-replication-config\n    ```\n\n3. Connect to this cluster to verify whether the configuration takes effect.\n\n   1. Get the username and password.\n\n      ```bash\n      kubectl get secrets -n demo mycluster-conn-credential -o jsonpath='' | base64 -d\n      >\n      default\n\n      kubectl get secrets -n demo mycluster-conn-credential -o jsonpath='' | base64 -d\n      >\n      kpz77mcs\n      ```\n\n   2. Connect to this cluster and verify whether the parameters are configured as expected.\n\n      ```bash\n      kubectl exec -t",
    "path": "docs/release-0_9/kubeblocks-for-redis/configuration/configuration",
    "description": "  # Configure cluster parameters  The KubeBlocks configuration function provides a set of consistent default configuration generation strategies for all the databases running on KubeBlocks and also provides a unified parameter configuration interface to facilitate managing parameter configuration, s"
  },
  {
    "id": "docs_en_release-0_9_kubeblocks-for-redis_high-availability_high-availability",
    "title": "High Availability for Redis",
    "content": "\n\n# High availability\n\nKubeBlocks integrates [the official Redis Sentinel solution](https://redis.io/docs/management/sentinel/) to realize high availability and adopts Noop as the switch policy.\n\nRedis Sentinel is the high availability solution for a Redis Replication Cluster, which is recommended by Redis and is also the main-stream solution in the community.\n\nIn the Redis Replication Cluster provided by KubeBlocks, Sentinel is deployed as an independent component.\n\n## Before you start\n\n* [Install KubeBlocks](./../../user_docs/installation/install-kubeblocks).\n* [Create a Redis Replication Cluster](./../cluster-management/create-and-connect-a-redis-cluster.md#create-a-redis-cluster).\n* Check the Switch Policy and the role probe.\n  * Check whether the switch policy is `Noop`.\n\n    ```bash\n    kubectl get cluster redis-cluster -o yaml\n    >\n    spec:\n      componentSpecs:\n      - name: redis\n        componentDefRef: redis\n        switchPolicy:\n          type: Noop\n    ```\n\n  * Check whether the following role probe parameters exist to verify the role probe is enabled.\n\n    ```bash\n    kubectl get cd redis -o yaml\n    >\n    probes:\n      roleProbe:\n        failureThreshold: 2\n        periodSeconds: 2\n        timeoutSeconds: 1\n    ```\n\n## Steps\n\n\n\nThis section takes the cluster `mycluster` in the namespace `demo` as an example.\n\n1. View the initial status of the Redis cluster.\n\n    ```bash\n    kubectl get pods -l kubeblocks.io/role=primary -n demo\n    >\n    NAME                READY   STATUS    RESTARTS   AGE\n    mycluster-redis-0   3/3     Running   0          24m\n\n    kubectl get pods -l kubeblocks.io/role=secondary -n demo\n    >\n    NAME                READY   STATUS    RESTARTS      AGE\n    mycluster-redis-1   3/3     Running   1 (24m ago)   24m\n    ```\n\n   Currently, `mycluster-redis-0` is the primary pod and `mycluster-redis-1` is the secondary pod.\n\n   :::note\n\n   To fetch a more complete output, you can modify the `-o` parameter.\n\n   ```bash\n   kubectl get pods",
    "path": "docs/release-0_9/kubeblocks-for-redis/high-availability/high-availability",
    "description": "  # High availability  KubeBlocks integrates [the official Redis Sentinel solution](https://redis.io/docs/management/sentinel/) to realize high availability and adopts Noop as the switch policy.  Redis Sentinel is the high availability solution for a Redis Replication Cluster, which is recommended b"
  },
  {
    "id": "docs_en_release-0_9_user_docs_connect_database_connect-database-in-production-environment",
    "title": "Connect database in production environment",
    "content": "\n\n# Connect database in production environment\n\nIn the production environment, it is normal to connect a database with CLI and SDK clients. There are three scenarios.\n\n- Scenario 1: Client1 and the database are in the same Kubernetes cluster. To connect client1 and the database, see [Use ClusterIP](#scenario-1-connect-database-in-the-same-kubernetes-cluster).\n- Scenario 2: Client2 is outside the Kubernetes cluster, but it is in the same VPC as the database. To connect client2 and the database, see [Expose VPC Private Address](#scenario-2-client-outside-the-kubernetes-cluster-but-in-the-same-vpc-as-the-kubernetes-cluster).\n- Scenario 3: Client3 and the database are in different VPCs, such as other VPCs or the public network. To connect client3 and the database, see [Expose VPC Public Address](#scenario-3-connect-database-with-clients-in-other-vpcs-or-public-networks).\n\nSee the figure below to get a clear image of the network location.\n\n![Example](/img/docs/en/connect_database_in_a_production_environment.png)\n\n## Scenario 1. Connect database in the same Kubernetes cluster\n\nYou can connect with the database ClusterIP or domain name.\n\n\n\nTo check the database endpoint, use `kubectl get service -`.\n\n```bash\nkubectl get service mycluster-mysql\n```\n\n\n\n\nTo check the database endpoint, use `kbcli cluster describe $`.\n\n```bash\nkbcli cluster describe x\n>\nName: x         Created Time: Mar 01,2023 11:45 UTC+0800\nNAMESPACE   CLUSTER-DEFINITION   VERSION           STATUS    TERMINATION-POLICY\ndefault     apecloud-mysql       ac-mysql-8.0.30   Running   Delete\n\nEndpoints:\nCOMPONENT   MODE        INTERNAL                                 EXTERNAL\nx           ReadWrite   x-mysql.default.svc.cluster.local:3306   \n\nTopology:\nCOMPONENT   INSTANCE    ROLE     STATUS    AZ                NODE                                                       CREATED-TIME\nmysql       x-mysql-0   leader   Running   cn-northwest-1b   ip-10-0-2-184.cn-northwest-1.compute.internal/10.0.2.184   Mar 01,2023 11",
    "path": "docs/release-0_9/user_docs/connect_database/connect-database-in-production-environment",
    "description": "  # Connect database in production environment  In the production environment, it is normal to connect a database with CLI and SDK clients. There are three scenarios.  - Scenario 1: Client1 and the database are in the same Kubernetes cluster. To connect client1 and the database, see [Use ClusterIP]("
  },
  {
    "id": "docs_en_release-0_9_user_docs_connect_database_connect-database-in-testing-environment",
    "title": "Connect database in testing environment",
    "content": "\n\n# Connect database in testing environment\n\n\n\n## Step 1. Retrieve Database Credentials\n\nBefore connecting to the MySQL database running inside your Kubernetes cluster, you may need to retrieve the username and password from a Kubernetes Secret. Secrets in Kubernetes are typically base64-encoded, so you will need to decode them to obtain the actual credentials. Here’s how you can do this with kubectl.\n\n1. Retrieve the `username`:\n\n   Use the kubectl get secrets command to extract the username from the secret named `mycluster-conn-credential` in the demo namespace.\n\n   ```bash\n   kubectl get secrets -n demo mycluster-conn-credential -o jsonpath='' | base64 -d\n   >\n   root\n   ```\n\n   - Replace \"mycluster\" with the actual name of your database cluster.\n   - Replace \"demo\" with the actual namespace of your database cluster.\n\n2. Retrieve the `password`:\n\n   ```bash\n   kubectl get secrets -n demo mycluster-conn-credential -o jsonpath='' | base64 -d\n   >\n   2gvztbvz\n   ```\n\n   - Replace \"mycluster\" with the actual name of your database cluster.\n   - Replace \"demo\" with the actual namespace of your database cluster.\n\n:::note\n\nIn most KubeBlocks v0.9 addons, the secret that contains the connection credentials follows the naming pattern `-conn-credential`. However, in some newer versions of KubeBlocks addons, the naming of the secret may have changed to `--account-`. To ensure you are using the correct secret name, run the following command to list all secrets in the namespace and search for the ones related to your database cluster.\n\nExample:\n\n```bash\nkubectl get secrets -n demo | grep mycluster\n```\n\n:::\n\n## Step 2. Connect to the cluster\n\nAfter retrieving the credentials, you can connect to the MySQL database running in your Kubernetes cluster using one of two methods:\n\n- Using `kubectl exec` to connect directly within the Pod.\n- Using `kubectl port-forward` to access the database from your local machine.\n\n### Option 1: Use kubectl exec command in pod\n\nIn some cases, you ma",
    "path": "docs/release-0_9/user_docs/connect_database/connect-database-in-testing-environment",
    "description": "  # Connect database in testing environment    ## Step 1. Retrieve Database Credentials  Before connecting to the MySQL database running inside your Kubernetes cluster, you may need to retrieve the username and password from a Kubernetes Secret. Secrets in Kubernetes are typically base64-encoded, so"
  },
  {
    "id": "docs_en_release-0_9_user_docs_connect_database_overview-of-database-connection",
    "title": "Connect database from anywhere",
    "content": "\n# Overview of Database Connection\n\nAfter deploying KubeBlocks and creating clusters, the databases run on Kubernetes, with each replica running in a Pod and managed by the InstanceSet. You can connect to the database using client tools or SDKs through the exposed database Service addresses (ClusterIP, LoadBalancer, or NodePort). See [Connect database in a production environment](connect-database-in-production-environment.md).\n\nIf you’ve created a database using KubeBlocks in a playground or test environment, you can also use `kubectl port-forward` to map the database service address to a local port on your machine. Then, you can connect to the database using a client tool or the common database clients integrated inside `kbcli`. However, please note that this is a temporary way to access services within the cluster and is intended for testing and debugging purposes only; it should not be used in a production environment. See [Connect database in a testing environment](connect-database-in-testing-environment.md).\n",
    "path": "docs/release-0_9/user_docs/connect_database/overview-of-database-connection",
    "description": " # Overview of Database Connection  After deploying KubeBlocks and creating clusters, the databases run on Kubernetes, with each replica running in a Pod and managed by the InstanceSet. You can connect to the database using client tools or SDKs through the exposed database Service addresses (Cluster"
  },
  {
    "id": "docs_en_release-0_9_user_docs_developer_overview",
    "title": "Overview",
    "content": "\n# Overview\n\nGuides and resources to unleash your development potential. Developer docs include integration tutorials for how to add an add-on to KubeBlocks, API reference, guides for referencing external components and simulating fault injections.\n\n* [Terminology](./terminology.md)\n* [Integration](./integration/add-ons-of-kubeblocks.md)\n* [API Reference](./api-reference/cluster.md)\n* [External Component](./external-component/reference-external-component.md)\n",
    "path": "docs/release-0_9/user_docs/developer/overview",
    "description": " # Overview  Guides and resources to unleash your development potential. Developer docs include integration tutorials for how to add an add-on to KubeBlocks, API reference, guides for referencing external components and simulating fault injections.  * [Terminology](./terminology.md) * [Integration]("
  },
  {
    "id": "docs_en_release-0_9_user_docs_developer_terminology",
    "title": "Terminology",
    "content": "# Terminology\n\n##### Addon\n\nAn addon is an efficient and open extension mechanism. With the KubeBlocks addon, developers can quickly add a new database engine to KubeBlocks and obtain specific foundational management functionalities of that database engine, including but not limited to lifecycle management, data backup and recovery, metrics and log collection, etc.\n##### ActionSet\n\nAn ActionSet declares a set of commands to perform backup and restore operations using specific tools, such as commands to backup MySQL using xtrabackup, as well as commands to restore data from the backup.\n\n##### BackupPolicy\n\nA BackupPolicy represents a backup strategy for a Cluster, including details such as the backup repository (BackupRepo), backup targets, and backup methods. Multiple backup methods can be defined within a backup policy, with each method referring to a corresponding ActionSet. When creating a backup, the backup policy and backup method can be specified for the backup process.\n\n##### BackupRepo\n\nBackupRepo is the storage repository for backup data. Its principle involves using a CSI driver to upload backup data to various storage systems, such as object storage systems like S3, GCS, as well as storage servers like FTP, NFS, and others.\n\n##### BackupSchedule\n\nBackupSchedule declares the configuration for automatic backups in a Cluster, including backup frequency, retention period, backup policy, and backup method. The BackupSchedule Controller creates a CronJob to automatically backup the Cluster based on the configuration specified in the Custom Resource (CR).\n\n##### Cluster \n\nCluster is composed by [components](#component-is-the-fundamental-assembly-component-used-to-build-a-data-storage-and-processing-system-a-component-utilizes-a-statefulset-either-native-to-kubernetes-or-specified-by-the-customer-such-as-openkruise-to-manage-one-to-multiple-pods).\n\n##### Component\n\nA component is the fundamental assembly component used to build a data storage and processing syste",
    "path": "docs/release-0_9/user_docs/developer/terminology",
    "description": "# Terminology  ##### Addon  An addon is an efficient and open extension mechanism. With the KubeBlocks addon, developers can quickly add a new database engine to KubeBlocks and obtain specific foundational management functionalities of that database engine, including but not limited to lifecycle man"
  },
  {
    "id": "docs_en_release-0_9_user_docs_handle-an-exception_full-disk-lock",
    "title": "Full disk lock",
    "content": "\n# Full disk lock\n\nThe full disk lock function of KubeBlocks ensures the stability and availability of a database. This function triggers a disk lock when the disk usage reaches a set threshold, thereby pausing write operations and only allowing read operations. Such a mechanism prevents a database from being affected by disk space exhaustion.\n\n## Mechanism of lock/unlock\n\nWhen the space water level of any configured volume exceeds the defined threshold, the instance is locked (read-only). Meanwhile, the system sends a related warning event, including the specific threshold and space usage information of each volume.\n\nWhen the space water level of all configured volumes falls below the defined threshold, the instance is unlocked (read and write). Meanwhile, the system sends a related warning event, including the specific threshold and space usage information of each volume.\n\n:::note\n\n1. The full disk lock function currently supports global (ClusterDefinition) enabling or disabling and does not support Cluster dimension control. Dynamically enabling or disabling this function may affect the existing Cluster instances that use this ClusterDefinition and cause them to restart. Please operate with caution.\n\n2. The full disk locking function relies on the read permission (get & list) of the two system resource nodes and nodes/stats. If you create an instance via kbcli, make sure to grant the controller administrative rights to the ClusterRoleBinding.\n\n3. Currently, full disk lock is available for ApeCloud MySQL, PostgreSQL and MongoDB.\n\n:::\n\n## Enable full disk lock\n\n- For MySQL, the read/write user cannot write to the disk when the disk usage reaches the `highwatermark` value, while the superuser can still write.\n- For PostgreSQL and MongoDB, both the read/write user and the superuser cannot write when the disk usage reaches `highwatermark`.\n- `90` is the default value setting for the high watermark at the component level which means the disk is locked when the disk usa",
    "path": "docs/release-0_9/user_docs/handle-an-exception/full-disk-lock",
    "description": " # Full disk lock  The full disk lock function of KubeBlocks ensures the stability and availability of a database. This function triggers a disk lock when the disk usage reaches a set threshold, thereby pausing write operations and only allowing read operations. Such a mechanism prevents a database "
  },
  {
    "id": "docs_en_release-0_9_user_docs_handle-an-exception_handle-a-cluster-exception",
    "title": "Handle a cluster exception",
    "content": "\n\n# Handle an exception\n\nWhen an exception occurs during your operation, you can perform the following steps to solve it.\n\n## Steps\n\n1. Check the cluster status.\n\n\n\n    ```bash\n    kubectl describe cluster mycluster\n    ```\n\n   \n\n\n   ```bash\n   kbcli cluster list mycluster\n   ```\n\n   \n\n   \n\n2. Handle the exception according to the status information.\n\n    | **Status**       | **Information** |\n    | :---             | :---            |\n    | Abnormal         | The cluster can be accessed but exceptions occur in some pods. This might be a mediate status of the operation process and the system recovers automatically without executing any extra operation. Wait until the cluster status changes to `Running`. |\n    | ConditionsError  | The cluster is normal but an exception occurs to the condition. It might be caused by configuration loss or exception, which further leads to operation failure. Manual recovery is required. |\n    | Failed | The cluster cannot be accessed. Check the `status.message` string and get the exception reason. Then manually recover it according to the hints. |\n\n    You can check the cluster's status for more information.\n\n## Fallback strategies\n\nIf the above operations can not solve the problem, try the following steps:\n\n- Restart this cluster. If the restart fails, you can delete the pod manually.\n- Roll the cluster status back to the status before changes.\n",
    "path": "docs/release-0_9/user_docs/handle-an-exception/handle-a-cluster-exception",
    "description": "  # Handle an exception  When an exception occurs during your operation, you can perform the following steps to solve it.  ## Steps  1. Check the cluster status.        ```bash     kubectl describe cluster mycluster     ```           ```bash    kbcli cluster list mycluster    ```            2. Handl"
  },
  {
    "id": "docs_en_release-0_9_user_docs_installation_install-addons",
    "title": "Install Addons",
    "content": "\n\n# Install Addons\n\nWith the release of KubeBlocks v0.8.0, Addons are decoupled from KubeBlocks and some Addons are not installed by default. If you want to use these Addons, install Addons first by index. Or if you uninstalled some Addons, you can follow the steps in this tutorial to install them again.\n\nThis tutorial takes elasticsearch as an example. You can replace elasticsearch with the Addon you need.\n\nThe official index repo is [KubeBlocks index](https://github.com/apecloud/block-index). Addons are maintained in the [KubeBlocks Addon repo](https://github.com/apecloud/kubeblocks-addons).\n\n:::note\n\nMake sure the major version of Addons and KubeBlocks are the same.\n\nFor example, you can install an Addon v0.9.0 with KubeBlocks v0.9.2, but using mismatched major versions, such as an Addon v0.8.0 with KubeBlocks v0.9.2, may lead to errors.\n\n:::\n\n\n\n1. (Optional) Add the KubeBlocks repo. If you install KubeBlocks with Helm, just run `helm repo update`.\n\n   ```bash\n   helm repo add kubeblocks https://apecloud.github.io/helm-charts\n   helm repo update\n   ```\n\n2. View the Addon versions.\n\n   ```bash\n   helm search repo kubeblocks/elasticsearch --devel --versions\n   ```\n\n3. Install the Addon (take elasticsearch as example). Specify a version with `--version`.\n\n   ```bash\n   helm install kb-addon-es kubeblocks/elasticsearch --namespace kb-system --create-namespace --version x.y.z\n   ```\n\n4. Verify whether this Addon is installed.\n\n   The STATUS is `deployed` and this Addon is installed successfully.\n\n   ```bash\n   helm list -A\n   >\n   NAME             NAMESPACE\tREVISION\tUPDATED                                STATUS  \t CHART                   APP VERSION\n   ...\n   kb-addon-es      kb-system\t1       \t2024-11-27 10:04:59.730127 +0800 CST   deployed\t elasticsearch-0.9.0     8.8.2 \n   ```\n\n5. (Optional) You can run the command below to uninstall the Addon.\n\n   If you have created a related cluster, delete the cluster first.\n\n   ```bash\n   helm uninstall kb-addon-es --namespace",
    "path": "docs/release-0_9/user_docs/installation/install-addons",
    "description": "  # Install Addons  With the release of KubeBlocks v0.8.0, Addons are decoupled from KubeBlocks and some Addons are not installed by default. If you want to use these Addons, install Addons first by index. Or if you uninstalled some Addons, you can follow the steps in this tutorial to install them a"
  },
  {
    "id": "docs_en_release-0_9_user_docs_installation_install-kbcli",
    "title": "Install kbcli",
    "content": "\n\n# Install kbcli\n\nYou can install kbcli on your laptop or virtual machines on the cloud.\n\n## Environment preparation\n\nFor Windows users, PowerShell version should be 5.0 or higher.\n\n## Install kbcli\n\nkbcli now supports macOS, Windows, and Linux.\n\n\nYou can install kbcli with `curl` or `brew`.\n\n- Option 1: Install kbcli using the `curl` command.\n\n1. Install kbcli.\n\n   ```bash\n   curl -fsSL https://kubeblocks.io/installer/install_cli.sh | bash\n   ```\n\n   If you want to install kbcli with a specified version, follow the steps below.\n\n   1. Check the available versions in [kbcli Release](https://github.com/apecloud/kbcli/releases/).\n   2. Specify a version with `-s` and run the command below.\n\n      ```bash\n      curl -fsSL https://kubeblocks.io/installer/install_cli.sh | bash -s x.y.z\n      ```\n   You can view all versions of kbcli, including alpha and beta releases, on the [kbcli releases list](https://github.com/apecloud/kbcli/releases).\n\n   To get stable releases, use this command:\n   ```bash\n   curl -s \"https://api.github.com/repos/apecloud/kbcli/releases?per_page=100&page=1\" | jq -r '.[] | select(.prerelease == false) | .tag_name' | sort -V -r\n   ```\n\n  :::note\n\n   By default, kbcli installs the latest release version. However, if you already have a running KubeBlocks instance, you need to install a matching version of kbcli.\n\n   For example, if your existing KubeBlocks deployment is v0.8.3, you should install kbcli v0.8.3 instead of a higher version like v0.9.0, as mismatched versions may lead to errors.\n\n  :::\n\n2. Run `kbcli version` to check the version of kbcli and ensure that it is successfully installed.\n\n  :::note\n\n  If a timeout exception occurs during installation, please check your network settings and retry.\n\n  :::\n\n- Option 2: Install kbcli using Homebrew.\n\n1. Install ApeCloud tap, the Homebrew package of ApeCloud.\n\n   ```bash\n   brew tap apecloud/tap\n   ```\n\n2. Install kbcli.\n\n   ```bash\n   brew install kbcli\n   ```\n\n   If you want to install kbcli wi",
    "path": "docs/release-0_9/user_docs/installation/install-kbcli",
    "description": "  # Install kbcli  You can install kbcli on your laptop or virtual machines on the cloud.  ## Environment preparation  For Windows users, PowerShell version should be 5.0 or higher.  ## Install kbcli  kbcli now supports macOS, Windows, and Linux.   You can install kbcli with `curl` or `brew`.  - Opt"
  },
  {
    "id": "docs_en_release-0_9_user_docs_installation_install-kubeblocks",
    "title": "Install KubeBlocks",
    "content": "\n\n\n# Install KubeBlocks\n\nThe quickest way to try out KubeBlocks is to create a new Kubernetes cluster and install KubeBlocks using the playground. However, production environments are more complex, with applications running in different namespaces and with resource or permission limitations. This document explains how to deploy KubeBlocks on an existing Kubernetes cluster.\n\nKubeBlocks is Kubernetes-native, you can use Helm or kubectl with a YAML file to install it. You can also use kbcli, an intuitive CLI tool, to install KubeBlocks.\n\nTo try out KubeBlocks on your local host, you can use the [Playground](./../try-out-on-playground/try-kubeblocks-on-your-laptop.md) or [create a local Kubernetes test cluster first](./prepare-a-local-k8s-cluster/prepare-a-local-k8s-cluster.md) and then follow the steps in this tutorial to install KubeBlocks.\n\n:::note\n\n- Note that you install and uninstall KubeBlocks with the same tool. For example, if you install KubeBlocks with Helm, to uninstall it, you have to use Helm too.\n- Make sure you have [kubectl](https://kubernetes.io/docs/tasks/tools/), [Helm](https://helm.sh/docs/intro/install/), or [kbcli](./install-kbcli) installed.\n\n:::\n\n## Environment preparation\n\n\n    \n\t\n\t    Resource Requirements\n\t\n    \n    \n\t\n\t    Control Plane\n\t    It is recommended to create 1 node with 4 cores, 4GB memory and 50GB storage. \n\t\n\t\n\t    Data Plane\n\t     MySQL \n\t    It is recommended to create at least 3 nodes with 2 cores, 4GB memory and 50GB storage. \n\t\n\t\n\t     PostgreSQL \n        It is recommended to create at least 2 nodes with 2 cores, 4GB memory and 50GB storage.  \n\t\n\t\n\t     Redis \n        It is recommended to create at least 2 nodes with 2 cores, 4GB memory and 50GB storage. \n\t\n\t\n\t     MongoDB \n\t    It is recommended to create at least 3 nodes with 2 cores, 4GB memory and 50GB storage. \n\t\n    \n\n\n## Installation steps\n\n\n\nUse Helm and follow the steps below to install KubeBlocks.\n\n1. Create dependent CRDs. Specify the version you want to install.",
    "path": "docs/release-0_9/user_docs/installation/install-kubeblocks",
    "description": "   # Install KubeBlocks  The quickest way to try out KubeBlocks is to create a new Kubernetes cluster and install KubeBlocks using the playground. However, production environments are more complex, with applications running in different namespaces and with resource or permission limitations. This do"
  },
  {
    "id": "docs_en_release-0_9_user_docs_installation_uninstall-kubeblocks-and-kbcli",
    "title": "Uninstall KubeBlocks",
    "content": "\n\n# Uninstall KubeBlocks and kbcli\n\nUninstallation order:\n\n1. Delete all clusters and backups before uninstalling KubeBlocks and kbcli.\n\n   ```bash\n   kubectl delete cluster  -n namespace\n   ```\n\n   ```bash\n   kubectl delete backup  -n namespace\n   ```\n\n2. Uninstall KubeBlocks.\n\n## Uninstall KubeBlocks\n\n\n\nDelete all the clusters and resources created before performing the following command, otherwise the uninstallation may not be successful.\n\n```bash\nhelm uninstall kubeblocks --namespace kb-system\n```\n\nHelm does not delete CRD objects. You can delete the ones KubeBlocks created with the following commands:\n\n```bash\nkubectl get crd -o name | grep kubeblocks.io | xargs kubectl delete\n```\n\n\n\n\nYou can generate YAMLs from the KubeBlocks chart and uninstall using `kubectl`. Use `--version x.y.z` to specify a version and make sure the uninstalled version is the same as the installed one.\n\n```bash\nhelm template kubeblocks kubeblocks/kubeblocks --namespace kb-system --version x.y.z | kubectl delete -f -\n```\n\n\n\n\n```bash\nkbcli kubeblocks uninstall\n```\n\n\n\n\n\n## Uninstall kbcli\n\nUninstall kbcli if you want to delete kbcli after your trial. Use the same option as the way you install kbcli.\n\n\n\nFor `curl`, run\n\n```bash\nsudo rm /usr/local/bin/kbcli\n```\n\nFor `brew`, run\n\n```bash\nbrew uninstall kbcli\n```\n\nkbcli creates a hidden folder named `~/.kbcli` under the HOME directory to store configuration information and temporary files. You can delete this folder after uninstalling kbcli.\n\n\n\n\n1. Go to the `kbcli` installation path and delete the installation folder.\n\n   * If you install `kbcli` by script, go to `C:\\Program Files` and delete the `kbcli-windows-amd64` folder.\n   * If you customize the installation path, go to your specified path and delete the installation folder.\n\n2. Delete the environment variable.\n\n   1. Click the Windows icon and click **System**.\n   2. Go to **Settings** -> **Related Settings** -> **Advanced system settings**.\n   3. On the **Advanced** tab, click **Enviro",
    "path": "docs/release-0_9/user_docs/installation/uninstall-kubeblocks-and-kbcli",
    "description": "  # Uninstall KubeBlocks and kbcli  Uninstallation order:  1. Delete all clusters and backups before uninstalling KubeBlocks and kbcli.     ```bash    kubectl delete cluster  -n namespace    ```     ```bash    kubectl delete backup  -n namespace    ```  2. Uninstall KubeBlocks.  ## Uninstall KubeBlo"
  },
  {
    "id": "docs_en_release-0_9_user_docs_observability_alert",
    "title": "Configure alert",
    "content": "\n# Configure alert\n\n## Configure alert\n\nAlerts are mainly used for daily error response to improve system availability. KubeBlocks uses the open-source version of Prometheus to configure alert rules and multiple notification channels. The alert capability of KubeBlocks can meet the operation and maintenance requirements of production-level online clusters.\n\n:::note\n\nThe alert function is the same for all engines.\n\n:::\n\n### Alert rules\n\nKubeBlocks uses the open-source version of Prometheus to meet the needs of various data products. These alert rules provide the best practice for cluster operation and maintenance, which further improve alert accuracy and reduce the probability of false negatives and false positives by experience-based smoothing windows, alert thresholds, alert levels, and alert indicators.\n\nTaking PostgreSQL as an example, the alert rules have built-in common abnormal events, such as instance down, instance restart, slow query, connection amount, deadlock, and cache hit rate.\n\nThe following example shows PostgreSQL alert rules (refer to [Prometheus](https://prometheus.io/docs/prometheus/latest/querying/basics/) for syntax). When the amount of active connections exceeds 80% of the threshold and lasts for 2 minutes, Prometheus triggers a warning alert and sends it to AlertManager.\n\n```bash\nalert: PostgreSQLTooManyConnections\n  expr: |\n    sum by (namespace,app_kubernetes_io_instance,pod) (pg_stat_activity_count)\n    > on(namespace,app_kubernetes_io_instance,pod)\n    (pg_settings_max_connections - pg_settings_superuser_reserved_connections) * 0.8\n  for: 2m\n  labels:\n    severity: warning\n  annotations:\n    summary: \"PostgreSQL too many connections (> 80%)\"\n    description: \"PostgreSQL has too many connections and the value is }. (instance: })\"\n```\n\nConfigure alert rules as needed. For more details, please refer to [Prometheus alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/#defining-alerting-rules).\n\n### Notific",
    "path": "docs/release-0_9/user_docs/observability/alert",
    "description": " # Configure alert  ## Configure alert  Alerts are mainly used for daily error response to improve system availability. KubeBlocks uses the open-source version of Prometheus to configure alert rules and multiple notification channels. The alert capability of KubeBlocks can meet the operation and mai"
  },
  {
    "id": "docs_en_release-0_9_user_docs_observability_monitor-database",
    "title": "Monitor database",
    "content": "\n\n# Monitor a database\n\nThis tutorial demonstrates how to configure the monitoring function for a PostgreSQL cluster, using Prometheus and Grafana.\n\n## Step 1. Install Prometheus Operator and Grafana\n\nInstall the Promethus Operator and Grafana to monitor the performance of a database. Skip this step if a Prometheus Operator is already installed in your environment.\n\n1. Create a new namespace for Prometheus Operator.\n\n   ```bash\n   kubectl create namespace monitoring\n   ```\n\n2. Add the Prometheus Operator Helm repository.\n\n   ```bash\n   helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\n   ```\n\n3. Install the Prometheus Operator.\n\n   ```bash\n   helm install prometheus-operator prometheus-community/kube-prometheus-stack --namespace monitoring\n   ```\n\n4. Verify the deployment of the Prometheus Operator. Make sure all pods are in the Ready state.\n\n   ```bash\n   kubectl get pods -n monitoring\n   ```\n\n5. Access the Prometheus and Grafana dashboards.\n\n   1. Check the service endpoints of Prometheus and Grafana.\n\n       ```bash\n       kubectl get svc -n monitoring\n       ```\n\n   2. Use port forwarding to access the Prometheus dashboard locally.\n\n       ```bash\n       kubectl port-forward svc/prometheus-operator-kube-p-prometheus -n monitoring 9090:9090\n       ```\n\n       You can also access the Prometheus dashboard by opening \"http://localhost:9090\" in your browser.\n\n   3. Retrieve the Grafana's login credential from the secret.\n\n       ```bash\n       kubectl get secrets prometheus-operator-grafana -n monitoring -o yaml\n       ```\n\n   4. Use port forwarding to access the Grafana dashboard locally.\n\n       ```bash\n       kubectl port-forward svc/prometheus-operator-grafana -n monitoring 3000:80\n       ```\n\n       You can also access the Grafana dashboard by opening \"http://localhost:3000\" in your browser.\n\n6. Configure the selectors for PodMonitor and ServiceMonitor to match your monitoring requirements.\n\n   Prometheus Operator uses Prometh",
    "path": "docs/release-0_9/user_docs/observability/monitor-database",
    "description": "  # Monitor a database  This tutorial demonstrates how to configure the monitoring function for a PostgreSQL cluster, using Prometheus and Grafana.  ## Step 1. Install Prometheus Operator and Grafana  Install the Promethus Operator and Grafana to monitor the performance of a database. Skip this step"
  },
  {
    "id": "docs_en_release-0_9_user_docs_overview_about-this-manual",
    "title": "About this manual",
    "content": "This manual introduces how to operate KubeBlocks with `kbcli`. For advanced users familiar with Kubernetes, this manual also includes guidance on how to operate KubeBlocks using `helm` and `kubectl`.",
    "path": "docs/release-0_9/user_docs/overview/about-this-manual",
    "description": "This manual introduces how to operate KubeBlocks with `kbcli`. For advanced users familiar with Kubernetes, this manual also includes guidance on how to operate KubeBlocks using `helm` and `kubectl`."
  },
  {
    "id": "docs_en_release-0_9_user_docs_overview_concept",
    "title": "Concepts",
    "content": "\n# Concepts\n\nYou've already seen the benefits of using unified APIs to represent various databases in the section [\"How Unified APIs Reduce Your Learning Curve\"](./introduction.md#how-unified-apis-reduce-your-learning-curve). If you take a closer look at those examples, you'll notice two key concepts in the sample YAML files: **Cluster** and **Component**. For instance, `test-mysql` is a Cluster that includes a Component called `mysql` (with a componentDef of `apecloud-mysql`). Similarly, `test-redis` is also a Cluster, and it includes two Components: one called `redis` (with a componentDef of `redis-7`), which has two replicas, and another called `redis-sentinel` (with a componentDef of `redis-sentinel`), which has three replicas.\n\nIn this document, we will explain the reasons behind these two concepts and provide a brief introduction to the underlying API (i.e., CRD).\n\n## Motivation of KubeBlocks’ Layered API\nIn KubeBlocks, to support the management of various databases through a unified API, we need to abstract the topologies and characteristics of different databases.\n\nWe’ve observed that database systems deployed in production environments often use a topology composed of multiple components. For example, a production MySQL cluster might include several Proxy nodes (such as ProxySQL, MaxScale, Vitess, WeScale, etc.) alongside multiple MySQL server nodes (like MySQL Community Edition, Percona, MariaDB, ApeCloud MySQL, etc.) to achieve higher availability and read-write separation. Similarly, Redis deployments typically consist of a primary node and multiple read replicas, managed for high availability via Sentinel. Some users even use twemproxy for horizontal sharding to achieve greater capacity and throughput.\n\nThis modular approach is even more pronounced in distributed database systems, where the entire system is divided into distinct components with clear and singular responsibilities, such as data storage, query processing, transaction management, logging, ",
    "path": "docs/release-0_9/user_docs/overview/concept",
    "description": " # Concepts  You've already seen the benefits of using unified APIs to represent various databases in the section [\"How Unified APIs Reduce Your Learning Curve\"](./introduction.md#how-unified-apis-reduce-your-learning-curve). If you take a closer look at those examples, you'll notice two key concept"
  },
  {
    "id": "docs_en_release-0_9_user_docs_overview_introduction",
    "title": "Introduction",
    "content": "\n\n# Introduction\n\n## What is KubeBlocks\n\nKubeBlocks is an open-source Kubernetes operator for databases (more specifically, for stateful applications, including databases and middleware like message queues), enabling users to run and manage multiple types of databases on Kubernetes. As far as we know, most database operators typically manage only one specific type of database. For example:\n- CloudNativePG, Zalando, CrunchyData, StackGres operator can manage PostgreSQL\n- Strimzi manages Kafka\n- Oracle and Percona MySQL operator manage MySQL\n\nIn contrast, KubeBlocks is designed to be a **general-purpose database operator**. This means that when designing the KubeBlocks API, we didn’t tie it to any specific database. Instead, we abstracted the common features of various databases, resulting in a universal, engine-agnostic API. Consequently, the operator implementation developed around this abstract API is also agnostic to the specific database engine.\n\n![Design of KubeBlocks, a general purpose database operator](/img/docs/en/kubeblocks_general_purpose_arch.png)\n\nIn above diagram, Cluster, Component, and InstanceSet are all CRDs provided by KubeBlocks. If you'd like to learn more about them, please refer to [concepts](concept.md).\n\nKubeBlocks offers an Addon API to support the integration of various databases. For instance, we currently have the following KubeBlocks Addons for mainstream open-source database engines:\n- MySQL\n- PostgreSQL\n- Redis\n- MongoDB\n- Kafka\n- RabbitMQ\n- Minio\n- Elasticsearch\n- StarRocks\n- Qdrant\n- Milvus\n- ZooKeeper\n- etcd\n- ...\n\nFor a detailed list of Addons and their features, please refer to [supported addons](supported-addons.md).\n\nThe unified API makes KubeBlocks an excellent choice if you need to run multiple types of databases on Kubernetes. It can significantly reduce the learning curve associated with mastering multiple operators.\n\n## How unified APIs reduce your learning curve\n\nHere is an example of how to use KubeBlocks' Cluster API to ",
    "path": "docs/release-0_9/user_docs/overview/introduction",
    "description": "  # Introduction  ## What is KubeBlocks  KubeBlocks is an open-source Kubernetes operator for databases (more specifically, for stateful applications, including databases and middleware like message queues), enabling users to run and manage multiple types of databases on Kubernetes. As far as we kno"
  },
  {
    "id": "docs_en_release-0_9_user_docs_overview_kubernetes_and_operator_101",
    "title": "Kubernetes and Operator 101",
    "content": "\n# Kubernetes and Operator 101\n\n# K8s\n\nWhat is Kubernetes? Some say it's a container orchestration system, others describe it as a distributed operating system, while some view it as a multi-cloud PaaS (Platform as a Service) platform, and others consider it a platform for building PaaS solutions.\n\nThis article will introduce the key concepts and building blocks within Kubernetes.\n\n## K8s Control Plane\n\nThe Kubernetes Control Plane is the brain and heart of Kubernetes. It manages the overall operation of the cluster, including processing API requests, storing configuration data, and ensuring the cluster's desired state. Key components include the API Server (which handles communication), etcd (which stores all cluster data), the Controller Manager (which enforces the desired state), the Scheduler (which assigns workloads to Nodes), and the Cloud Controller Manager (which manages cloud-specific integrations, such as load balancers, storage, and networking). Together, these components orchestrate the deployment, scaling, and management of containers across the cluster.\n\n## Node\n\nSome describe Kubernetes as a distributed operating system, capable of managing many Nodes. A Node is a physical or virtual machine that acts as a worker within the cluster. Each Node runs essential services, including the container runtime (such as Docker or containerd), the kubelet, and the kube-proxy. The kubelet ensures that containers are running as specified in a Pod, the smallest deployable unit in Kubernetes. The kube-proxy handles network routing, maintaining network rules, and enabling communication between Pods and services. Nodes provide the computational resources needed to run containerized applications and are managed by the Kubernetes Master, which distributes tasks, monitors Node health, and maintains the desired state of the cluster.\n\n:::note\n\nIn certain contexts, the term \"Node\" can be confusing when discussing Kubernetes (K8s) alongside databases. In Kubernetes, a \"Node\" re",
    "path": "docs/release-0_9/user_docs/overview/kubernetes_and_operator_101",
    "description": " # Kubernetes and Operator 101  # K8s  What is Kubernetes? Some say it's a container orchestration system, others describe it as a distributed operating system, while some view it as a multi-cloud PaaS (Platform as a Service) platform, and others consider it a platform for building PaaS solutions.  "
  },
  {
    "id": "docs_en_release-0_9_user_docs_overview_supported-addons",
    "title": "Supported Addons",
    "content": "\n# Supported Addons\n\nKubeBlocks uses Addons to extend support for various database engines. And there are currently over 30 Addons available in the KubeBlocks repository, which can be further categorized as follows sections.\n\nFor installing and enabling Addons, refer to [Addon installation tutorial](./../../user_docs/installation/install-addons).\n\n## Relational Databases\n\nMySQL and PostgreSQL are the two most popular open-source relational databases in the world, and they have branches/variants.\n\n### MySQL and its variants\n\n**Addon List**\n\n| Addons          | Description                                                                                                                                                                                                                                                                                                                                                             |\n|:----------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| mysql           | This addon uses the community edition MySQL image officially released by Oracle.                                                                                                                                                                                                                                                                                        |\n| apecloud-mysql  | ApeCloud MySQL is a free, fully compatible drop-in replacement for MySQL Community Edition, offering enhanced high availability through a RAFT protocol replication plugin. The image is provided by ApeCloud. Additionally, ApeCloud MySQL includes an open-source proxy called WeScale, which ",
    "path": "docs/release-0_9/user_docs/overview/supported-addons",
    "description": " # Supported Addons  KubeBlocks uses Addons to extend support for various database engines. And there are currently over 30 Addons available in the KubeBlocks repository, which can be further categorized as follows sections.  For installing and enabling Addons, refer to [Addon installation tutorial]"
  },
  {
    "id": "docs_en_release-0_9_user_docs_try-out-on-playground_try-kubeblocks-on-cloud",
    "title": "Try out KubeBlocks on Cloud",
    "content": "\n\n# Try out KubeBlocks on Cloud\n\nThis guide walks you through the quickest way to get started with KubeBlocks on cloud, demonstrating how to create a demo environment (Playground) with one command.\n\n## Preparation\n\nWhen deploying KubeBlocks on the cloud, cloud resources are initialized with the help of [the terraform script](https://github.com/apecloud/cloud-provider). `kbcli` downloads the script and stores it locally, then calls the terraform commands to initialize a fully-managed Kubernetes cluster and deploy KubeBlocks on this cluster.\n\n\n### Before you start to try KubeBlocks on AWS\n\nMake sure you have all the followings prepared.\n\n* [Install AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)\n* [Install kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)\n* [Install kbcli](./../../user_docs/installation/install-kbcli)\n\n### Configure access key\n\n**Option 1.** Use `aws configure`.\n\nFill in an access key and run the command below to authenticate the requests.\n\n```bash\naws configure\nAWS Access Key ID [None]: YOUR_ACCESS_KEY_ID\nAWS Secret Access Key [None]: YOUR_SECRET_ACCESS_KEY\n```\n\nYou can refer to [Quick configuration with aws configure](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-config) for detailed information.\n\n**Option 2.** Use environment variables.\n\n```bash\n```\n\n### Initialize Playground\n\n```bash\nkbcli playground init --cloud-provider aws --region us-west-2\n```\n\n* `cloud-provider` specifies the cloud provider.\n* `region` specifies the region to deploy a Kubernetes cluster.\n   You can find the region list on [the official website](https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/?nc1=h_ls).\n\nDuring the initialization, `kbcli` clones [the GitHub repository](https://github.com/apecloud/cloud-provider) to the directory `~/.kbcli/playground`, installs KubeBlocks, and creates a MySQL cluster. After executing the `kbcli playground ",
    "path": "docs/release-0_9/user_docs/try-out-on-playground/try-kubeblocks-on-cloud",
    "description": "  # Try out KubeBlocks on Cloud  This guide walks you through the quickest way to get started with KubeBlocks on cloud, demonstrating how to create a demo environment (Playground) with one command.  ## Preparation  When deploying KubeBlocks on the cloud, cloud resources are initialized with the help"
  },
  {
    "id": "docs_en_release-0_9_user_docs_try-out-on-playground_try-kubeblocks-on-your-laptop",
    "title": "Try out KubeBlocks in 5 minutes on laptop",
    "content": "\n# Try out KubeBlocks in 5 minutes on laptop\n\nThis guide walks you through the quickest way to get started with KubeBlocks, demonstrating how to create a demo environment (Playground) with one command.\n\n## Before you start\n\nMeet the following requirements for a smooth user experience:\n\n* Minimum system requirements:\n  * CPU: 4 cores, use `sysctl hw.physicalcpu` command to check CPU;\n  * RAM: 4 GB, use `top -d` command to check memory.\n\n* Make sure the following tools are installed on your laptop:\n  * [Docker](https://docs.docker.com/get-docker/): v20.10.5 (runc ≥ v1.0.0-rc93) or above;\n  * [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl): it is used to interact with Kubernetes clusters;\n  * [kbcli](./../../user_docs/installation/install-kbcli): it is used for the interaction between Playground and KubeBlocks.\n\n## Initialize Playground\n\n***Steps:***\n\n1. Install Playground.\n\n   ```bash\n   kbcli playground init\n   ```\n\n   This command:\n   1. Creates a Kubernetes cluster in the container with [K3d](https://k3d.io/v5.4.6/).\n   2. Deploys KubeBlocks in the K3d cluster.\n   3. Creates a standalone MySQL cluster.\n\n   :::note\n\n   * If you previously ran `kbcli playground init` and it failed, running it again may cause errors. Please run the `kbcli playground destroy` command first to clean up the environment, then run `kbcli playground init` again.\n   * If you run Playground on Windows and the error below occurs, it is caused by the safety strategy of Windows 11 and the `kbcli.exe` you operate might be tampered with by a third party (or when you built the kbcli binary file through source on Windows).\n\n   ```bash\n   error: failed to set up k3d cluster: failed to create k3d cluster kb-playground: Failed Cluster Start: Failed to start server k3d-kb-playground-server-0: Node k3d-kb-playground-server-0 failed to get ready: error waiting for log line `k3s is up and running` from node 'k3d-kb-playground-server-0': stopped returning log lines\n   ```\n\n   You can follow the s",
    "path": "docs/release-0_9/user_docs/try-out-on-playground/try-kubeblocks-on-your-laptop",
    "description": " # Try out KubeBlocks in 5 minutes on laptop  This guide walks you through the quickest way to get started with KubeBlocks, demonstrating how to create a demo environment (Playground) with one command.  ## Before you start  Meet the following requirements for a smooth user experience:  * Minimum sys"
  },
  {
    "id": "docs_en_release-0_9_user_docs_upgrade-kubeblocks_faq",
    "title": "FAQ",
    "content": "\n\n# FAQ\n\nThis guide addresses common questions and issues that may arise when upgrading KubeBlocks, ensuring a smooth and efficient process.\n\n## Manually mark Addons\n\nIn earlier versions, KubeBlocks pre-installed some Addons in the Helm chart, but some of these Addons have been removed in the new version. Consequently, if you upgrade directly from an older version to the latest, Helm will remove the CRs of these removed Addons, affecting the clusters created by these Addons. To prevent this, it is recommended to add the `\"helm.sh/resource-policy\": \"keep\"` annotation for Addons to ensure they remain during upgrading.\n\n### View the Addon annotation\n\nRun the command below to view the annotations of Addons.\n\n```bash\nkubectl get addon -o json | jq '.items[] | '\n```\n\n### Manually add an annotation for Addons\n\nReplace `-l app.kubernetes.io/name=kubeblocks` with your actual filter and run the command below to add an annotation.\n\n```bash\nkubectl annotate addons.extensions.kubeblocks.io -l app.kubernetes.io/name=kubeblocks helm.sh/resource-policy=keep\n```\n\nIf you want to add the annotation for a specified Addon, replace `` with the actual Addon name and run the command below.\n\n```bash\nkubectl annotate addons.extensions.kubeblocks.io  helm.sh/resource-policy=keep\n```\n\nIf you want to check whether the annotation for an Addon was added successfully, replace `` with the actual Addon name and run the command below.\n\n```bash\nkubectl get addon  -o json | jq ''\n```\n\n## Fix \"cannot patch 'kubeblocks-dataprotection' with kind Deployment\" error\n\nWhen upgrading KubeBlocks to v0.8.x/v0.9.x, you might encounter the following error:\n\n```bash\nError: UPGRADE FAILED: cannot patch \"kubeblocks-dataprotection\" with kind Deployment: Deployment.apps \"kubeblocks-dataprotection\" is invalid: spec.selector: Invalid value: v1.LabelSelector, MatchExpressions:[]v1.LabelSelectorRequirement(nil)}: field is immutable && cannot patch \"kubeblocks\" with kind Deployment: Deployment.apps \"kubeblocks\" is invalid: ",
    "path": "docs/release-0_9/user_docs/upgrade-kubeblocks/faq",
    "description": "  # FAQ  This guide addresses common questions and issues that may arise when upgrading KubeBlocks, ensuring a smooth and efficient process.  ## Manually mark Addons  In earlier versions, KubeBlocks pre-installed some Addons in the Helm chart, but some of these Addons have been removed in the new ve"
  },
  {
    "id": "docs_en_release-0_9_user_docs_upgrade-kubeblocks_upgrade-to-0_8",
    "title": "Upgrade to v0.8",
    "content": "\n\n# Upgrade to KubeBlocks v0.8\n\nIn this tutorial, you will learn how to upgrade to KubeBlocks v0.8.\n\n:::note\n\nExecute `helm -n kb-system list | grep kubeblocks` or `kbcli version` to check the current KubeBlocks version you are running, and then upgrade it.\n\n:::\n\n## Upgrade from KubeBlocks v0.7\n\n\n\n1. Set keepAddons.\n\n    KubeBlocks v0.8 streamlines the default installed engines and separates the addons from KubeBlocks operators to KubeBlocks-Addons repo, such as greptime, influxdb, neon, oracle-mysql, orioledb, tdengine, mariadb, nebula, risingwave, starrocks, tidb, and zookeeper. To avoid deleting addon resources that are already in use during the upgrade, execute the following commands:\n\n- Check the current KubeBlocks version.\n\n    ```bash\n    helm -n kb-system list | grep kubeblocks\n    ```\n\n- Set the value of keepAddons as true.\n\n    ```bash\n    helm repo add kubeblocks https://apecloud.github.io/helm-charts\n    helm repo update kubeblocks\n    helm -n kb-system upgrade kubeblocks kubeblocks/kubeblocks --version \\ --set keepAddons=true\n    ```\n\n    Replace \\ with your current KubeBlocks version, such as 0.7.2.\n\n- Check addons.\n\n    Execute the following command to ensure that the addon annotations contain `\"helm.sh/resource-policy\": \"keep\"`.\n\n    ```bash\n    kubectl get addon -o json | jq '.items[] | '\n    ```\n\n2. Install CRD.\n\n    To reduce the size of Helm chart, KubeBlocks v0.8 removes CRD from the Helm chart. Before upgrading, you need to install CRD.\n\n    ```bash\n    kubectl replace -f https://github.com/apecloud/kubeblocks/releases/download/v0.8.1/kubeblocks_crds.yaml\n    ```\n\n3. Upgrade KubeBlocks.\n\n    ```bash\n    helm -n kb-system upgrade kubeblocks kubeblocks/kubeblocks --version 0.8.1 --set dataProtection.image.datasafed.tag=0.1.0\n    ```\n\n:::note\n\nTo avoid affecting existing database clusters, when upgrading to KubeBlocks v0.8, the versions of already-installed addons will not be upgraded by default. If you want to upgrade the addons to the versions b",
    "path": "docs/release-0_9/user_docs/upgrade-kubeblocks/upgrade-to-0_8",
    "description": "  # Upgrade to KubeBlocks v0.8  In this tutorial, you will learn how to upgrade to KubeBlocks v0.8.  :::note  Execute `helm -n kb-system list | grep kubeblocks` or `kbcli version` to check the current KubeBlocks version you are running, and then upgrade it.  :::  ## Upgrade from KubeBlocks v0.7    1"
  },
  {
    "id": "docs_en_release-0_9_user_docs_upgrade-kubeblocks_upgrade-to-0_9_0",
    "title": "Upgrade to v0.9.0",
    "content": "\n\n# Upgrade to KubeBlocks v0.9.0\n\nIn this tutorial, you will learn how to upgrade to KubeBlocks v0.9.0.\n\n:::note\n\nExecute `helm -n kb-system list | grep kubeblocks` or `kbcli version` to check the current KubeBlocks version you are running, and then upgrade KubeBlocks.\n\n:::\n\n## Compatibility\n\nKubeBlocks 0.9.0 is compatible with KubeBlocks 0.8 APIs, but compatibility with APIs from versions prior to v0.8 is not guaranteed. If you are using Addons from KubeBlocks 0.7 or earlier (0.6, etc), DO [upgrade KubeBlocks and all Addons to v0.8 first](https://github.com/apecloud/kubeblocks/blob/main/docs/user_docs/upgrade-kubeblocks/upgrade-to-0.8.md) to ensure service availability before upgrading to v0.9.0.\n\n## Upgrade from KubeBlocks v0.8\n\n\n\n1. Add the `\"helm.sh/resource-policy\": \"keep\"` for Addons.\n\n    KubeBlocks v0.8 streamlines the default installed engines. To avoid deleting Addon resources that are already in use during the upgrade, execute the following commands first.\n\n    - Add the `\"helm.sh/resource-policy\": \"keep\"` for Addons. You can replace `-l app.kubernetes.io/name=kubeblocks` with your actual filter name.\n\n         ```bash\n         kubectl annotate addons.extensions.kubeblocks.io -l app.kubernetes.io/name=kubeblocks helm.sh/resource-policy=keep\n         ```\n\n    - Check Addons.\n\n         Execute the following command to ensure that the Addon annotations contain `\"helm.sh/resource-policy\": \"keep\"`.\n\n         ```bash\n         kubectl get addon -o json | jq '.items[] | '\n         ```\n\n2. Delete the incompatible OpsDefinition.\n\n   ```bash\n   kubectl delete opsdefinitions.apps.kubeblocks.io kafka-quota kafka-topic kafka-user-acl switchover\n   ```\n\n3. Install the StorageProvider CRD before the upgrade.\n\n    If the network is slow, it's recommended to download the CRD YAML file on your localhost before further operations.\n\n    ```bash\n    kubectl create -f https://github.com/apecloud/kubeblocks/releases/download/v0.9.0/dataprotection.kubeblocks.io_storageproviders.y",
    "path": "docs/release-0_9/user_docs/upgrade-kubeblocks/upgrade-to-0_9_0",
    "description": "  # Upgrade to KubeBlocks v0.9.0  In this tutorial, you will learn how to upgrade to KubeBlocks v0.9.0.  :::note  Execute `helm -n kb-system list | grep kubeblocks` or `kbcli version` to check the current KubeBlocks version you are running, and then upgrade KubeBlocks.  :::  ## Compatibility  KubeBl"
  },
  {
    "id": "docs_en_release-0_9_user_docs_upgrade-kubeblocks_upgrade-to-latest-version",
    "title": "Upgrade to v0.9.3",
    "content": "\n\n# Upgrade to KubeBlocks v0.9.3\n\n:::note\n\n- Before upgrading, check your current KubeBlocks version:\n\n   Run `helm -n kb-system list | grep kubeblocks` or `kbcli version`.\n- For upgrading to different versions:\n\n   - For v0.9.2 and v0.9.1, follow this upgrade tutorial, replacing the version number with v0.9.2 or v0.9.1 respectively.\n   - [v0.9.0 upgrade guide](https://github.com/apecloud/kubeblocks/blob/main/docs/user_docs/upgrade-kubeblocks/upgrade-to-0.9.0.md).\n   - [v0.8.x upgrade guide](https://github.com/apecloud/kubeblocks/blob/main/docs/user_docs/upgrade-kubeblocks/upgrade-to-0.8.md).\n\n   Installing the latest version is recommended for better performance and features.\n\n:::\n\n## Compatibility\n\nKubeBlocks v0.9.3 is compatible with KubeBlocks v0.8 APIs, but compatibility with APIs from versions prior to v0.8 is not guaranteed. If you are using Addons from KubeBlocks v0.7 or earlier (v0.6, etc), DO [upgrade KubeBlocks and all Addons to v0.8 first](https://github.com/apecloud/kubeblocks/blob/main/docs/user_docs/upgrade-kubeblocks/upgrade-to-0.8.md) to ensure service availability before upgrading to v0.9.\n\nIf you are upgrading from v0.8 to v0.9, it's recommended to enable webhook to ensure the availability.\n\n## Upgrade from KubeBlocks v0.9.x\n\n\n\n1. View Addon and check whether the `\"helm.sh/resource-policy\": \"keep\"` annotation exists.\n\n    KubeBlocks streamlines the default installed engines. Add the `\"helm.sh/resource-policy\": \"keep\"` annotation to avoid deleting Addon resources that are already in use during the upgrade.\n\n    Check whether the `\"helm.sh/resource-policy\": \"keep\"` annotation is added.\n\n    ```bash\n    kubectl get addon -o json | jq '.items[] | '\n    ```\n\n    If the annotation doesn't exist, run the command below to add it. You can replace `-l app.kubernetes.io/name=kubeblocks` with your actual filter name.\n\n    ```bash\n    kubectl annotate addons.extensions.kubeblocks.io -l app.kubernetes.io/name=kubeblocks helm.sh/resource-policy=keep\n    ```\n\n2. ",
    "path": "docs/release-0_9/user_docs/upgrade-kubeblocks/upgrade-to-latest-version",
    "description": "  # Upgrade to KubeBlocks v0.9.3  :::note  - Before upgrading, check your current KubeBlocks version:     Run `helm -n kb-system list | grep kubeblocks` or `kbcli version`. - For upgrading to different versions:     - For v0.9.2 and v0.9.1, follow this upgrade tutorial, replacing the version number "
  },
  {
    "id": "docs_en_release-0_9_user_docs_user-management_manage_user_accounts",
    "title": "Manage user accounts",
    "content": "\n# Manage user accounts\n\nKubeBlocks offers a variety of services to enhance the usability, availability, and observability of database clusters. Different components require user accounts with different permissions to create connections.\n\n:::note\n\nCurrently, KubeBlocks only supports managing user accounts for ApeCloud MySQL, MySQL Community version, PostgreSQL, and Redis.\n\n:::\n\n***Steps***\n\n- Create a user account\n\n  ```bash\n  kbcli cluster create-account  --name  --password  \n  ```\n\n- Grant a role to a user\n\n  ```bash\n  kbcli cluster grant-role   --name  --role \n  ```\n\n  KubeBlocks provides three role levels of permission.\n\n  - Superuser: with all permissions.\n  - ReadWrite: read and write.\n  - ReadOnly: read only.\n  \n  For different database engines, the detailed permission are varied. Check the table below.\n\n    | Role      | MySQL    | PostgreSQL | Redis  |\n    | :------   | :------- | :------    | :----- |\n    | Superuser | GRANT SELECT, INSERT, UPDATE, DELETE, CREATE, DROP, RELOAD, SHUTDOWN, PROCESS, FILE, REFERENCES, INDEX, ALTER, SHOW DATABASES, SUPER, CREATE TEMPORARY TABLES, LOCK TABLES, EXECUTE, REPLICATION SLAVE, REPLICATION CLIENT, CREATE VIEW, SHOW VIEW, CREATE ROUTINE, ALTER ROUTINE, CREATE USER, EVENT, TRIGGER, CREATE TABLESPACE, CREATE ROLE, DROP ROLE ON * a user | ALTER USER WITH SUPERUSER | +@ALL allkeys|\n    | ReadWrite | GRANT SELECT, INSERT, DELETE ON * TO a user | GRANT pg_write_all_data TO a user | -@ALL +@Write +@READ allkeys |\n    | ReadOnly  | GRANT SELECT, SHOW VIEW ON * TO a user | GRANT pg_read_all_data TO a user | -@ALL +@READ allkeys |\n\n- Check the role level of a user account\n\n  ```bash\n  kbcli cluster describe-account  --name \n  ```\n\n- Revoke role from a user account\n\n  ```bash\n  kbcli cluster revoke-role  --name  --role  \n  ```\n\n- List all user accounts\n\n  ```bash\n  kbcli cluster list-accounts    \n  ```\n\n  :::note\n\n  For security reasons, the `list-accounts` command does not show all accounts. Accounts with high privilege such as o",
    "path": "docs/release-0_9/user_docs/user-management/manage_user_accounts",
    "description": " # Manage user accounts  KubeBlocks offers a variety of services to enhance the usability, availability, and observability of database clusters. Different components require user accounts with different permissions to create connections.  :::note  Currently, KubeBlocks only supports managing user ac"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-elasticsearch_04-operations_01-stop-start-restart",
    "title": "Elasticsearch  Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# Elasticsearch  Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a Elasticsearch  Cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a Elasticsearch  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a Elasticsearch  Cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Retains persistent storage (PVCs)\n3. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\nOption 1: OpsRequest API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: es-multinode-stop-ops\n  namespace: demo\nspec:\n  clusterName: es-multinode\n  type: Stop\n```\n\n\n\nOption 2: Cluster API Patch\n\nModify the cluster spec directly by patching the stop field:\n\n```bash\nkubectl patch cluster es-multinode -n demo --type='json' -p='[\n,\n\n]'\n```\n\n\n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster es-multinode -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME           CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\n    es-multinode                        Delete               Stopping   8m6s\n    es-multinode                        Delete               Stopped    9m41s\n    ```\n\n",
    "path": "docs/release-1_0/kubeblocks-for-elasticsearch/04-operations/01-stop-start-restart",
    "description": "  # Elasticsearch  Cluster Lifecycle Management  This guide demonstrates how to manage a Elasticsearch  Cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help optimize"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-elasticsearch_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a Elasticsearch  Cluster",
    "content": "\n\n\n\n# Vertical Scaling for Elasticsearch  Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a Elasticsearch  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for Elasticsearch instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks ensures minimal impact during scaling operations by following a controlled, role-aware update strategy:\n**Role-Aware Replicas (Primary/Secondary Replicas)**\n- Secondary replicas update first – Non-leader pods are upgraded to minimize disruption.\n- Primary updates last – Only after all secondaries are healthy does the primary pod restart.\n- Cluster state progresses from Updating → Running once all replicas are stable.\n\n**Role-Unaware Replicas (Ordinal-Based Scaling)**\nIf replicas have no defined roles, updates follow Kubernetes pod ordinal order:\n- Highest ordinal first (e.g., pod-2 → pod-1 → pod-0) to ensure deterministic rollouts.\n\n## Prerequisites\n\n\n\n## Deploy a Elasticsearch  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Pods are updated in pod ordinal order, from highest to lowest, (e.g., pod-2 → pod-1 → pod-0)\n1. Cluster status transitions from `Updating` to `Running`\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the elasticsearch-broker component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: es-multinode-vscale-ops\n    namespace: demo\n  spec:\n    clusterName: es-multinode\n    type: VerticalScaling\n    verticalScaling:\n    - componentName: dit\n      requests:\n        cpu: '1'\n        memory: 1Gi\n      limits:\n       ",
    "path": "docs/release-1_0/kubeblocks-for-elasticsearch/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for Elasticsearch  Clusters with KubeBlocks  This guide demonstrates how to vertically scale a Elasticsearch  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies compute resourc"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-elasticsearch_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of Elasticsearch Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for Elasticsearch Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a Elasticsearch cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a Elasticsearch  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running`.\n2. Cluster status changes from `Updating` to `Running`\n\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the Elasticsearch cluster by adding 1 replica to elasticsearch component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: es-multinode-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: es-multinode\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: dit\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops es-multinode-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME                         TYPE                CLUSTER        STATUS    PROGRESS   AGE\n  es-multinode-scale-out-ops   HorizontalScaling   es-multinode   Running   0/1        9s\n  es-multinode-scale-out-ops   HorizontalScaling   es-multinode   Running   1/1        16s\n  es-multinode-scale-out-ops   HorizontalScaling   es-multinode   Succeed   1/1        16s\n  ```\n  \n\n\n  Option 2: Direct Cluster API Update\n\n  Alternatively, you can perform a direct update to the `replicas` field in the Cluster resource:\n\n  ```yaml\n  apiVersion: apps.kubeblocks.io/v1\n  kind: Cluster\n  spec:\n    componentSpecs:\n      - name: dit\n        replicas: 4 # increase replicas",
    "path": "docs/release-1_0/kubeblocks-for-elasticsearch/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for Elasticsearch Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a Elasticsearch cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Pr"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-elasticsearch_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a Elasticsearch Cluster",
    "content": "\n\n\n\n# Expanding Volume in a Elasticsearch Cluster\n\nThis guide explains how to expand Persistent Volume Claims (PVCs) in a Elasticsearch cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the underlying storage class, this operation can be performed without downtime.\n\nVolume expansion allows you to increase the size of a Persistent Volume Claim (PVC) after it has been created. This feature was introduced in Kubernetes v1.11 and became generally available (GA) in Kubernetes v1.24.\n\n## Prerequisites\n\n\n\n### Check the Storage Class for Volume Expansion Support\n\nList all available storage classes and verify if volume expansion is supported by checking the `ALLOWVOLUMEEXPANSION` field:\n```bash\nkubectl get storageclass\n```\n\nExample Output:\n```bash\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4d10h\nkb-default-sc       ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   3d7h\nsc-s3-repo-2qsxfh   ru.yandex.s3.csi        Retain          Immediate              false                  3d7h\n```\nEnsure the storage class you are using has `ALLOWVOLUMEEXPANSION` set to true. If it is false, the storage class does not support volume expansion.\n\n## Deploy a Elasticsearch  Cluster with StorageClass\n\nKubeBlocks uses a declarative approach to manage Elasticsearch clusters. Below is an example configuration for deploying a Elasticsearch cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: es-multinode\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  componentSpecs:\n    - name: dit\n      componentDef: elasticsearch-8\n      serviceVersion: 8.8.2\n      configs:\n",
    "path": "docs/release-1_0/kubeblocks-for-elasticsearch/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a Elasticsearch Cluster  This guide explains how to expand Persistent Volume Claims (PVCs) in a Elasticsearch cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When suppo"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-elasticsearch_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy Elasticsearch Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage Elasticsearch Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing Elasticsearch services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n## Deploy a Elasticsearch  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## View Network Services\nList the Services created for the Elasticsearch cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=es-multinode -n demo\n```\n\nExample Services:\n```bash\nNAME                       TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE\nes-multinode-dit-http      ClusterIP   10.96.224.72           9200/TCP   56m\nes-multinode-master-http   ClusterIP   10.96.153.35           9200/TCP   56m\n```\n\n## Expose Elasticsearch Service\n\nExternal service addresses enable public internet access to Elasticsearch, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the Elasticsearch service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: es-multinode-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: es-multinode\n    expose:\n    - componentName: master\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options are 'ClusterIP', 'NodePort', and 'LoadBalancer'.\n        serviceTyp",
    "path": "docs/release-1_0/kubeblocks-for-elasticsearch/04-operations/05-manage-loadbalancer",
    "description": "    # Manage Elasticsearch Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing Elasticsearch services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer se"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-elasticsearch_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed Elasticsearch Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed Elasticsearch Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in Elasticsearch clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a Elasticsearch Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\nTo decommission a specific Pod (e.g., 'es-multinode-dit-1'), you can use one of the following methods:\n\n\n\n  Option 1: Using OpsRequest\n\n  Create an OpsRequest to mark the Pod as offline:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: es-multinode-decommission-ops\n    namespace: demo\n  spec:\n    clusterName: es-multinode\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: dit\n      scaleIn:\n        onlineInstancesToOffline:\n          - 'es-multinode-dit-1'  # Specifies the instance names that need to be taken offline\n  ```\n\n  #### Monitor the Decommiss",
    "path": "docs/release-1_0/kubeblocks-for-elasticsearch/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed Elasticsearch Clusters  This guide explains how to decommission (take offline) specific Pods in Elasticsearch clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use th"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-elasticsearch_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for Elasticsearch Clusters with the Prometheus Operator",
    "content": "\n\n# Elasticsearch Monitoring with Prometheus Operator\n\nThis guide demonstrates how to configure comprehensive monitoring for Elasticsearch clusters in KubeBlocks using:\n\n1. Prometheus Operator for metrics collection\n2. Built-in Elasticsearch exporter for metrics exposure\n3. Grafana for visualization\n\n## Prerequisites\n\n\n\n## Install Monitoring Stack\n\n### 1. Install Prometheus Operator\nDeploy the kube-prometheus-stack using Helm:\n\n```bash\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  -n monitoring \\\n  --create-namespace\n```\n\n### 2. Verify Installation\nCheck all components are running:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-jmxg8                1/1     Running   0          2m1s\n```\n\n\n## Deploy a Elasticsearch Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Configure Metrics Collection\n\n### 1. Verify Exporter Endpoint\n\n```bash\nkubectl -n demo exec -it pods/es-multinode-dit-0 -- \\\n  curl -s http://127.0.0.1:9114/metrics | head -n 50\n\nkubectl -n demo exec -it pods/es-multinode-master-0 -- \\\n  curl -s http:",
    "path": "docs/release-1_0/kubeblocks-for-elasticsearch/08-monitoring/01-integrate-with-prometheus-operator",
    "description": "  # Elasticsearch Monitoring with Prometheus Operator  This guide demonstrates how to configure comprehensive monitoring for Elasticsearch clusters in KubeBlocks using:  1. Prometheus Operator for metrics collection 2. Built-in Elasticsearch exporter for metrics exposure 3. Grafana for visualization"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-elasticsearch__tpl__create-cluster",
    "title": "_create-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing Elasticsearch  Clusters.\nBelow is an example configuration for deploying a Elasticsearch Cluster with\ncreate a cluster with replicas for different roles.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: es-multinode\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  componentSpecs:\n    - name: dit\n      componentDef: elasticsearch-8\n      serviceVersion: 8.8.2\n      configs:\n        - name: es-cm\n          variables:\n            # use key `roles` to specify roles this component assume\n            roles: data,ingest,transform\n      replicas: 3\n      disableExporter: false\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: \"2Gi\"\n        requests:\n          cpu: \"1\"\n          memory: \"2Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n    - name: master\n      componentDef: elasticsearch-8\n      serviceVersion: 8.8.2\n      configs:\n        - name: es-cm\n          variables:\n            # use key `roles` to specify roles this component assume\n            roles: master\n      replicas: 3\n      disableExporter: false\n      resources:\n        limits:\n          cpu: \"1\"\n          memory: \"2Gi\"\n        requests:\n          cpu: \"1\"\n          memory: \"2Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```",
    "path": "docs/release-1_0/kubeblocks-for-elasticsearch/_tpl/_create-cluster",
    "description": "KubeBlocks uses a declarative approach for managing Elasticsearch  Clusters. Below is an example configuration for deploying a Elasticsearch Cluster with create a cluster with replicas for different roles.  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps.kubeb"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-elasticsearch__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/release-1_0/kubeblocks-for-elasticsearch/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-elasticsearch__tpl__verify-cluster",
    "title": "_verify-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster es-multinode -n demo -w\n```\n\nExpected Output:\n\n```bash\nNAME           CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nes-multinode                        Delete               Creating   10s\nes-multinode                        Delete               Updating   41s\nes-multinode                        Delete               Running    42s\n```\n\nCheck the pod status and roles:\n```bash\nkubectl get pods -l app.kubernetes.io/instance=es-multinode -n demo\n```\n\nExpected Output:\n```bash\nNAME                    READY   STATUS    RESTARTS   AGE\nes-multinode-dit-0      3/3     Running   0          6m21s\nes-multinode-dit-1      3/3     Running   0          6m21s\nes-multinode-dit-2      3/3     Running   0          6m21s\nes-multinode-master-0   3/3     Running   0          6m21s\nes-multinode-master-1   3/3     Running   0          6m21s\nes-multinode-master-2   3/3     Running   0          6m21s\n```\n\nOnce the cluster status becomes Running, your Elasticsearch cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::\n",
    "path": "docs/release-1_0/kubeblocks-for-elasticsearch/_tpl/_verify-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster es-multinode -n demo -w ```  Expected Output:  ```bash NAME           CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE es-multinode                        Delete               Creating   10s es-"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-kafka_04-operations_01-stop-start-restart",
    "title": "Kafka  Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# Kafka  Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a Kafka  Cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a Kafka  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a Kafka  Cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Retains persistent storage (PVCs)\n3. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\nOption 1: OpsRequest API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: kafka-separated-cluster-stop-ops\n  namespace: demo\nspec:\n  clusterName: kafka-separated-cluster\n  type: Stop\n```\n\n\n\nOption 2: Cluster API Patch\n\nModify the cluster spec directly by patching the stop field:\n\n```bash\nkubectl patch cluster kafka-separated-cluster -n demo --type='json' -p='[\n,\n,\n\n]'\n```\n\n\n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster kafka-separated-cluster -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME                      CLUSTER-DEFINITION    TERMINATION-POLICY   STATUS     AGE\n    kafka-separated-cluster   kafka                 Delete               Stopping   16m3s\n    kafka-separated-cluster   kafka             ",
    "path": "docs/release-1_0/kubeblocks-for-kafka/04-operations/01-stop-start-restart",
    "description": "  # Kafka  Cluster Lifecycle Management  This guide demonstrates how to manage a Kafka  Cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help optimize resource usage "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-kafka_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a Kafka  Cluster",
    "content": "\n\n\n\n# Vertical Scaling for Kafka  Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a Kafka  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for Kafka instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks ensures minimal impact during scaling operations by following a controlled, role-aware update strategy:\n**Role-Aware Replicas (Primary/Secondary Replicas)**\n- Secondary replicas update first – Non-leader pods are upgraded to minimize disruption.\n- Primary updates last – Only after all secondaries are healthy does the primary pod restart.\n- Cluster state progresses from Updating → Running once all replicas are stable.\n\n**Role-Unaware Replicas (Ordinal-Based Scaling)**\nIf replicas have no defined roles, updates follow Kubernetes pod ordinal order:\n- Highest ordinal first (e.g., pod-2 → pod-1 → pod-0) to ensure deterministic rollouts.\n\n## Prerequisites\n\n\n\n## Deploy a Kafka  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Pods are updated in pod ordinal order, from highest to lowest, (e.g., pod-2 → pod-1 → pod-0)\n1. Cluster status transitions from `Updating` to `Running`\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the kafka-broker component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: kafka-separated-cluster-vscale-ops\n    namespace: demo\n  spec:\n    clusterName: kafka-separated-cluster\n    type: VerticalScaling\n    verticalScaling:\n    - componentName: kafka-broker\n      requests:\n        cpu: '1'\n        memory: 1Gi\n      limits:\n        cpu: '1'",
    "path": "docs/release-1_0/kubeblocks-for-kafka/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for Kafka  Clusters with KubeBlocks  This guide demonstrates how to vertically scale a Kafka  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies compute resources (CPU and memo"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-kafka_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of Kafka Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for Kafka Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a Kafka cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a Kafka  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running`.\n2. Cluster status changes from `Updating` to `Running`\n\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the Kafka cluster by adding 1 replica to kafka component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: kafka-separated-cluster-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: kafka-separated-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: kafka-broker\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops kafka-separated-cluster-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME                                    TYPE                CLUSTER                   STATUS    PROGRESS   AGE\n  kafka-separated-cluster-scale-out-ops   HorizontalScaling   kafka-separated-cluster   Running   0/1        9s\n  kafka-separated-cluster-scale-out-ops   HorizontalScaling   kafka-separated-cluster   Running   1/1        16s\n  kafka-separated-cluster-scale-out-ops   HorizontalScaling   kafka-separated-cluster   Succeed   1/1        16s\n  ```\n  \n\n\n  Option 2: Direct Cluster API Update\n\n  Alternatively, you can perform a direct update to the `replicas` field in the Cluster resource:\n\n  ```yaml\n  apiVersion: apps.kubeblocks.io/v1\n  kind: Clu",
    "path": "docs/release-1_0/kubeblocks-for-kafka/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for Kafka Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a Kafka cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequisites    #"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-kafka_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a Kafka Cluster",
    "content": "\n\n\n\n# Expanding Volume in a Kafka Cluster\n\nThis guide explains how to expand Persistent Volume Claims (PVCs) in a Kafka cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the underlying storage class, this operation can be performed without downtime.\n\nVolume expansion allows you to increase the size of a Persistent Volume Claim (PVC) after it has been created. This feature was introduced in Kubernetes v1.11 and became generally available (GA) in Kubernetes v1.24.\n\n## Prerequisites\n\n\n\n### Check the Storage Class for Volume Expansion Support\n\nList all available storage classes and verify if volume expansion is supported by checking the `ALLOWVOLUMEEXPANSION` field:\n```bash\nkubectl get storageclass\n```\n\nExample Output:\n```bash\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4d10h\nkb-default-sc       ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   3d7h\nsc-s3-repo-2qsxfh   ru.yandex.s3.csi        Retain          Immediate              false                  3d7h\n```\nEnsure the storage class you are using has `ALLOWVOLUMEEXPANSION` set to true. If it is false, the storage class does not support volume expansion.\n\n## Deploy a Kafka  Cluster with StorageClass\n\nKubeBlocks uses a declarative approach to manage Kafka clusters. Below is an example configuration for deploying a Kafka cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: kafka-separated-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: kafka\n  topology: separated_monitor\n  componentSpecs:\n    - name: kafka-broker\n      replicas: 3\n      resources:\n        limits",
    "path": "docs/release-1_0/kubeblocks-for-kafka/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a Kafka Cluster  This guide explains how to expand Persistent Volume Claims (PVCs) in a Kafka cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the unde"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-kafka_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy Kafka Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage Kafka Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing Kafka services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n## Deploy a Kafka  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## View Network Services\nList the Services created for the Kafka cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=kafka-separated-cluster -n demo\n```\n\nExample Services:\n```bash\nNAME                                                         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nkafka-separated-cluster-kafka-broker-advertised-listener-0   ClusterIP   10.96.101.247           9092/TCP   19m\n```\n\n## Expose Kafka Service\n\nExternal service addresses enable public internet access to Kafka, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the Kafka service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: kafka-separated-cluster-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: kafka-separated-cluster\n    expose:\n    - componentName: kafka-broker\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options are 'ClusterIP', 'NodePort', and 'LoadBalancer'.\n        serviceType: LoadBalancer\n        ",
    "path": "docs/release-1_0/kubeblocks-for-kafka/04-operations/05-manage-loadbalancer",
    "description": "    # Manage Kafka Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing Kafka services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage i"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-kafka_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed Kafka Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed Kafka Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in Kafka clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a Kafka Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\n\nBefore decommissioning a specific pod from a component, make sure this component has more than one replicas.\nIf not, please scale out the component ahead.\n\nE.g. you can patch the cluster CR with command, to declare there are 3 replicas in component querynode.\n\n```bash\nkubectl patch cluster kafka-separated-cluster -n demo --type='json' -p='[\n  \n]'\n```\n\nWait till all pods are running\n```bash\nkubectl get pods -n demo -l app.kubernetes.io/instance=kafka-separated-cluster,apps.kubeblocks.io/component-name=kafka-broker\n```\nExpected Output:\n```\nNAME                                     READY   STATUS    RESTARTS   AGE\nkafka-separated-cluster-kafka-broke",
    "path": "docs/release-1_0/kubeblocks-for-kafka/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed Kafka Clusters  This guide explains how to decommission (take offline) specific Pods in Kafka clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-kafka_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for Kafka Clusters with the Prometheus Operator",
    "content": "\n\n# Kafka Monitoring with Prometheus Operator\n\nThis guide demonstrates how to configure comprehensive monitoring for Kafka clusters in KubeBlocks using:\n\n1. Prometheus Operator for metrics collection\n2. Built-in Kafka exporter for metrics exposure\n3. Grafana for visualization\n\n## Prerequisites\n\n\n\n## Install Monitoring Stack\n\n### 1. Install Prometheus Operator\nDeploy the kube-prometheus-stack using Helm:\n\n```bash\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  -n monitoring \\\n  --create-namespace\n```\n\n### 2. Verify Installation\nCheck all components are running:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-jmxg8                1/1     Running   0          2m1s\n```\n\n\n## Deploy a Kafka Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Configure Metrics Collection\n\n### 1. Get Exporter details\n\n```bash\nkubectl get po -n demo kafka-separated-cluster-kafka-broker-0 -oyaml | yq '.spec.containers[] | select(.name==\"jmx-exporter\") | .ports'\n```\n\n\nExample Output:\n\n```text\n- containerPort: 5556\n  name: metrics\n  protocol:",
    "path": "docs/release-1_0/kubeblocks-for-kafka/08-monitoring/01-integrate-with-prometheus-operator",
    "description": "  # Kafka Monitoring with Prometheus Operator  This guide demonstrates how to configure comprehensive monitoring for Kafka clusters in KubeBlocks using:  1. Prometheus Operator for metrics collection 2. Built-in Kafka exporter for metrics exposure 3. Grafana for visualization  ## Prerequisites    ##"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-kafka__tpl__create-cluster",
    "title": "_create-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing Kafka  Clusters.\nBelow is an example configuration for deploying a Kafka Cluster with 3 components\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: kafka-separated-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: kafka\n  topology: separated_monitor\n  componentSpecs:\n    - name: kafka-broker\n      replicas: 1\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      env:\n        - name: KB_KAFKA_BROKER_HEAP\n          value: \"-XshowSettings:vm -XX:MaxRAMPercentage=100 -Ddepth=64\"\n        - name: KB_KAFKA_CONTROLLER_HEAP\n          value: \"-XshowSettings:vm -XX:MaxRAMPercentage=100 -Ddepth=64\"\n        - name: KB_BROKER_DIRECT_POD_ACCESS\n          value: \"true\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n        - name: metadata\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 1Gi\n    - name: kafka-controller\n      replicas: 1\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: metadata\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 1Gi\n    - name: kafka-exporter\n      replicas: 1\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"1Gi\"\n        requests:\n          cpu: \"0.1\"\n          memory: \"0.2Gi\"\n```\n\n\n:::note\n\nThese three c",
    "path": "docs/release-1_0/kubeblocks-for-kafka/_tpl/_create-cluster",
    "description": "KubeBlocks uses a declarative approach for managing Kafka  Clusters. Below is an example configuration for deploying a Kafka Cluster with 3 components  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps.kubeblocks.io/v1 kind: Cluster metadata:   name: kafka-separ"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-kafka__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/release-1_0/kubeblocks-for-kafka/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-kafka__tpl__verify-cluster",
    "title": "_verify-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster kafka-separated-cluster -n demo -w\n```\n\nExpected Output:\n\n```bash\nkubectl get cluster kafka-separated-cluster -n demo\nNAME                      CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nkafka-separated-cluster   kafka                Delete               Creating   13s\nkafka-separated-cluster   kafka                Delete               Running    63s\n```\n\nCheck the pod status and roles:\n```bash\nkubectl get pods -l app.kubernetes.io/instance=kafka-separated-cluster -n demo\n```\n\nExpected Output:\n```bash\nNAME                                         READY   STATUS    RESTARTS   AGE\nkafka-separated-cluster-kafka-broker-0       2/2     Running   0          13m\nkafka-separated-cluster-kafka-controller-0   2/2     Running   0          13m\nkafka-separated-cluster-kafka-exporter-0     1/1     Running   0          12m\n```\n\nOnce the cluster status becomes Running, your Kafka cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::\n",
    "path": "docs/release-1_0/kubeblocks-for-kafka/_tpl/_verify-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster kafka-separated-cluster -n demo -w ```  Expected Output:  ```bash kubectl get cluster kafka-separated-cluster -n demo NAME                      CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE k"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-milvus_03-topologies_01-standlone",
    "title": "Deploying a Milvus Standalone Cluster with KubeBlocks",
    "content": "\n# Deploying a Milvus Standalone Cluster with KubeBlocks\n\nStandalone is a lightweight deployment suitable for development and testing with following components:\n\n- **Milvus Core**: Provides vector search and database functionality\n- **Metadata Storage (ETCD)**: Stores cluster metadata and configuration\n- **Object Storage (MinIO/S3)**: Persists vector data and indexes\n\n## Prerequisites\n\n\n\n## Deploying the Milvus Standalone Cluster\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: milvus-standalone\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  # The value must be `milvus` to create a Milvus Cluster\n  clusterDef: milvus\n  # Valid options are: [standalone,cluster]\n  topology: standalone\n  componentSpecs:\n    - name: etcd\n      replicas: 1\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n    - name: minio\n      replicas: 1\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n    - name: milvus\n      replicas: 1\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 10Gi\n```\n\n**Key Configuration",
    "path": "docs/release-1_0/kubeblocks-for-milvus/03-topologies/01-standlone",
    "description": " # Deploying a Milvus Standalone Cluster with KubeBlocks  Standalone is a lightweight deployment suitable for development and testing with following components:  - **Milvus Core**: Provides vector search and database functionality - **Metadata Storage (ETCD)**: Stores cluster metadata and configurat"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-milvus_03-topologies_02-cluster",
    "title": "Deploying a Milvus Cluster with KubeBlocks",
    "content": "\n# Deploying a Milvus Cluster with KubeBlocks\n\nMilvus Cluster is a distributed deployment for production workloads with multiple specialized components:\n\n**Access Layer**\n\n- Stateless proxies that handle client connections and request routing\n\n**Compute Layer**\n\n- Query Nodes: Execute search operations\n- Data Nodes: Handle data ingestion and compaction\n- Index Nodes: Build and maintain vector indexes\n\n**Coordination Layer**\n\n- Root Coordinator: Manages global metadata\n- Query Coordinator: Orchestrates query execution\n- Data Coordinator: Manages data distribution\n- Index Coordinator: Oversees index building\n\n**Storage Layer**\n\n- Metadata Storage (ETCD): Cluster metadata and configuration\n- Object Storage (MinIO/S3): Persistent vector data storage\n- Log Storage (Pulsar): Message queue for change data capture\n\n## Prerequisites\n\n\n\n## Deploying the Milvus Cluster\n\n### Step 1. Deploy an ETCD Cluster\n\nETCD cluster is for metadata storage\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: etcdm-cluster\n  namespace: demo\nspec:\n  terminationPolicy: WipeOut\n  componentSpecs:\n    - name: etcd\n      componentDef: etcd-3-1.0.0\n      serviceVersion: 3.5.6\n      replicas: 1\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n\n### Step 2. Deploy a minio Cluster\n\nMinio is for object storage\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: miniom-cluster\n  namespace: demo\nspec:\n  terminationPolicy: WipeOut\n  componentSpecs:\n    - name: minio\n      componentDef: milvus-minio-1.0.0\n      replicas: 1\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n       ",
    "path": "docs/release-1_0/kubeblocks-for-milvus/03-topologies/02-cluster",
    "description": " # Deploying a Milvus Cluster with KubeBlocks  Milvus Cluster is a distributed deployment for production workloads with multiple specialized components:  **Access Layer**  - Stateless proxies that handle client connections and request routing  **Compute Layer**  - Query Nodes: Execute search operati"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-milvus_04-operations_01-stop-start-restart",
    "title": "Milvus Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# Milvus Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a Milvus Cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a Milvus Cluster\n\nPlease refer to [Deploying a Milvus Cluster with KubeBlocks](../03-topologies/02-cluster) to deploy a milvus cluster.\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a Milvus Cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\nOption 1: OpsRequest API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: milvus-cluster-stop-ops\n  namespace: demo\nspec:\n  clusterName: milvus-cluster\n  type: Stop\n```\n\n\n\nOption 2: Cluster API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: milvus-cluster-stop-ops\n  namespace: demo\nspec:\n  clusterName: milvus-cluster\n  type: Stop\n```\n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster milvus-cluster -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME             CLUSTER-DEFINITION  TERMINATION-POLICY   STATUS     AGE\n    milvus-cluster   milvus              Delete               Stopping   ",
    "path": "docs/release-1_0/kubeblocks-for-milvus/04-operations/01-stop-start-restart",
    "description": "  # Milvus Cluster Lifecycle Management  This guide demonstrates how to manage a Milvus Cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help optimize resource usage "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-milvus_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a Milvus  Cluster",
    "content": "\n\n\n\n# Vertical Scaling for Milvus Standalone Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a Milvus  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for Milvus instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks ensures minimal impact during scaling operations by following a controlled, role-aware update strategy:\n**Role-Aware Replicas (Primary/Secondary Replicas)**\n- Secondary replicas update first – Non-leader pods are upgraded to minimize disruption.\n- Primary updates last – Only after all secondaries are healthy does the primary pod restart.\n- Cluster state progresses from Updating → Running once all replicas are stable.\n\n**Role-Unaware Replicas (Ordinal-Based Scaling)**\nIf replicas have no defined roles, updates follow Kubernetes pod ordinal order:\n- Highest ordinal first (e.g., pod-2 → pod-1 → pod-0) to ensure deterministic rollouts.\n\n## Prerequisites\n\n\n\n## Deploy a Milvus Cluster\n\nPlease refer to [Deploying a Milvus Cluster with KubeBlocks](../03-topologies/02-cluster) to deploy a milvus cluster.\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Pods are updated in pod ordinal order, from highest to lowest, (e.g., pod-2 → pod-1 → pod-0)\n1. Cluster status transitions from `Updating` to `Running`\n\n**Check Components**\n\nThere are five components in Milvus Cluster. To get the list of components,\n```bash\nkubectl get cluster -n demo milvus-cluster -oyaml | yq '.spec.componentSpecs[].name'\n```\n\nExpected Output:\n```text\nproxy\nmixcoord\ndatanode\nindexnode\nquerynode\n```\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the **querynode** component",
    "path": "docs/release-1_0/kubeblocks-for-milvus/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for Milvus Standalone Clusters with KubeBlocks  This guide demonstrates how to vertically scale a Milvus  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies compute resources ("
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-milvus_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of Milvus Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for Milvus Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a Milvus cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a Milvus Cluster\n\nPlease refer to [Deploying a Milvus Cluster with KubeBlocks](../03-topologies/02-cluster) to deploy a milvus cluster.\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running`.\n2. Cluster status changes from `Updating` to `Running`\n\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the Milvus cluster by adding 1 replica to milvus component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: milvus-cluster-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: milvus-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: querynode\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops milvus-cluster-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME                             TYPE                CLUSTER          STATUS    PROGRESS   AGE\n  milvus-cluster-scale-out-ops     HorizontalScaling   milvus-cluster   Running   0/1        9s\n  milvus-cluster-scale-out-ops     HorizontalScaling   milvus-cluster   Running   1/1        16s\n  milvus-cluster-scale-out-ops     HorizontalScaling   milvus-cluster   Succeed   1/1        16s\n  ```\n  \n\n\n  Option 2: Direct Cluster API Update\n\n  Alternatively, you can perform a direct update to the `replicas` field in the Cluster resource:\n\n  ```yaml\n  apiVersion: apps.kubeblocks.io/v1\n  kind: Cluster\n",
    "path": "docs/release-1_0/kubeblocks-for-milvus/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for Milvus Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a Milvus cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequisites   "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-milvus_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy Milvus Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage Milvus Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing Milvus services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n\n## Deploy a Milvus Cluster\n\nPlease refer to [Deploying a Milvus Cluster with KubeBlocks](../03-topologies/02-cluster) to deploy a milvus cluster.\n\n## View Network Services\nList the Services created for the Milvus cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=milvus-cluster -n demo\n```\n\nExample Services:\n```bash\nNAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)              AGE\nmilvus-cluster-proxy   ClusterIP   10.96.157.187           19530/TCP,9091/TCP   133m\n```\n\n## Expose Milvus Service\n\nExternal service addresses enable public internet access to Milvus, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the Milvus service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: milvus-cluster-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: milvus-cluster\n    expose:\n    - componentName: milvus\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options are 'ClusterIP', 'NodePort', and 'LoadBalancer'.\n        serviceType: LoadBalancer\n   ",
    "path": "docs/release-1_0/kubeblocks-for-milvus/04-operations/05-manage-loadbalancer",
    "description": "    # Manage Milvus Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing Milvus services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-milvus_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed Milvus Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed Milvus Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in Milvus clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a Milvus Cluster\n\nPlease refer to [Deploying a Milvus Cluster with KubeBlocks](../03-topologies/02-cluster) to deploy a milvus cluster.\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\n\nBefore decommissioning a specific pod from a component, make sure this component has more than one replicas.\nIf not, please scale out the componen ahead.\n\nE.g. you can patch the cluster CR with command, to declare there are 3 replicas in component querynode.\n\n```bash\nkubectl patch cluster milvus-cluster -n demo --type='json' -p='[\n  \n]'\n```\n\n\nTo decommission a specific Pod (e.g., 'milvus-cluster-querynode-1'), you can use one of the following methods:\n\n\n\n  Option 1: Using OpsRequest\n\n  Create an OpsRequest to mark the Pod as offline:\n\n  ```yaml\n  apiVersion:",
    "path": "docs/release-1_0/kubeblocks-for-milvus/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed Milvus Clusters  This guide explains how to decommission (take offline) specific Pods in Milvus clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workloa"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-milvus__tpl__create-cluster",
    "title": "_create-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing Milvus Clusters.\nBelow is an example configuration for deploying a Milvus Cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: milvus-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: milvus\n  topology: clustermode\n  componentSpecs:\n    - name: milvus\n      serviceVersion: 3.13.7\n      replicas: 3\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n",
    "path": "docs/release-1_0/kubeblocks-for-milvus/_tpl/_create-cluster",
    "description": "KubeBlocks uses a declarative approach for managing Milvus Clusters. Below is an example configuration for deploying a Milvus Cluster with 3 replicas.  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps.kubeblocks.io/v1 kind: Cluster metadata:   name: milvus-clus"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-milvus__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/release-1_0/kubeblocks-for-milvus/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-milvus__tpl__verify-cluster",
    "title": "_verify-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster milvus-cluster -n demo -w\n```\n\nExpected Output:\n\n```bash\nNAME                CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nmilvus-standalone   milvus               Delete               Creating   40s\nmilvus-standalone   milvus               Delete               Creating   71s\nmilvus-standalone   milvus               Delete               Creating   71s\nmilvus-standalone   milvus               Delete               Updating   71s\nmilvus-standalone   milvus               Delete               Running    2m55s\n```\n\nCheck the component and pod status:\n\n```bash\nkubectl get component -n demo -l app.kubernetes.io/instance=milvus-standalone\n```\nExpected Output:\n```bash\nNAME                       DEFINITION                        SERVICE-VERSION   STATUS    AGE\nmilvus-standalone-etcd     etcd-3-1.0.0                      3.5.15            Running   3m5s\nmilvus-standalone-milvus   milvus-standalone-1.0.0           v2.3.2            Running   114s\nmilvus-standalone-minio    milvus-minio-1.0.0                8.0.17            Running   3m5s\n```\n\n\n```bash\nkubectl get pods -l app.kubernetes.io/instance=milvus-standalone -n demo\n```\n\nExpected Output:\n```bash\nNAME                         READY   STATUS    RESTARTS   AGE\nmilvus-standalone-etcd-0     2/2     Running   0          4m31s\nmilvus-standalone-milvus-0   1/1     Running   0          3m20s\nmilvus-standalone-minio-0    1/1     Running   0          4m31s\n```\n\nOnce the cluster status becomes Running, your Milvus cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::\n",
    "path": "docs/release-1_0/kubeblocks-for-milvus/_tpl/_verify-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster milvus-cluster -n demo -w ```  Expected Output:  ```bash NAME                CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE milvus-standalone   milvus               Delete               Creati"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-milvus_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for Milvus Clusters with the Prometheus Operator",
    "content": "\n\n# Milvus Monitoring with Prometheus Operator\n\nThis guide demonstrates how to configure comprehensive monitoring for Milvus clusters in KubeBlocks using:\n\n1. Prometheus Operator for metrics collection\n2. Built-in Milvus exporter for metrics exposure\n3. Grafana for visualization\n\n## Prerequisites\n\n\n\n## Install Monitoring Stack\n\n### 1. Install Prometheus Operator\nDeploy the kube-prometheus-stack using Helm:\n\n```bash\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  -n monitoring \\\n  --create-namespace\n```\n\n### 2. Verify Installation\nCheck all components are running:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-jmxg8                1/1     Running   0          2m1s\n```\n\n\n## Deploy a Milvus Cluster\n\nPlease refer to [Deploying a Milvus Cluster with KubeBlocks](../03-topologies/02-cluster) to deploy a milvus cluster.\n\n## Configure Metrics Collection\n\n### 1. Verify Exporter Endpoint\n\n```bash\nkubectl -n demo exec -it pods/milvus-cluster-proxy-0 -- \\\n  curl -s http://127.0.0.1:9091/metrics | head -n 50\n```\n\nPerfor",
    "path": "docs/release-1_0/kubeblocks-for-milvus/08-monitoring/01-integrate-with-prometheus-operator",
    "description": "  # Milvus Monitoring with Prometheus Operator  This guide demonstrates how to configure comprehensive monitoring for Milvus clusters in KubeBlocks using:  1. Prometheus Operator for metrics collection 2. Built-in Milvus exporter for metrics exposure 3. Grafana for visualization  ## Prerequisites   "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_04-operations_01-stop-start-restart",
    "title": "MongoDB ReplicaSet Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# MongoDB ReplicaSet Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a MongoDB ReplicaSet Cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB ReplicaSet Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a MongoDB ReplicaSet Cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Retains persistent storage (PVCs)\n3. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\nOption 1: OpsRequest API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: mongo-cluster-stop-ops\n  namespace: demo\nspec:\n  clusterName: mongo-cluster\n  type: Stop\n```\n\n\n\nOption 2: Cluster API Patch\n\nModify the cluster spec directly by patching the stop field:\n\n```bash\nkubectl patch cluster mongo-cluster -n demo --type='json' -p='[\n\n]'\n```\n\n\n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster mongo-cluster -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME            CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\n    mongo-cluster   mongodb              Delete               Stopping   6m3s\n    mongo-cluster   mongodb              Delete               Stopp",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/04-operations/01-stop-start-restart",
    "description": "  # MongoDB ReplicaSet Cluster Lifecycle Management  This guide demonstrates how to manage a MongoDB ReplicaSet Cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a MongoDB ReplicaSet Cluster",
    "content": "\n\n\n\n# Vertical Scaling for MongoDB Replication Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a MongoDB ReplicaSet Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for MongoDB instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks orchestrates scaling with minimal impact:\n1. Secondary replicas update first\n2. Primary updates last after secondaries are healthy\n3. Cluster status transitions from `Updating` to `Running`\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB ReplicaSet Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Secondary replicas are updated first (one at a time)\n1. Primary is updated last after secondary replicas are healthy\n1. Cluster status transitions from `Updating` to `Running`\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the mongodb component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: mongo-cluster-vscale-ops\n    namespace: demo\n  spec:\n    clusterName: mongo-cluster\n    type: VerticalScaling\n    verticalScaling:\n    - componentName: mongodb\n      requests:\n        cpu: '1'\n        memory: 1Gi\n      limits:\n        cpu: '1'\n        memory: 1Gi\n  ```\n  What Happens During Vertical Scaling?\n  - Secondary Pods are recreated first to ensure the primary Pod remains available.\n  - Once all secondary Pods are updated, the primary Pod is restarted with the new resource configuration.\n\n\n  You can check the progress of the scaling operation with the following command:\n\n  ```bash\n  kubectl -n demo get ops mongo-cluster-vscale-ops -w\n  ```\n\n  E",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for MongoDB Replication Clusters with KubeBlocks  This guide demonstrates how to vertically scale a MongoDB ReplicaSet Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies comput"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of MongoDB Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for MongoDB Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a MongoDB cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB ReplicaSet Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running` with `secondary` role\n2. Data synced from primary to new replica\n3. Cluster status changes from `Updating` to `Running`\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the MongoDB cluster by adding 1 replica to mongodb component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: mongo-cluster-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: mongo-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: mongodb\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops mongo-cluster-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME                          TYPE                CLUSTER         STATUS    PROGRESS   AGE\n  mongo-cluster-scale-out-ops   HorizontalScaling   mongo-cluster   Running   0/1        9s\n  mongo-cluster-scale-out-ops   HorizontalScaling   mongo-cluster   Running   1/1        20s\n  mongo-cluster-scale-out-ops   HorizontalScaling   mongo-cluster   Succeed   1/1        20s\n  ```\n  \n\n\n  Option 2: Direct Cluster API Update\n\n  Alternatively, you can perform a direct update to the `replicas` field in the Cluster resource:\n\n  ```yaml\n  apiVersion: apps.kubeblocks.io/v1\n  kind: Cluster\n  spec:\n    componentSpecs:",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for MongoDB Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a MongoDB cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequisites "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a MongoDB Cluster",
    "content": "\n\n\n\n# Expanding Volume in a MongoDB Cluster\n\nThis guide explains how to expand Persistent Volume Claims (PVCs) in a MongoDB cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the underlying storage class, this operation can be performed without downtime.\n\nVolume expansion allows you to increase the size of a Persistent Volume Claim (PVC) after it has been created. This feature was introduced in Kubernetes v1.11 and became generally available (GA) in Kubernetes v1.24.\n\n## Prerequisites\n\n\n\n### Check the Storage Class for Volume Expansion Support\n\nList all available storage classes and verify if volume expansion is supported by checking the `ALLOWVOLUMEEXPANSION` field:\n```bash\nkubectl get storageclass\n```\n\nExample Output:\n```bash\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4d10h\nkb-default-sc       ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   3d7h\nsc-s3-repo-2qsxfh   ru.yandex.s3.csi        Retain          Immediate              false                  3d7h\n```\nEnsure the storage class you are using has `ALLOWVOLUMEEXPANSION` set to true. If it is false, the storage class does not support volume expansion.\n\n## Deploy a MongoDB ReplicaSet Cluster with StorageClass\n\nKubeBlocks uses a declarative approach to manage MongoDB clusters. Below is an example configuration for deploying a MongoDB cluster with 2 replicas (1 primary, 1 secondary).\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: mongo-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: mongodb\n  topology: replicaset\n  componentSpecs:\n    - name: mongodb\n      serviceVersion: \"6",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a MongoDB Cluster  This guide explains how to expand Persistent Volume Claims (PVCs) in a MongoDB cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy MongoDB Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage MongoDB Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing MongoDB services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB ReplicaSet Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## View Network Services\nList the Services created for the MongoDB cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=mongo-cluster -n demo\n```\n\nExample Services:\n```bash\n# service for to all replicas\nmongo-cluster-mongodb              ClusterIP   10.96.249.157           27017/TCP   44m\n# read-write service\nmongo-cluster-mongodb-mongodb      ClusterIP   10.96.17.58             27017/TCP   44m\n# read-only servcie\nmongo-cluster-mongodb-mongodb-ro   ClusterIP   10.96.2.71              27017/TCP   44m\n```\n\n## Expose MongoDB Service\n\nExternal service addresses enable public internet access to MongoDB, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the MongoDB service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: mongo-cluster-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: mongo-cluster\n    expose:\n    - componentName: mongodb\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options ",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/04-operations/05-manage-loadbalancer",
    "description": "    # Manage MongoDB Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing MongoDB services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, mana"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_04-operations_08-switchover",
    "title": "MongoDB Cluster Switchover",
    "content": "\n\n# MongoDB Cluster Switchover\n\nA **switchover** is a planned operation that transfers the primary role from one MongoDB instance to another. Unlike failover which occurs during failures, switchover provides:\n- Controlled role transitions\n- Minimal downtime (typically a few hundred milliseconds)\n- Predictable maintenance windows\n\nSwitchover is ideal for:\n- Node maintenance/upgrades\n- Workload rebalancing\n- Testing high availability\n- Planned infrastructure changes\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Check Roles\nList the Pods and their roles (primary or secondary):\n\n```bash\nkubectl get pods -n demo -l app.kubernetes.io/instance=mongo-cluster,apps.kubeblocks.io/component-name=mongodb -L kubeblocks.io/role\n```\n\nExample Output:\n\n```text\nNAME                      READY   STATUS    RESTARTS   AGE   ROLE\nmongo-cluster-mongodb-0   2/2     Running   0          20m   primary\nmongo-cluster-mongodb-1   2/2     Running   0          21m   secondary\nmongo-cluster-mongodb-2   2/2     Running   0          19m   secondary\n```\n\n## Performing a Planned Switchover\n\nTo initiate a planned switchover, create an OpsRequest resource as shown below:\n\n  Option 1: Automatic Switchover (No preferred candidate)\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: mongodb-switchover-ops\n    namespace: demo\n  spec:\n    clusterName: mongo-cluster\n    type: Switchover\n    switchover:\n    - componentName: mongodb\n      instanceName: mongo-cluster-mongodb-0\n  ```\n **Key Parameters:**\n  - `instanceName`: Specifies the instance (Pod) that is primary or leader before a switchover operation.\n\n  \n  Option 2: Targeted Switchover (Specific candidate)\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: mongodb-switchover-targeted\n    namespace: demo\n  spec:\n    clusterName: mongo-cluster\n    type: Switchover\n    switchover:\n    - componentName: mongodb\n      # Specifi",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/04-operations/08-switchover",
    "description": "  # MongoDB Cluster Switchover  A **switchover** is a planned operation that transfers the primary role from one MongoDB instance to another. Unlike failover which occurs during failures, switchover provides: - Controlled role transitions - Minimal downtime (typically a few hundred milliseconds) - P"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed MongoDB Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed MongoDB Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in MongoDB clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\nTo decommission a specific Pod (e.g., 'mongo-cluster-mongodb-1'), you can use one of the following methods:\n\n\n\n  Option 1: Using OpsRequest\n\n  Create an OpsRequest to mark the Pod as offline:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: mongo-cluster-decommission-ops\n    namespace: demo\n  spec:\n    clusterName: mongo-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: mongodb\n      scaleIn:\n        onlineInstancesToOffline:\n          - 'mongo-cluster-mongodb-1'  # Specifies the instance names that need to be taken offline\n  ```\n\n  #### Monitor the Decommissio",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed MongoDB Clusters  This guide explains how to decommission (take offline) specific Pods in MongoDB clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workl"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_06-custom-secret_01-custom-secret",
    "title": "Create a MongoDB Cluster with a Custom Root Password on KubeBlocks",
    "content": "\n# Create MongoDB Cluster With Custom Password on KubeBlocks\n\nThis guide demonstrates how to deploy a MongoDB cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.\n\n## Prerequisites\n\n\n\n## Deploying the MongoDB ReplicaSet Cluster\n\nKubeBlocks uses a declarative approach for managing MongoDB clusters. Below is an example configuration for deploying a MongoDB cluster with 2 nodes (1 primary, 1 replicas) and a custom root password.\n\n### Step 1: Create a Secret for the Defaults Account\n\nThe custom root password is stored in a Kubernetes Secret. Create the Secret by applying the following YAML:\n\n```yaml\napiVersion: v1\ndata:\n  password: Y3VzdG9tcGFzc3dvcmQ= # custompassword\n  username: cm9vdA== #root\nimmutable: true\nkind: Secret\nmetadata:\n  name: custom-secret\n  namespace: demo\n```\n- password: Replace custompassword with your desired password and encode it using Base64 (`echo -n \"custompassword\" | base64`).\n- username: The default MongoDB root user is 'root', encoded as 'cm9vdA=='.\n\n\n### Step 2: Deploy the MongoDB Cluster\n\nApply the following manifest to deploy the MongoDB cluster, referencing the Secret created in Step 1 for the root account:\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: mongo-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: mongodb\n  topology: replicaset\n  componentSpecs:\n    - name: mongodb\n      serviceVersion: \"6.0.16\"\n      replicas: 3\n      systemAccounts:  # override systemaccount password\n        - name: root\n          secretRef:\n            name: custom-secret\n            namespace: demo\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n**Explanation",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/06-custom-secret/01-custom-secret",
    "description": " # Create MongoDB Cluster With Custom Password on KubeBlocks  This guide demonstrates how to deploy a MongoDB cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.  ## Prerequisites    ## Deploying the MongoDB ReplicaSet Cluster  KubeBlocks uses a declarative approach for "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb__tpl__create-cluster",
    "title": "_create-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing MongoDB Replication Clusters.\nBelow is an example configuration for deploying a MongoDB ReplicaSet Cluster with one primary replica and two secondary replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: mongo-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: mongodb\n  topology: replicaset\n  componentSpecs:\n    - name: mongodb\n      serviceVersion: \"6.0.16\"\n      replicas: 3\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/_tpl/_create-cluster",
    "description": "KubeBlocks uses a declarative approach for managing MongoDB Replication Clusters. Below is an example configuration for deploying a MongoDB ReplicaSet Cluster with one primary replica and two secondary replicas.  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb__tpl__verify-cluster",
    "title": "_verify-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster mongo-cluster -n demo -w\n```\n\nExpected Output:\n\n```bash\nkubectl get cluster mongo-cluster -n demo\nNAME            CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nmongo-cluster   mongodb              Delete               Creating   49s\nmongo-cluster   mongodb              Delete               Running    62s\n```\n\nCheck the pod status and roles:\n```bash\nkubectl get pods -l app.kubernetes.io/instance=mongo-cluster -L  kubeblocks.io/role -n demo\n```\n\nExpected Output:\n```bash\nNAME                      READY   STATUS    RESTARTS   AGE   ROLE\nmongo-cluster-mongodb-0   2/2     Running   0          78s   primary\nmongo-cluster-mongodb-1   2/2     Running   0          63s   secondary\nmongo-cluster-mongodb-2   2/2     Running   0          48s   secondary\n```\n\nOnce the cluster status becomes Running, your MongoDB cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::\n",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/_tpl/_verify-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster mongo-cluster -n demo -w ```  Expected Output:  ```bash kubectl get cluster mongo-cluster -n demo NAME            CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE mongo-cluster   mongodb        "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_03-topologies_01-semisync",
    "title": "Deploying a MySQL Semi-Synchronous Cluster with KubeBlocks",
    "content": "\n# Deploying a MySQL Semi-Synchronous Cluster with KubeBlocks\n\n**Semi-synchronous replication** improves data consistency between the primary and replica nodes by requiring the primary node to wait for acknowledgment from at least one replica before committing transactions. This guide walks you through the process of setting up a MySQL semi-synchronous replication cluster using KubeBlocks.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n  - A Kubernetes cluster is up and running.\n  - The kubectl CLI tool is configured to communicate with your cluster.\n  - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploying the MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode.\n\nApply the following YAML configuration:\n```yaml\nkubectl apply -f - \n\nTopology:\nCOMPONENT   INSTANCE                         ROLE        STATUS    AZ                NODE                                                       CREATED-TIME\nmysql       example-mysql-cluster-mysql-0   primary     Running   ap-southeast-1a   ip-10-0-1-93.ap-southeast-1.compute.internal/10.0.1.93     Dec 24,2024 09:09 UTC+0800\nmysql       example-mysql-cluster-mysql-1   secondary   Running   ap-southeast-1b   ip-10-0-2-253.ap-southeast-1.compute.internal/10.0.2.253   Dec 24,2024 09:09 UTC+0800\n\nResources Allocation:\nCOMPONENT   DEDICATED   CPU(REQUEST/LIMIT)   MEMORY(REQUEST/LIMIT)   STORAGE-SIZE   STORAGE-CLASS\nmysql       false       500m / 500m          512Mi / 512Mi           data:20Gi      \n\nImages:\nCOMPONENT   TYPE   I",
    "path": "docs/release-1_0/kubeblocks-for-mysql/03-topologies/01-semisync",
    "description": " # Deploying a MySQL Semi-Synchronous Cluster with KubeBlocks  **Semi-synchronous replication** improves data consistency between the primary and replica nodes by requiring the primary node to wait for acknowledgment from at least one replica before committing transactions. This guide walks you thro"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_03-topologies_02-semisync-with-proxysql",
    "title": "Deploying a MySQL Semi-Synchronous Cluster with ProxySQL on KubeBlocks",
    "content": "\n# Deploying a MySQL Semi-Synchronous Cluster and ProxySQL with KubeBlocks\n\n**Semi-synchronous replication** enhances data consistency between the primary and replica nodes by ensuring that the primary waits for acknowledgment from at least one replica before committing transactions.\n\n**ProxySQL** is a high-performance MySQL proxy that acts as a middleware between MySQL clients and servers. It provides advanced features such as query routing, load balancing, query caching, and high availability. When combined with a MySQL semi-synchronous cluster, ProxySQL ensures seamless failover and efficient traffic management, resulting in optimal performance and reliability.\n\nThis guide walks you through deploying a MySQL semi-synchronous replication cluster integrated with ProxySQL using KubeBlocks.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n  - A Kubernetes cluster is up and running.\n  - The kubectl CLI tool is configured to communicate with your cluster.\n  - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploying the MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative configuration approach to manage MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 MySQL nodes (1 primary, 1 replica) and 2 ProxySQL instances.\n\nApply the following configuration:\n```yaml\nkubectl apply -f - \nproxysql    example-mysql-cluster-proxysql-proxy-ordinal-0.demo.svc.cluster.local:6032   \n            example-mysql-cluster-proxysql-proxy-ordinal-0.demo.svc.cluster.local:6033\n            example-mysql-cluster-proxysql-proxy-ordinal-1.demo.svc.cluster.local:6032\n            example-mysql-cluster-proxysql-proxy-ordinal-1.",
    "path": "docs/release-1_0/kubeblocks-for-mysql/03-topologies/02-semisync-with-proxysql",
    "description": " # Deploying a MySQL Semi-Synchronous Cluster and ProxySQL with KubeBlocks  **Semi-synchronous replication** enhances data consistency between the primary and replica nodes by ensuring that the primary waits for acknowledgment from at least one replica before committing transactions.  **ProxySQL** i"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_03-topologies_03-mgr",
    "title": "Deploying a MySQL Group Replication Cluster Using KubeBlocks",
    "content": "\n# Deploying a MySQL Group Replication Cluster Using KubeBlocks\n\n**MySQL Group Replication (MGR)** offers high availability and scalability by synchronizing data across multiple MySQL instances. It ensures that all nodes in the cluster participate in replication seamlessly, with automatic failover and self-healing capabilities. This guide walks you through deploying a MySQL Group Replication cluster using **KubeBlocks**, which simplifies the management and deployment of MySQL clusters in Kubernetes.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploying the MySQL Group Replication Cluster\n\nKubeBlocks uses a declarative approach to manage MySQL clusters. Below is an example configuration for deploying a MySQL Group Replication cluster with three nodes.\n\nApply the following YAML configuration to deploy a MySQL Group Replication (MGR) cluster:\n```yaml\nkubectl apply -f - \n\nTopology:\nCOMPONENT   SERVICE-VERSION   INSTANCE                        ROLE        STATUS    AZ                NODE                                                       CREATED-TIME\nmysql       8.0.35            example-mysql-cluster-mysql-0   primary     Running   ap-southeast-1c   ip-10-0-3-155.ap-southeast-1.compute.internal/10.0.3.155   Feb 10,2025 22:23 UTC+0800\nmysql       8.0.35            example-mysql-cluster-mysql-1   secondary   Running   ap-southeast-1c   ip-10-0-3-204.ap-southeast-1.compute.internal/10.0.3.204   Feb 10,2025 22:23 UTC+0800\nmysql       8.0.35            example-mysql",
    "path": "docs/release-1_0/kubeblocks-for-mysql/03-topologies/03-mgr",
    "description": " # Deploying a MySQL Group Replication Cluster Using KubeBlocks  **MySQL Group Replication (MGR)** offers high availability and scalability by synchronizing data across multiple MySQL instances. It ensures that all nodes in the cluster participate in replication seamlessly, with automatic failover a"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_03-topologies_04-mgr-with-proxysql",
    "title": "Deploying a MySQL Group Replication Cluster with ProxySQL Using KubeBlocks",
    "content": "\n# Deploying a MySQL Group Replication Cluster with ProxySQL Using KubeBlocks\n\n**MySQL Group Replication (MGR)** ensures high availability and fault tolerance by synchronizing data across multiple MySQL instances. It provides automatic failover, promoting a secondary node to primary in case of failure, ensuring continuous availability.\n\n**ProxySQL** is a high-performance MySQL proxy that acts as a middleware between MySQL clients and database servers. It provides features such as query routing, load balancing, query caching, and seamless failover. When combined with MGR, ProxySQL enhances cluster performance and enables efficient traffic management.\n\nThis guide explains how to deploy a **MySQL Group Replication (MGR) cluster with ProxySQL integration** using **KubeBlocks**, simplifying the process of managing MySQL clusters in Kubernetes.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploying the MySQL Group Replication Cluster\n\nKubeBlocks uses a declarative configuration approach to simplify MySQL cluster management. Below is an example configuration to deploy a MySQL Group Replication cluster with three MySQL nodes and two ProxySQL instances.\n\nApply the following YAML configuration:\n```yaml\nkubectl apply -f - \nproxysql    example-mysql-cluster-proxysql-proxy-ordinal-0.demo.svc.cluster.local:6032   \n            example-mysql-cluster-proxysql-proxy-ordinal-0.demo.svc.cluster.local:6033\n            example-mysql-cluster-proxysql-proxy-ordinal-1.demo.svc.cluster.",
    "path": "docs/release-1_0/kubeblocks-for-mysql/03-topologies/04-mgr-with-proxysql",
    "description": " # Deploying a MySQL Group Replication Cluster with ProxySQL Using KubeBlocks  **MySQL Group Replication (MGR)** ensures high availability and fault tolerance by synchronizing data across multiple MySQL instances. It provides automatic failover, promoting a secondary node to primary in case of failu"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_03-topologies_05-orchestrator",
    "title": "Deploying a MySQL Cluster and Orchestrator with KubeBlocks",
    "content": "\n# Deploying a MySQL Cluster and Orchestrator with KubeBlocks\n\nSemi-synchronous replication improves data consistency between primary and replica nodes by requiring acknowledgment from at least one replica before committing transactions.\n\nOrchestrator is a robust MySQL High Availability (HA) and failover management tool. It provides automated monitoring, fault detection, and topology management for MySQL clusters, making it an essential component for managing large-scale MySQL deployments. With Orchestrator, you can:\n- **Monitor Replication Topology**: Orchestrator continuously monitors the MySQL replication topology and provides a real-time view of the cluster's state.\n- **Automated Failover**: In case of a primary node failure, Orchestrator automatically promotes a healthy replica to primary, ensuring minimal downtime.\n- **Topology Management**: Orchestrator allows you to reconfigure, rebalance, and recover your MySQL topology with ease.\n\nThis guide walks you through the process of setting up a MySQL semi-synchronous replication cluster using **KubeBlocks**, alongside **Orchestrator** for effective failover and recovery management.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n  - A Kubernetes cluster is up and running.\n  - The kubectl CLI tool is configured to communicate with your cluster.\n  - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Install the Orchestrator Addon\n\n1. View the Addon versions.\n```bash\n# including pre-release versions\nhelm search repo kubeblocks/orchestrator --devel --versions\n```\n\n2. Install the Addon. Specify a version with '--version'.\n```bash\nhelm install kb-addon-orc kubeblocks/orchestrator --namespace ",
    "path": "docs/release-1_0/kubeblocks-for-mysql/03-topologies/05-orchestrator",
    "description": " # Deploying a MySQL Cluster and Orchestrator with KubeBlocks  Semi-synchronous replication improves data consistency between primary and replica nodes by requiring acknowledgment from at least one replica before committing transactions.  Orchestrator is a robust MySQL High Availability (HA) and fai"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_03-topologies_06-orchestrator-with-proxysql",
    "title": "Deploying a MySQL Cluster with Orchestrator and ProxySQL using KubeBlocks",
    "content": "\n# Deploying a MySQL Cluster and Orchestrator and ProxySQL with KubeBlocks\n\nSemi-synchronous replication enhances data consistency between primary and replica nodes by requiring acknowledgment from at least one replica before committing transactions.\n\nThis guide demonstrates how to deploy a MySQL cluster using **KubeBlocks** with **Orchestrator** for high availability and failover management, and **ProxySQL** for advanced query routing and load balancing. Together, these tools create a robust and efficient MySQL cluster infrastructure.\n\n### **What is Orchestrator?**\n\nOrchestrator is a powerful MySQL High Availability (HA) and failover management tool. It automates monitoring, fault detection, and topology management for MySQL clusters, making it ideal for managing large-scale deployments. Key features include:\n\n- **Replication Topology Monitoring**: Provides a real-time view of the MySQL replication topology.\n- **Automated Failover**: Promotes a healthy replica to primary in case of failure, ensuring minimal downtime.\n- **Topology Management**: Simplifies reconfiguration, rebalancing, and recovery of MySQL clusters.\n\n### **What is ProxySQL?**\n\nProxySQL is a high-performance MySQL proxy that acts as a middleware between MySQL clients and database servers. It enhances cluster performance with features such as:\n\n- **Query Routing**: Directs queries to the appropriate servers based on their purpose (e.g., read or write).\n- **Load Balancing**: Distributes traffic across replicas to optimize resource usage.\n- **Query Caching**: Reduces database load by caching frequent queries.\n- **Failover Support**: Seamlessly handles failover scenarios without interrupting application services.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../",
    "path": "docs/release-1_0/kubeblocks-for-mysql/03-topologies/06-orchestrator-with-proxysql",
    "description": " # Deploying a MySQL Cluster and Orchestrator and ProxySQL with KubeBlocks  Semi-synchronous replication enhances data consistency between primary and replica nodes by requiring acknowledgment from at least one replica before committing transactions.  This guide demonstrates how to deploy a MySQL cl"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_05-backup-restore_01-create-backuprepo",
    "title": "Create a Backup Repository for KubeBlocks",
    "content": "\n# Create a BackupRepo for KubeBlocks\n\nThis guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.\n\n## Prerequisites\n- AWS CLI configured with appropriate permissions to create S3 buckets.\n- kubectl access to your Kubernetes cluster.\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks)) and running in the kb-system namespace.\n\n## Step 1: Create S3 Bucket\n\nUse the AWS CLI to create an S3 bucket in your desired region. Replace `` with your target AWS region (e.g., `us-east-1`, `ap-southeast-1`).\n\n```bash\n aws s3api create-bucket --bucket kubeblocks-backup-repo --region  --create-bucket-configuration LocationConstraint=\n```\n\nExample (for us-west-1):\n```bash\naws s3api create-bucket \\\n  --bucket kubeblocks-backup-repo \\\n  --region us-west-1 \\\n  --create-bucket-configuration LocationConstraint=us-west-1\n```\n\nExample Output:\n\n```json\n\n```\n\nVerification:\nConfirm the bucket was created by listing its contents (it will be empty initially):\n\n```bash\naws s3 ls s3://kubeblocks-backup-repo\n```\n\n## Step 2: Create a Kubernetes Secret for AWS Credentials\n\nStore your AWS credentials securely in a Kubernetes Secret. Replace `` and `` with your actual AWS credentials:\n\n```bash\n# Create a secret to save the access key\nkubectl create secret generic s3-credential-for-backuprepo \\\n  --from-literal=accessKeyId= \\\n  --from-literal=secretAccessKey= \\\n  -n kb-system\n```\n\n## Step 3: Configure Backup Repository\n\nA BackupRepo is a custom resource that defines a storage repository for backups. In this step, you'll integrate your S3 bucket with KubeBlocks by creating a BackupRepo resource.\n\nApply the following YAML to create the BackupRepo. Replace fields(e.g., bucket name, region) with your specific settings.\n\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupRepo\nmetadata:\n  name: s3-repo\n  annotations:\n    # mark this backuprepo as default one\n    dataprotection.kubeblocks.io/is",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/05-backup-restore/01-create-backuprepo",
    "description": " # Create a BackupRepo for KubeBlocks  This guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.  ## Prerequisites - AWS CLI configured with appropriate permissions to create S3 buckets. - kubectl access to your Kubernetes cluster. -"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_05-backup-restore_02-create-full-backup",
    "title": "Create a Full Backup for a MongoDB Cluster on KubeBlocks",
    "content": "\n\n\n# Create a Full Backup for MongoDB on KubeBlocks\n\nThis guide demonstrates how to create and validate full backups for MongoDB clusters on KubeBlocks using the `pg-basebackup` method through both:\n- The Backup API (direct backup operations)\n- The OpsRequest API (managed backup operations with enhanced monitoring)\n\nWe will cover how to restore data from a backup in the [Restore From Full Backup](./05-restoring-from-full-backup) guide.\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Backup Prerequisites\n\nBefore creating backups, ensure:\n1. Backup repository is configured:\n   - `BackupRepo` resource exists\n   - Network connectivity between cluster and repository\n   - `BackupRepo` status shows \"Ready\"\n\n2. Cluster is ready:\n   - Cluster status is \"Running\"\n   - No ongoing operations (scaling, upgrades, etc.)\n\n## Identify Backup Configuration\n\nCheck available backup policies and schedules:\n\n```bash\n# List backup policies\nkubectl get backuppolicy -n demo -l app.kubernetes.io/instance=mongo-cluster\n\n# List backup schedules\nkubectl get backupschedule -n demo -l app.kubernetes.io/instance=mongo-cluster\n```\n\nExpected Output:\n```bash\nNAME                                  BACKUP-REPO   STATUS      AGE\nmongo-cluster-mongodb-backup-policy                 Available   62m\n\nNAME                                    STATUS      AGE\nmongo-cluster-mongodb-backup-schedule   Available   62m\n```\n\nView supported backup methods in the BackupPolicy CR 'mongo-cluster-mongodb-backup-policy':\n\n```bash\nkubectl get backuppolicy mongo-cluster-mongodb-backup-policy -n demo -oyaml | yq '.spec.backupMethods[].name'\n```\n**List of Backup methods**\n\nKubeBlocks MongoDB supports these backup methods:\n\n| Feature           | Method          | Description |\n|-------------|--------|------------|\n| Full Backup | dump   | Uses `mongodump`, a MongoDB utility used to create a binary export of the contents of a database  |\n| Full Backup | datafile | Backup the data files of the",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/05-backup-restore/02-create-full-backup",
    "description": "   # Create a Full Backup for MongoDB on KubeBlocks  This guide demonstrates how to create and validate full backups for MongoDB clusters on KubeBlocks using the `pg-basebackup` method through both: - The Backup API (direct backup operations) - The OpsRequest API (managed backup operations with enha"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_05-backup-restore_03-scheduled-full-backup",
    "title": "Setting Up a MongoDB Cluster with Scheduled Backups in KubeBlocks",
    "content": "\n\n# Setting Up a MongoDB Cluster with Scheduled Backups in KubeBlocks\n\nThis guide demonstrates how to deploy a MongoDB cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.\n\n## Prerequisites\n\n\n\n## Deploy a MongoDB Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Prerequisites for Backup\n\n1. Backup Repository Configured:\n   - Configured `BackupRepo`\n   - Network connectivity between cluster and repo, `BackupRepo` status is `Ready`\n\n2. Cluster is Running:\n   - Cluster must be in `Running` state\n   - No ongoing operations (scaling, upgrades etc.)\n\n## Configure Scheduled Backups\n\nKubeBlocks automatically creates a `BackupSchedule` resource when the cluster is created. Follow these steps to enable and configure scheduled backups:\n\n1. Verify the default backup schedule configuration:\n\n```bash\nkubectl get backupschedule mongo-cluster-mongodb-backup-schedule  -n demo -oyaml\n```\n\nExample Output:\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupSchedule\nspec:\n  backupPolicyName: mongo-cluster-MongoDB-backup-policy\n  schedules:\n  - backupMethod: datafile\n    # ┌───────────── minute (0-59)\n    # │ ┌───────────── hour (0-23)\n    # │ │ ┌───────────── day of month (1-31)\n    # │ │ │ ┌───────────── month (1-12)\n    # │ │ │ │ ┌───────────── day of week (0-6) (Sunday=0)\n    # │ │ │ │ │\n    # 0 18 * * *\n    # schedule this job every day at 6:00 PM (18:00).\n    cronExpression: 0 18 * * * # update the cronExpression to your need\n    enabled: true # set to `true` to schedule base backup periodically\n    retentionPeriod: 7d # set the retention period to your need\n```\n\n2. Enable and customize the backup schedule:\n```bash\nkubectl edit backupschedule mongo-cluster-mongodb-backup-schedule -n demo\n```\n\nUpdate these key parameters:\n- `enabled`: Set to `true` to activate scheduled backups\n- `cronExpression`: Configure backup frequency using cron syntax\n- `retentionPeriod`: Set how long to keep backups (e.g., `7d`, `1mo`)\n\nExample configurat",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/05-backup-restore/03-scheduled-full-backup",
    "description": "  # Setting Up a MongoDB Cluster with Scheduled Backups in KubeBlocks  This guide demonstrates how to deploy a MongoDB cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.  ## Prerequisites    ## Deploy a MongoDB Cluster    ## Verifying the Deployment    ## Pr"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_05-backup-restore_04-scheduled-continuous-backup",
    "title": "Setting Up a MongoDB Cluster with Scheduled Continuous Backup in KubeBlocks",
    "content": "\n# Setting Up a MongoDB Cluster with Scheduled Continuous Backup Enabled in KubeBlocks\n\nThis guide demonstrates how to configure a MongoDB cluster on KubeBlocks with:\n\n- Scheduled full backups (base backups)\n- Continuous WAL (Write-Ahead Log) archiving\n- Point-In-Time Recovery (PITR) capabilities\n\nThis combination provides comprehensive data protection with minimal recovery point objectives (RPO).\n\n## What is PITR?\nPoint-In-Time Recovery (PITR) allows you to restore a database to a specific moment in time by combining full backups with continuous binlog/wal/archive log backups.\n\nFor details on restoring data from both full backups and continuous binlog backups, refer to the [Restore From PITR](restore-with-pitr.mdx)  guide.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Prerequisites for Backup\n\n1. Backup Repository Configured:\n   - Configured `BackupRepo`\n   - Network connectivity between cluster and repo, `BackupRepo` status is `Ready`\n\n2. Cluster is Running:\n   - Cluster must be in `Running` state\n   - No ongoing operations (scaling, upgrades etc.)\n\n## List of Backup methods\n\nKubeBlocks MongoDB supports these backup methods:\n\n| Feature           | Method          | Description |\n|-------------|--------|------------|\n| Full Backup | dump   | Uses `mongodump`, a MongoDB utility used to create a binary export of the contents of a database  |\n| Full Backup | datafile | Backup the data files of the database |\n| Continuous Backup | archive-oplog | Continuously archi",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/05-backup-restore/04-scheduled-continuous-backup",
    "description": " # Setting Up a MongoDB Cluster with Scheduled Continuous Backup Enabled in KubeBlocks  This guide demonstrates how to configure a MongoDB cluster on KubeBlocks with:  - Scheduled full backups (base backups) - Continuous WAL (Write-Ahead Log) archiving - Point-In-Time Recovery (PITR) capabilities  T"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_05-backup-restore_05-restoring-from-full-backup",
    "title": "Restore a MongoDB Cluster from Backup",
    "content": "\n\n# Restore a MongoDB Cluster from Backup\n\nThis guide demonstrates two methods to restore a MongoDB cluster from backup in KubeBlocks:\n\n1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations\n2. **OpsRequest API Method** - Enhanced operational control with progress monitoring\n\n## Prerequisites\n\n\n\n## Preparing for Restoration: Locate one Full Backup\nBefore restoring, ensure that there is a full backup available. The restoration process will use this backup to create a new MongoDB cluster.\n\n- Backup repository accessible from new cluster\n- Valid full backup in `Completed` state\n- Adequate CPU/memory resources\n- Sufficient storage capacity\n\nFind available full backups:\n\n```bash\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Full,app.kubernetes.io/instance=mongo-cluster # get the list of full backups\n```\n\nPick ONE of the Backups whose status is `Completed`.\n\n## Option 1: Cluster Annotation Restoration\n\n### Step 1: Create Restored Cluster\nCreate a new cluster with restore configuration:\n\nKey parameters:\n- `kubeblocks.io/restore-from-backup` annotation\n- Backup name and namespace located from the previous steps\n\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: mongo-cluster-restored\n  namespace: demo\n  annotations:\n    # NOTE: replace  with your backup\n    kubeblocks.io/restore-from-backup: '}'\nspec:\n  terminationPolicy: Delete\n  clusterDef: mongodb\n  topology: replicaset\n  componentSpecs:\n    - name: mongodb\n      serviceVersion: \"6.0.16\"\n      replicas: 3\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n\n### Step 2: Monitor Restoration\nTrack restore progress with:\n\n```bas",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/05-backup-restore/05-restoring-from-full-backup",
    "description": "  # Restore a MongoDB Cluster from Backup  This guide demonstrates two methods to restore a MongoDB cluster from backup in KubeBlocks:  1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations 2. **OpsRequest API Method** - Enhanced operational control with progress moni"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mongodb_05-backup-restore_06-restore-with-pitr",
    "title": "Restore a MongoDB Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks",
    "content": "\n\n# Restore a MongoDB Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks\n\nThis guide demonstrates how to perform Point-In-Time Recovery (PITR) for MongoDB clusters in KubeBlocks using:\n\n1. A full base backup\n2. Continuous WAL (Write-Ahead Log) backups\n3. Two restoration methods:\n   - Cluster Annotation (declarative approach)\n   - OpsRequest API (operational control)\n\nPITR enables recovery to any moment within the `timeRange` specified.\n\n## Prerequisites\n\n\n\n## Prepare for PITR Restoration\nTo perform a PITR restoration, both a full backup and continuous backup are required. Refer to the documentation to configure these backups if they are not already set up.\n\n- Completed full backup\n- Active continuous WAL backup\n- Backup repository accessible\n- Sufficient resources for new cluster\n\nTo identify the list of full and continuous backups, you may follow the steps:\n\n### 1. Verify Continuous Backup\nConfirm you have a continuous WAL backup, either running or completed:\n\n```bash\n# expect EXACTLY ONE continuous backup per cluster\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Continuous,app.kubernetes.io/instance=mongo-cluster\n```\n\n### 2. Check Backup Time Range\nGet the valid recovery window:\n\n```bash\nkubectl get backup  -n demo -o yaml | yq '.status.timeRange'\n```\n\nExpected Output:\n```text\nstart: \"2025-05-07T09:12:47Z\"\nend: \"2025-05-07T09:22:50Z\"\n```\n\n### 3. Identify Full Backup\nFind available full backups that meet:\n- Status: Completed\n- Completion time after continuous backup start time\n\n```bash\n# expect one or more Full backups\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Full,app.kubernetes.io/instance=mongo-cluster\n```\n\n:::tip\nKubeBlocks automatically selects the most recent qualifying full backup as the base.\nMake sure there is a full backup meets the condition: its `stopTime`/`completionTimestamp` must **AFTER** Continuous backup's `startTime`, otherwise PITR restoration will fail.\n:::\n\n## Option 1: Clu",
    "path": "docs/release-1_0/kubeblocks-for-mongodb/05-backup-restore/06-restore-with-pitr",
    "description": "  # Restore a MongoDB Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks  This guide demonstrates how to perform Point-In-Time Recovery (PITR) for MongoDB clusters in KubeBlocks using:  1. A full base backup 2. Continuous WAL (Write-Ahead Log) backups 3. Two restoration methods:    "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_04-operations_01-stop_start_restart",
    "title": "Managing MySQL Cluster Lifecycle (Stop, Start, and Restart)",
    "content": "\n\n\n# Managing MySQL Cluster Lifecycle\n\nThis guide demonstrates how to manage the lifecycle of a MySQL cluster in **KubeBlocks**, including stopping, starting, and restarting the cluster. Proper lifecycle management helps optimize resource usage, reduce operational costs, and ensure flexibility in your Kubernetes environment.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\nkubectl apply -f - \n\n\nOption 2: Using the Declarative Cluster API\n\nAlternatively, you may stop the cluster by setting the `spec.componentSpecs.stop` field to `true`  in the cluster configuration:\n\n```bash\nkubectl patch cluster example-mysql-cluster -n demo --type='json' -p='[\n  \n]'\n```\n\n    \n\n    \n\n### Verifying Cluster Stop\nMonitor the cluster's status to ensure it transitions to the Stopped state:\n```bash\nkubectl get cluster -n demo -w\n```\nExample Output:\n```bash\nNAME                       CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nexample-mysql-cluster   mysql                Delete               Stopping   93s\nexample-mysql-cluster   mysql                Delete               Stopped    101s\n```\n\nThere is no Pods running in the cluster, but the persistent storage is retained.\n```bash\nkubectl get pods -n demo\n```\nExpecte",
    "path": "docs/release-1_0/kubeblocks-for-mysql/04-operations/01-stop_start_restart",
    "description": "   # Managing MySQL Cluster Lifecycle  This guide demonstrates how to manage the lifecycle of a MySQL cluster in **KubeBlocks**, including stopping, starting, and restarting the cluster. Proper lifecycle management helps optimize resource usage, reduce operational costs, and ensure flexibility in yo"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a MySQL Cluster",
    "content": "\n\n\n\n# Vertical Scaling in a MySQL Cluster\n\nThis guide explains how to perform **vertical scaling** in a MySQL cluster managed by KubeBlocks.\nVertical scaling adjusts the resource limits and requests (such as CPU and memory) allocated to the cluster components, allowing for better performance or resource optimization.\n\n## What is Vertical Scaling?\nVertical scaling involves increasing or decreasing the resources (e.g., CPU and memory) allocated to a running database cluster.\nUnlike horizontal scaling, which adjusts the number of replicas, vertical scaling focuses on scaling the capacity of individual Pods.\n\nResources that can be scaled include:\n- CPU cores: Processing power for the database.\n- Memory (RAM): Memory available for database operations.\n\nKubeBlocks ensures seamless vertical scaling by carefully orchestrating Pod restarts to minimize downtime. For example:\n- Secondary Pods are recreated first.\n- Primary Pods are updated last to maintain cluster availability.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node semi-sync MySQL cluster (1 primary, 1 secondary):\n\n```yaml\nkubectl apply -f - \n\nOption 2: Direct Cluster API Update\n\nAlternatively, you may update `spec.componentSpecs.resources` field to the desired resources for vertical scale.\n\n```yaml\nkubectl apply -f - \n\n    \n\n## Verification\nVerify the updated resources by inspecting the cluster configuration or Pod details:\n```bash\nkbcli cluster describe exa",
    "path": "docs/release-1_0/kubeblocks-for-mysql/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling in a MySQL Cluster  This guide explains how to perform **vertical scaling** in a MySQL cluster managed by KubeBlocks. Vertical scaling adjusts the resource limits and requests (such as CPU and memory) allocated to the cluster components, allowing for better performance or reso"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of MySQL Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for MySQL Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a MySQL cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node MySQL cluster (1 primary, 1 replica) with semi-synchronous replication:\n\n```yaml\nkubectl apply -f - \n\n\n### Option 2.: Direct Cluster API Update\n\nAlternatively, you can perform a direct update to the `replicas` field in the Cluster resource:\n```yaml\nkubectl patch cluster example-mysql-cluster -n demo --type=json -p='[]'\n```\n\n    \n\n    \n\n\n### Verify Scale-Out\n\nAfter applying the operation, you will see a new pod created and the MySQL cluster status goes from `Updating` to `Running`, and the newly created pod has a new role `secondary`.\n\n```bash\nkubectl get pods -n demo -l app.kubernetes.io/instance=example-mysql-cluster\n```\n\nExample Output (3 Pods):\n```bash\nNAME                           READY   STATUS    RESTARTS   AGE\nexample-mysql-cluster-mysql-0   4/4     Running   0          4m30s\nexample-mysql-cluster-mysql-1   4/4     Running   0          4m30s\nexample-mysql-cluster-mysql-2   4/4     Running   0          49s\n```\n\nNew replicas automatically join as secondary nodes.\n```bash\nkubectl get pods -n demo -l app.kubernetes.io/instance=example-mysql-cluster -o jsonpath=''\n```\nExample Output:\n`",
    "path": "docs/release-1_0/kubeblocks-for-mysql/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for MySQL Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a MySQL cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequisites  Bef"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a MySQL Cluster",
    "content": "\n\n\n\n# Expanding Volume in a MySQL Cluster\n\nThis guide explains how to expand the Persistent Volume Claims (PVCs) in a MySQL cluster managed by **KubeBlocks**. Volume expansion allows you to increase storage capacity dynamically, ensuring your database can scale seamlessly as data grows. If supported by the underlying storage class, this operation can be performed without downtime.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node semi-sync MySQL cluster (1 primary, 1 secondary):\n\n```yaml\nkubectl apply -f - \n\n\nOption 2: Direct Cluster API Update\n\nAlternatively, you may update the `spec.componentSpecs.volumeClaimTemplates.spec.resources.requests.storage` field to the desired size.\n\n```yaml\nkubectl apply -f - \n\n    \n\n## Verification\n\nUse the following command to inspect the updated cluster configuration:\n```bash\nkbcli cluster describe example-mysql-cluster -n demo\n```\nExpected Output:\n```bash\nResources Allocation:\nCOMPONENT   INSTANCE-TEMPLATE   CPU(REQUEST/LIMIT)   MEMORY(REQUEST/LIMIT)   STORAGE-SIZE   STORAGE-CLASS\nmysql                           500m / 500m          512Mi / 512Mi           data:30Gi      \n```\nThe volume size for the data PVC has been updated to the specified value (e.g., 30Gi in this case).\n\nCheck the status of the PVCs in the cluster to confirm that the resize operation has completed:\n```bash\nkubectl get pvc -l app.kubernetes.io/instance=example-mysql-cluster -n demo\n```\nExpected Output:\n```",
    "path": "docs/release-1_0/kubeblocks-for-mysql/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a MySQL Cluster  This guide explains how to expand the Persistent Volume Claims (PVCs) in a MySQL cluster managed by **KubeBlocks**. Volume expansion allows you to increase storage capacity dynamically, ensuring your database can scale seamlessly as data grows. If supported"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy MySQL Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage MySQL Service Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions on how to expose a MySQL service managed by KubeBlocks, either externally or internally. You will learn how to configure external access using a cloud provider's LoadBalancer service, manage internal services, and correctly disable external exposure when no longer needed.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode.\n\nCluster Configuration\n```yaml\nkubectl apply -f -                                                                                3306/TCP                                                5m16s\nexample-mysql-cluster-mysql-headless   ClusterIP      None                                                                                            3306/TCP,3601/TCP,9104/TCP,3501/TCP,3502/TCP,9901/TCP   5m16s\n```\n\n## Expose MySQL Service Externally or Internally\n\nExternal addresses allow public internet access to the MySQL service, while internal addresses restrict access to the user’s VPC.\n\n### Service Types Comparison\n\n| Type | \tUse Case\t| Cloud Cost |\tSecurity |\n|----|---|----|---|\n| ClusterIP |\tInternal service communication |\tFree |\tHighest|\n| NodePort |\tDevelopment/testing\t| Low |\tModerate |\n| LoadBalancer\t| Production external access |\tHigh\t| ",
    "path": "docs/release-1_0/kubeblocks-for-mysql/04-operations/05-manage-loadbalancer",
    "description": "    # Manage MySQL Service Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions on how to expose a MySQL service managed by KubeBlocks, either externally or internally. You will learn how to configure external access using a cloud provider's LoadBalancer ser"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_04-operations_06-minior-version-upgrade",
    "title": "Upgrading the Minor Version of a MySQL Cluster in KubeBlocks",
    "content": "\n# Upgrading the Minor Version of a MySQL Cluster in KubeBlocks\n\nThis guide walks you through the deployment and minor version upgrade of a MySQL cluster managed by KubeBlocks, ensuring minimal downtime during the process.\n\nTo minimize the impact on database availability, the upgrade process starts with the replicas (secondary instances). Once the replicas are upgraded, a switchover operation promotes one of the upgraded replicas to primary. The switchover process is very fast, typically completing in a few hundred milliseconds. After the switchover, the original primary instance is upgraded, ensuring minimal disruption to the application.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node semi-sync MySQL cluster (1 primary, 1 secondary):\n\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: Delete\n  componentSpecs:\n    - name: mysql\n      serviceVersion: 8.0.35\n      replicas: 2\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\nEOF\n```\n\n## Ver",
    "path": "docs/release-1_0/kubeblocks-for-mysql/04-operations/06-minior-version-upgrade",
    "description": " # Upgrading the Minor Version of a MySQL Cluster in KubeBlocks  This guide walks you through the deployment and minor version upgrade of a MySQL cluster managed by KubeBlocks, ensuring minimal downtime during the process.  To minimize the impact on database availability, the upgrade process starts "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_04-operations_07-modify-parameters",
    "title": "Modify MySQL Parameters",
    "content": "\n# Modify MySQL Parameters\n\nReconfiguring a database involves modifying database parameters, settings, or configurations to improve performance, security, or availability. These changes can be categorized as:\n- Dynamic: Changes applied without requiring a database restart.\n- Static: Changes that require a database restart to take effect.\n\nEven for static parameters, **KubeBlocks** ensures minimal downtime. It modifies and restarts the replica nodes first, then performs a **switchover** to promote the updated replica as the primary node (a process typically completed within a few milliseconds). Finally, it restarts the original primary node.\n\nThis guide demonstrates how to modify both dynamic and static parameters of a MySQL cluster managed by KubeBlocks using a Reconfiguring OpsRequest.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploying the MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode.\n\nDeploy the cluster using the following YAML manifest:\n\n```yaml\nkubectl apply -f -  SHOW VARIABLES LIKE 'max_connections';\n+-----------------+-------+\n| Variable_name   | Value |\n+-----------------+-------+\n| max_connections | 83    |\n+-----------------+-------+\n1 row in set (0.00 sec)\n\nmysql> SHOW VARIABLES LIKE 'performance_schema';\n+--------------------+-------+\n| Variable_name      | Value |\n+-",
    "path": "docs/release-1_0/kubeblocks-for-mysql/04-operations/07-modify-parameters",
    "description": " # Modify MySQL Parameters  Reconfiguring a database involves modifying database parameters, settings, or configurations to improve performance, security, or availability. These changes can be categorized as: - Dynamic: Changes applied without requiring a database restart. - Static: Changes that req"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_04-operations_08-switchover",
    "title": "Planned Switchover in a MySQL Cluster",
    "content": "\n# Planned Switchover in a MySQL Cluster\n\nA **switchover** is a planned operation where the primary instance in a MySQL cluster proactively transfers its role to a secondary instance. Unlike an unplanned failover, which occurs during unexpected failures, a switchover ensures a controlled and predictable role transition with minimal service disruption.\n\n## **Benefits of Switchover**\n1. **Minimal Downtime**: The primary instance actively transfers its role to the secondary instance, resulting in very short service downtime (typically a few hundred milliseconds)\n2. **Controlled Transition**: Ensures a seamless and predictable role change compared to failover, which involves detecting and recovering from a failure, often causing longer delays (several seconds or more).\n3. **Maintenance-Friendly**: Ideal for planned maintenance tasks, such as node upgrades or decommissioning, while ensuring uninterrupted service.\n\n## **Switchover vs. Failover**\n\n| **Aspect**                  | **Switchover**                            | **Failover**                         |\n|-----------------------------|-------------------------------------------|---------------------------------------|\n| **Initiation**              | Planned and manually triggered            | Unplanned and automatically triggered|\n| **Downtime**                | Few hundred milliseconds                  | Several seconds or more               |\n| **Primary Role Transition** | Proactively transferred                   | Reactively promoted                   |\n| **Use Case**                | Planned maintenance (e.g., upgrades)      | Handling unexpected failures          |\n\nUsing a switchover ensures smooth transitions and minimal service disruption, making it the preferred choice for planned maintenance activities.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n  - A Kubernetes cluster is up and running.\n  - The kubectl CLI tool is configured to communicate with your cluster.\n  - [K",
    "path": "docs/release-1_0/kubeblocks-for-mysql/04-operations/08-switchover",
    "description": " # Planned Switchover in a MySQL Cluster  A **switchover** is a planned operation where the primary instance in a MySQL cluster proactively transfers its role to a secondary instance. Unlike an unplanned failover, which occurs during unexpected failures, a switchover ensures a controlled and predict"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed MySQL Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in a KubeBlocks-Managed MySQL Clusters\n\nThis guide explains how to decommission (take offline) a specific Pod in a MySQL cluster managed by KubeBlocks. Decommissioning a Pod allows precise control over cluster resources without disrupting the cluster's overall functionality. This is particularly useful for workload rebalancing, node maintenance, or addressing specific failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 3-node MySQL semi-synchronous cluster (1 primary, 2 replicas):\n\n```yaml\nkubectl apply -f - \n\n\n### Option 2.: Using Cluster API\nAlternatively, update the Cluster resource directly to decommission the Pod:\n\n```yaml\nkubectl apply -f - \n\n    \n\n### Verify the D",
    "path": "docs/release-1_0/kubeblocks-for-mysql/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in a KubeBlocks-Managed MySQL Clusters  This guide explains how to decommission (take offline) a specific Pod in a MySQL cluster managed by KubeBlocks. Decommissioning a Pod allows precise control over cluster resources without disrupting the cluster's overall funct"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_04-operations_11-rebuild-replica",
    "title": "Recovering MySQL Replica in KubeBlocks",
    "content": "\n# Recovering MySQL Replica in KubeBlocks\n\nThis guide demonstrates how to perform the following tasks in a MySQL semi-synchronous cluster managed by KubeBlocks:\n- Write a record to the primary instance and verify replication on the replica.\n- Stop HA, break replication, modify data on the replica, and remove replication.\n- Rebuild the replica using both 'in-place' repair and 'non-in-place' repair methods.\n- Verify data recovery on the replica.\n\n> **Note**: Above steps are intended for testing purpose only. Disabling HA, breaking replication, and modifying data on a replica can compromise database consistency. Do not perform these operations on a production database.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode.\n\nCluster Configuration\n```yaml\nkubectl apply -f -  CREATE DATABASE test;\nmysql> USE test;\nmysql> CREATE TABLE t1 (id INT PRIMARY KEY, name VARCHAR(255));\nmysql> INSERT INTO t1 VALUES (1, 'John Doe');\n```\n\n### Step 3: Verify Data Replication\nConnect to the replica instance (example-mysql-cluster-mysql-0) to verify that the data has been replicated:\n```bash\nkubectl exec -ti -n demo example-mysql-cluster-mysql-0 -- mysql  -uroot -pR0z5Z1DS02\n```\nNote: If the primary instance is 'example-mysql-cluster-mysql-0', you should conne",
    "path": "docs/release-1_0/kubeblocks-for-mysql/04-operations/11-rebuild-replica",
    "description": " # Recovering MySQL Replica in KubeBlocks  This guide demonstrates how to perform the following tasks in a MySQL semi-synchronous cluster managed by KubeBlocks: - Write a record to the primary instance and verify replication on the replica. - Stop HA, break replication, modify data on the replica, a"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_06-custom-secret_01-custom-secret",
    "title": "Create a MySQL Cluster with a Custom Root Password on KubeBlocks",
    "content": "\n# Create MySQL Cluster With Custom Password on KubeBlocks\n\nThis guide demonstrates how to deploy a MySQL cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\n```\nExpected Output:\n```bash\nnamespace/demo created\n```\n\n## Deploying the MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode and a custom root password.\n\n### Step 1: Create a Secret for the Root Account\n\nThe custom root password is stored in a Kubernetes Secret. Create the Secret by applying the following YAML:\n\n```yaml\nkubectl apply -f - <<EOF\napiVersion: v1\ndata:\n  password: Y3VzdG9tcGFzc3dvcmQ= # custompassword\n  username: cm9vdA== #root\nimmutable: true\nkind: Secret\nmetadata:\n  name: custom-mysql-root-secret\n  namespace: demo\nEOF\n```\n- password: Replace custompassword with your desired password and encode it using Base64 (`echo -n \"custompassword\" | base64`).\n- username: The default MySQL root user is 'root', encoded as 'cm9vdA=='.\n\n\n### Step 2: Deploy the MySQL Cluster\n\nApply the following manifest to deploy the MySQL cluster, referencing the Secret created in Step 1 for the root account:\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPoli",
    "path": "docs/release-1_0/kubeblocks-for-mysql/06-custom-secret/01-custom-secret",
    "description": " # Create MySQL Cluster With Custom Password on KubeBlocks  This guide demonstrates how to deploy a MySQL cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.  ## Prerequisites  Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_06-custom-secret_02-custom-password-generation-policy",
    "title": "Deploy a MySQL Cluster with a Custom Password Generation Policy on KubeBlocks",
    "content": "\n# Create a MySQL Cluster With Custom Password Generation Policy on KubeBlocks\nThis guide explains how to deploy a MySQL cluster in KubeBlocks with a custom password generation policy for the root user. By defining specific password rules, you can ensure strong, secure credentials for your cluster.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n  - A Kubernetes cluster is up and running.\n  - The kubectl CLI tool is configured to communicate with your cluster.\n  - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploying the MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode and a custom root password that adheres to a specific pattern.\n\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: Delete\n  componentSpecs:\n    - name: mysql\n      serviceVersion: 8.0.35\n      replicas: 2\n      systemAccounts:\n        - name: root\n          passwordConfig:\n            length: 20           # Password length: 20 characters\n            numDigits: 4         # At least 4 digits\n            numSymbols: 4        # At least 4 symbols\n            letterCase: MixedCases # Uppercase and lowercase letters\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassNam",
    "path": "docs/release-1_0/kubeblocks-for-mysql/06-custom-secret/02-custom-password-generation-policy",
    "description": " # Create a MySQL Cluster With Custom Password Generation Policy on KubeBlocks This guide explains how to deploy a MySQL cluster in KubeBlocks with a custom password generation policy for the root user. By defining specific password rules, you can ensure strong, secure credentials for your cluster. "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_05-backup-restore_01-create-backuprepo",
    "title": "Create a Backup Repository for KubeBlocks",
    "content": "\n# Create a BackupRepo for KubeBlocks\n\nThis guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.\n\n## Prerequisites\n- AWS CLI configured with appropriate permissions to create S3 buckets.\n- kubectl access to your Kubernetes cluster.\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks)) and running.\n\n## Step 1: Create S3 Bucket\n\nUse the AWS CLI to create an S3 bucket in your desired region. Replace `` with your target AWS region (e.g., `us-east-1`, `ap-southeast-1`).\n\n```bash\naws s3api create-bucket --bucket kubeblocks-backup-repo --region  --create-bucket-configuration LocationConstraint=\n```\n\nExample (for us-west-1):\n```bash\naws s3api create-bucket \\\n  --bucket kubeblocks-backup-repo \\\n  --region us-west-1 \\\n  --create-bucket-configuration LocationConstraint=us-west-1\n```\n\nExample Output:\n\n```json\n\n```\n\nVerification:\nConfirm the bucket was created by listing its contents (it will be empty initially):\n\n```bash\naws s3 ls s3://kubeblocks-backup-repo\n```\n\n## Step 2: Create a Kubernetes Secret for AWS Credentials\n\nStore your AWS credentials securely in a Kubernetes Secret. Replace `` and `` with your actual AWS credentials:\n\n```bash\n# Create a secret to save the access key\nkubectl create secret generic s3-credential-for-backuprepo \\\n  --from-literal=accessKeyId= \\\n  --from-literal=secretAccessKey= \\\n  -n kb-system\n```\n\n## Step 3: Configure Backup Repository\n\nA BackupRepo is a custom resource that defines a storage repository for backups. In this step, you'll integrate your S3 bucket with KubeBlocks by creating a BackupRepo resource.\n\nApply the following YAML to create the BackupRepo. Replace fields(e.g., bucket name, region) with your specific settings.\n\n```yaml\nkubectl apply -f - <<EOF\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupRepo\nmetadata:\n  name: s3-repo\n  annotations:\n    dataprotection.kubeblocks.io/is-default-repo: 'true'\nspec:\n  # Currently, Ku",
    "path": "docs/release-1_0/kubeblocks-for-mysql/05-backup-restore/01-create-backuprepo",
    "description": " # Create a BackupRepo for KubeBlocks  This guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.  ## Prerequisites - AWS CLI configured with appropriate permissions to create S3 buckets. - kubectl access to your Kubernetes cluster. -"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_05-backup-restore_02-create-full-backup",
    "title": "Create a Full Backup for a MySQL Cluster on KubeBlocks",
    "content": "\n\n\n# Create a Full Backup for a MySQL Cluster on KubeBlocks\n\nThis guide demonstrates how to create and validate full backups for a MySQL cluster deployed on KubeBlocks using XtraBackup through both Backup API and Ops API.\nThe Ops API is essentially a wrapper around the Backup API, offering enhanced control and progress monitoring capabilities for backup operations.\n\nWe will cover how to restore data from a backup in the [Restore From Full Backup](restoring-from-full-backup.md) guide.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode.\n\nCluster Configuration\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: WipeOut\n  componentSpecs:\n    - name: mysql\n      serviceVersion: 8.0.35\n      replicas: 2\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\nEOF\n```\n**Key Notes:**\n",
    "path": "docs/release-1_0/kubeblocks-for-mysql/05-backup-restore/02-create-full-backup",
    "description": "   # Create a Full Backup for a MySQL Cluster on KubeBlocks  This guide demonstrates how to create and validate full backups for a MySQL cluster deployed on KubeBlocks using XtraBackup through both Backup API and Ops API. The Ops API is essentially a wrapper around the Backup API, offering enhanced "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_05-backup-restore_03-scheduled-full-backup",
    "title": "Setting Up a MySQL Cluster with Scheduled Backups in KubeBlocks",
    "content": "\n\n# Setting Up a MySQL Cluster with Scheduled Backups in KubeBlocks\n\nThis guide demonstrates how to deploy a MySQL cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode and scheduled backups.\n\nCluster Configuration\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: WipeOut\n  componentSpecs:\n    - name: mysql\n      serviceVersion: 8.0.35\n      replicas: 2\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n  backup:\n    enabled: true\n    retentionPeriod: 30d\n    method: xtrabackup\n    cronExpression: '0 0 * * *'\n    repoName: s3-repo\nEOF\n```\n\n**Explanation of Key Fields**\n- `terminationPolicy: WipeOut`:\n  - When set to 'WipeOut', deleting the cluster also deletes all associated data,",
    "path": "docs/release-1_0/kubeblocks-for-mysql/05-backup-restore/03-scheduled-full-backup",
    "description": "  # Setting Up a MySQL Cluster with Scheduled Backups in KubeBlocks  This guide demonstrates how to deploy a MySQL cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.  ## Prerequisites  Before proceeding, ensure the following: - Environment Setup:     - A Kub"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_05-backup-restore_04-scheduled-continuous-backup",
    "title": "Setting Up a MySQL Cluster with Scheduled Continuous Backup in KubeBlocks",
    "content": "\n# Setting Up a MySQL Cluster with Scheduled Continuous Backup Enabled in KubeBlocks\n\nThis guide explains how to deploy a MySQL cluster on KubeBlocks with scheduled full backups and continuous binlog backups, enabling Point-In-Time Recovery (PITR) for enhanced data protection and recovery capabilities.\n\n## What is PITR?\nPoint-In-Time Recovery (PITR) allows you to restore a database to a specific moment in time by combining full backups with incremental binlog backups.\n\nFor details on restoring data from both full backups and continuous binlog backups, refer to the [Restore From PITR](restore-with-pitr.mdx)  guide.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode, along with scheduled backups (both full backup and continuous backup).\n\nCluster Configuration\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: WipeOut\n  componentSpecs:\n    - name: mysql\n      serviceVersion: 8.0.35\n      replicas: 2\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n",
    "path": "docs/release-1_0/kubeblocks-for-mysql/05-backup-restore/04-scheduled-continuous-backup",
    "description": " # Setting Up a MySQL Cluster with Scheduled Continuous Backup Enabled in KubeBlocks  This guide explains how to deploy a MySQL cluster on KubeBlocks with scheduled full backups and continuous binlog backups, enabling Point-In-Time Recovery (PITR) for enhanced data protection and recovery capabiliti"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_05-backup-restore_05-restoring-from-full-backup",
    "title": "Restore a MySQL Cluster from Backup",
    "content": "\n\n# Restore a MySQL Cluster from Backup\n\nThis guide explains how to restore a new MySQL cluster from an existing backup in KubeBlocks. It showcases two methods: using **Cluster Annotation** or the **Ops API**.\n\nThe **Ops API** provides enhanced control and progress monitoring capabilities for restore operations, acting as a wrapper around the Cluster Annotation.\n\n\n## Prerequisites\n- KubeBlocks Environment:\n  - KubeBlocks operator and required CRDs installed.\n  - kubectl configured to access your Kubernetes cluster.\n- Existing Backup:\n  - A valid backup named example-mysql-backup-backup in the 'demo' namespace.\n\n## Verify Backup Status\n\nBefore restoring, ensure that a full backup is available. The restoration process will use this backup to create a new MySQL cluster.\n\nRun the following command to check the backup status:\n```bash\nkubectl get backup -n demo\n```\n\nExpected Output:\n```bash\nNAME                           POLICY                                      METHOD       REPO      STATUS      TOTAL-SIZE   DURATION   DELETION-POLICY   CREATION-TIME          COMPLETION-TIME        EXPIRATION-TIME\nexample-mysql-cluster-backup   example-mysql-cluster-mysql-backup-policy   xtrabackup   s3-repo   Completed   1633717      18s        Delete            2025-03-07T03:25:22Z   2025-03-07T03:25:40Z\n```\n\n## Restore the Backup Using Cluster Annotation\n\n### Create Restored Cluster\n\nCreate a new cluster with restoration configuration referencing the backup.\n\n```yaml\nkubectl apply -f -    0/1     Init:0/1   0          6s\nrestore-preparedata-XXXXX-   1/1     Running    0          12s\nrestore-preparedata-XXXXX-   0/1     Completed 0          20s\n```\nThese pods copy backup data to Persistent Volumes (PVCs).\n\n\n2. MySQL Cluster Pods:\n```bash\nexample-mysql-cluster-restored-mysql-0     0/4     Pending        0          0s\nexample-mysql-cluster-restored-mysql-0     4/4     Running        0          20s\n```\nPods initialize with restored data and start MySQL services.\n\n## Perform Restoration ",
    "path": "docs/release-1_0/kubeblocks-for-mysql/05-backup-restore/05-restoring-from-full-backup",
    "description": "  # Restore a MySQL Cluster from Backup  This guide explains how to restore a new MySQL cluster from an existing backup in KubeBlocks. It showcases two methods: using **Cluster Annotation** or the **Ops API**.  The **Ops API** provides enhanced control and progress monitoring capabilities for restor"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_05-backup-restore_06-restore-with-pitr",
    "title": "Restore a MySQL Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks",
    "content": "\n\n# Restore a MySQL Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks\n\nThis guide provides a step-by-step walk-through for restoring a MySQL cluster from an existing full backup in KubeBlocks, along with continuous binlog backups for Point-In-Time Recovery (PITR).\n\n## Prerequisites\nBefore proceeding, ensure the following:\n- Environment Setup:\n  - A Kubernetes cluster is up and running.\n  - The kubectl CLI tool is configured to communicate with your cluster.\n  - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Check Existing Backups\nTo perform a PITR restoration, both a full backup and continuous backup are required. Refer to the documentation to configure these backups if they are not already set up.\n\nList available backups with the following command:\n```bash\nkubectl get backup -n demo\n```\n\nExpected Output:\n```bash\nNAME                                               POLICY                                      METHOD           REPO      STATUS      TOTAL-SIZE   DURATION   DELETION-POLICY   CREATION-TIME          COMPLETION-TIME        EXPIRATION-TIME\n77a788fa-example-mysql-cluster-archive-binlog      example-mysql-cluster-mysql-backup-policy   archive-binlog   s3-repo   Running     2110030                 Delete            2025-03-04T02:28:55Z                          2025-04-03T02:28:55Z\nexample-mysql-cluster-xtrabackup-20250305000008    example-mysql-cluster-mysql-backup-policy   xtrabackup       s3-repo   Completed   3102161      18s        Delete            2025-03-05T00:00:11Z   2025-03-05T00:00:29Z   2025-04-04T00:00:29Z\n```\n- '77a788fa-example-mysql-cluster-archive-binlog': Continuous backup (binlog archive).\n- 'example-mysql-cluster-xtrabackup-2",
    "path": "docs/release-1_0/kubeblocks-for-mysql/05-backup-restore/06-restore-with-pitr",
    "description": "  # Restore a MySQL Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks  This guide provides a step-by-step walk-through for restoring a MySQL cluster from an existing full backup in KubeBlocks, along with continuous binlog backups for Point-In-Time Recovery (PITR).  ## Prerequisites"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_07-tls_01-tls-overview",
    "title": "Deploying a MySQL Cluster with TLS on KubeBlocks",
    "content": "\n# Deploying a MySQL Cluster with TLS on KubeBlocks\n\nThis guide demonstrates how to deploy a MySQL cluster with **TLS encryption** using KubeBlocks. TLS ensures secure communication between the MySQL client and server by encrypting data in transit, protecting sensitive information. You will learn how to deploy the cluster, connect securely using TLS, and clean up resources after testing.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploying the MySQL Semi-Synchronous Cluster\n\nKubeBlocks uses a declarative approach for managing MySQL clusters. Below is an example configuration for deploying a MySQL cluster with 2 nodes (1 primary, 1 replicas) in semi-synchronous mode with TLS enabled.\n\nApply the following YAML configuration:\n```yaml\nkubectl apply -f -  STATUS;\n--------------\n\nSSL:\t\t\tCipher in use is TLS_AES_256_GCM_SHA384\n```\nIf the SSL field displays a cipher, the connection is successfully encrypted using TLS.\n\n## Cleanup\nTo remove all resources created in this tutorial, run the following commands:\n```bash\nkubectl delete cluster example-mysql-cluster -n demo\nkubectl delete ns demo\n```\n\n## Summary\nIn this guide, you learned how to:\n- Deploy a MySQL cluster using KubeBlocks and enable TLS encryption for secure communication between the MySQL client and server.\n- Establish a secure MySQL connection with TLS.\n- Verify the secure connection using the MySQL shell.\n\nTLS encryption ensures secure communication by encrypting data in transit and protecting sensitive in",
    "path": "docs/release-1_0/kubeblocks-for-mysql/07-tls/01-tls-overview",
    "description": " # Deploying a MySQL Cluster with TLS on KubeBlocks  This guide demonstrates how to deploy a MySQL cluster with **TLS encryption** using KubeBlocks. TLS ensures secure communication between the MySQL client and server by encrypting data in transit, protecting sensitive information. You will learn ho"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_07-tls_02-tls-custom-cert",
    "title": "Deploy a MySQL Cluster with User-Provided TLS on KubeBlocks",
    "content": "\n# Deploy a MySQL Cluster with User-Provided TLS on KubeBlocks\n\nThis guide explains how to deploy a MySQL cluster with **user-provided TLS certificates** using KubeBlocks. By supplying your own certificates, you have full control over the security configuration for encrypted communication between the MySQL client and server. This guide covers generating certificates, deploying the cluster, and verifying the secure connection.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Generating Certificates\n\nTo enable TLS encryption, you will need to provide a Certificate Authority (CA), a server certificate, and a private key. Follow these steps to generate these using OpenSSL:\n\n1. Generate the Root Certificate (CA)\n```bash\n# Create the CA private key (password optional)\nopenssl genrsa -aes256 -out ca-key.pem 4096\n\n# Generate a self-signed root certificate (valid for 10 years)\nopenssl req -x509 -new -nodes -key ca-key.pem -sha256 -days 3650 -out ca.pem\n# Enter the required information (e.g., Common Name can be \"MySQL Root CA\")\n```\n\n2. Generate the Server Certificate & Key\n```bash\n# Generate the server private key\nopenssl genrsa -out server-key.pem 4096\n\n# Create a Certificate Signing Request (CSR)\nopenssl req -new -key server-key.pem -out server-req.pem\n# Enter server identification details, such as:\n# Common Name (CN) = Server domain name or IP (must match the MySQL server address!)\n\n# Sign the server certificate with the CA (valid for 10 years)\nopenssl x509 -req -in serve",
    "path": "docs/release-1_0/kubeblocks-for-mysql/07-tls/02-tls-custom-cert",
    "description": " # Deploy a MySQL Cluster with User-Provided TLS on KubeBlocks  This guide explains how to deploy a MySQL cluster with **user-provided TLS certificates** using KubeBlocks. By supplying your own certificates, you have full control over the security configuration for encrypted communication between th"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_07-tls_03-mtls",
    "title": "Deploy a MySQL Cluster with mTLS on KubeBlocks",
    "content": "\n# Create a MySQL Cluster With mTLS on KubeBlocks\n\nThis guide explains how to configure a MySQL cluster with **mutual TLS (mTLS)** encryption using KubeBlocks. mTLS ensures both the server and client authenticate each other during a connection, providing enhanced security for your database infrastructure. This guide covers certificate generation, cluster deployment, user configuration for mTLS, and secure connection verification.\n\n\n## What is mTLS?\nMutual TLS (mTLS) is an enhanced security protocol that ensures both the server and the client authenticate each other during a connection. Unlike traditional TLS, where only the client verifies the server's identity, mTLS adds an extra layer of security by requiring both sides to present valid certificates issued by a trusted Certificate Authority (CA).\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Generating Certificates\n\nTo enable TLS encryption, you will need to provide a Certificate Authority (CA), a server certificate, and a private key. Follow these steps to generate these using OpenSSL:\n\n1. Generate the Root Certificate (CA)\n```bash\n# Create the CA private key (password optional)\nopenssl genrsa -aes256 -out ca-key.pem 4096\n\n# Generate a self-signed root certificate (valid for 10 years)\nopenssl req -x509 -new -nodes -key ca-key.pem -sha256 -days 3650 -out ca.pem\n# Enter the required information (e.g., Common Name can be \"MySQL Root CA\")\n```\n\n2. Generate the Server Certificate & Key\n```bash\n# Generate the server",
    "path": "docs/release-1_0/kubeblocks-for-mysql/07-tls/03-mtls",
    "description": " # Create a MySQL Cluster With mTLS on KubeBlocks  This guide explains how to configure a MySQL cluster with **mutual TLS (mTLS)** encryption using KubeBlocks. mTLS ensures both the server and client authenticate each other during a connection, providing enhanced security for your database infrastru"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for MySQL Clusters with the Prometheus Operator",
    "content": "\n# Observability for MySQL Clusters with the Prometheus Operator\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Installing the Prometheus Operator\n\nIf the Prometheus Operator is not already installed, you can install it using Helm:\n\n```bash\nkubectl create namespace monitoring\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack -n monitoring --create-namespace\n```\n\nOr you can follow the steps in [How to install the Prometheus Operator](../docs/install-prometheus.md) to install the Prometheus Operator.\n\nCheck the status of deployed pods:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs             ",
    "path": "docs/release-1_0/kubeblocks-for-mysql/08-monitoring/01-integrate-with-prometheus-operator",
    "description": " # Observability for MySQL Clusters with the Prometheus Operator  ## Prerequisites  Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_09-advanced-pod-management_01-custom-scheduling-policy",
    "title": "Configuring Custom Scheduling Policies for MySQL Cluster Pods in KubeBlocks",
    "content": "\n# Configuring Custom Scheduling Policies for MySQL Cluster Pods in KubeBlocks\n\nThis guide demonstrates how to configure custom scheduling policies for MySQL Cluster Pods in KubeBlocks. For example:\n1. Distribute Pods across different availability zones (AZs) to improve high availability.\n2. Deploy Pods in the same AZ to reduce latency.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n   - A Kubernetes cluster is up and running.\n   - The kubectl CLI tool is configured to communicate with your cluster.\n   - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Verify K8s Node Distribution\n\nOur Kubernetes cluster (EKS) consists of 9 nodes distributed across 3 availability zones, with 3 nodes in each AZ. To confirm the node distribution across availability zones, run the following command:\n\n```bash\nkubectl get nodes -o jsonpath='' | while read node; do echo -n \"Node: $node, Zone: \"; kubectl get node \"$node\" -o jsonpath=''; echo; done\n```\nExpected Output:\n```bash\nip-10-0-1-107.ap-southeast-1.compute.internal   Ready       91m     v1.31.5-eks-5d632ec\nip-10-0-1-183.ap-southeast-1.compute.internal   Ready       71m     v1.31.5-eks-5d632ec\nip-10-0-1-217.ap-southeast-1.compute.internal   Ready       2m13s   v1.31.5-eks-5d632ec\nip-10-0-2-186.ap-southeast-1.compute.internal   Ready       91m     v1.31.5-eks-5d632ec\nip-10-0-2-252.ap-southeast-1.compute.internal   Ready       71m     v1.31.5-eks-5d632ec\nip-10-0-2-71.ap-southeast-1.compute.internal    Ready       2m24s   v1.31.5-eks-5d632ec\nip-10-0-3-143.ap-southeast-1.compute.internal   Ready       91m     v1.31.5-eks-5d632ec\nip-10-0-3-205.ap-southeast-1.compute.internal   Ready       36s     v1.31.5-eks",
    "path": "docs/release-1_0/kubeblocks-for-mysql/09-advanced-pod-management/01-custom-scheduling-policy",
    "description": " # Configuring Custom Scheduling Policies for MySQL Cluster Pods in KubeBlocks  This guide demonstrates how to configure custom scheduling policies for MySQL Cluster Pods in KubeBlocks. For example: 1. Distribute Pods across different availability zones (AZs) to improve high availability. 2. Deploy "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_09-advanced-pod-management_02-custom-pod-resources",
    "title": "Customizing Pod Resource Configurations and Labels in a MySQL Cluster Managed by KubeBlocks",
    "content": "\n# Customizing Pod Resource Configurations and Labels in a MySQL Cluster Managed by KubeBlocks\n\nIn certain scenarios, different Pods within the same database cluster may require varying resource allocations. For example:\n- Some replicas dedicated to report generation might leverage additional resources to handle analytical queries efficiently.\n\nWith KubeBlocks, you can customize Pod resource configurations and labels to tailor each instance to meet its unique requirements.\nThis guide demonstrates how to deploy a MySQL cluster with a custom replica provisioned with higher CPU and memory resources.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node semi-sync MySQL cluster (1 primary, 1 secondary):\n\n```yaml\nkubectl apply -f - \nmysql                           500m / 500m          512Mi / 512Mi           data:20Gi      \n```\n**Observation**:\n- The default replica has 0.5 CPU and 0.5Gi memory.\n- The custom replica has 1 CPU and 1Gi memory.\n\n## Expose the custom Pod as a Service\nTo expose the custom Pod via a separate Service, use the following configuration:\n```yaml\nkubectl apply -f -         3306/TCP                                                12m\nexample-mysql-cluster-mysql            ClusterIP   172.20.11.166            3306/TCP                                                12m\nexample-mysql-cluster-mysql-headless   ClusterIP   None                     3306/TCP,3601/TCP,9104/TCP,3501/TCP,3502/TCP,9901/TCP   12m",
    "path": "docs/release-1_0/kubeblocks-for-mysql/09-advanced-pod-management/02-custom-pod-resources",
    "description": " # Customizing Pod Resource Configurations and Labels in a MySQL Cluster Managed by KubeBlocks  In certain scenarios, different Pods within the same database cluster may require varying resource allocations. For example: - Some replicas dedicated to report generation might leverage additional resour"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_09-advanced-pod-management_03-parallel-pod-management-concurrency",
    "title": "Configuring MySQL Cluster with Controlled Pod Creation, Scaling, and Deletion Parallelism in KubeBlocks",
    "content": "\n# Configuring a MySQL Cluster with Controlled Pod Creation, Scaling, and Deletion Parallelism in KubeBlocks\n\nThis guide demonstrates how to control pod creation, scaling, and deletion parallelism for MySQL clusters in KubeBlocks using the `parallelPodManagementConcurrency` parameter. By defining the maximum number of pods that can be managed in parallel, it allows users to balance operational speed and system stability. Unlike the `podManagementPolicy` in StatefulSet, which only provides two fixed options (`OrderedReady` or `Parallel`), `parallelPodManagementConcurrency` offers more flexibility, making it ideal for both resource-sensitive and production environments.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node semi-sync MySQL cluster (1 primary, 1 secondary) and set the `parallelPodManagementConcurrency` parameter to 1 to enforce sequential pod creation.\n\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: Delete\n  componentSpecs:\n    - name: mysql\n      serviceVersion: 8.0.35\n      replicas: 2\n      parallelPodManagementConcurrency: 1\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            ",
    "path": "docs/release-1_0/kubeblocks-for-mysql/09-advanced-pod-management/03-parallel-pod-management-concurrency",
    "description": " # Configuring a MySQL Cluster with Controlled Pod Creation, Scaling, and Deletion Parallelism in KubeBlocks  This guide demonstrates how to control pod creation, scaling, and deletion parallelism for MySQL clusters in KubeBlocks using the `parallelPodManagementConcurrency` parameter. By defining th"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_09-advanced-pod-management_04-instance-update-strategy-ondelete",
    "title": "Setting Instance Update Strategy to OnDelete for MySQL Clusters in KubeBlocks",
    "content": "\n# Set Instance Update Strategy to OnDelete for MySQL Clusters in KubeBlocks\n\nThe `instanceUpdateStrategy.type` field supports two values: 'OnDelete' and 'RollingUpdate'.\n- 'OnDelete': Updates that require a Pod restart are blocked until the Pods are manually deleted. This provides fine-grained control over updates, as ordered rolling restarts are disabled. You decide when and how to restart the Pods, ensuring minimal disruption to your workload.\n- 'RollingUpdate' (default): Updates are applied automatically with ordered rolling restarts. The Operator restarts Pods in a controlled manner to ensure availability and a seamless update process.\n\nBy using the OnDelete strategy, you can tailor update behavior to meet specific requirements, such as maintaining maximum stability during updates or scheduling restarts during maintenance windows.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node MySQL cluster with semi-synchronous replication (1 primary, 1 secondary) using the following YAML configuration:\n\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: Delete\n  componentSpecs:\n    - name: mysql\n      serviceVersion: 8.0.35\n      replicas: 2\n      instanceUpdateStrategy:\n        type: OnDelete\n      resources:\n        limits:\n          cpu: '0.5'\n          memory:",
    "path": "docs/release-1_0/kubeblocks-for-mysql/09-advanced-pod-management/04-instance-update-strategy-ondelete",
    "description": " # Set Instance Update Strategy to OnDelete for MySQL Clusters in KubeBlocks  The `instanceUpdateStrategy.type` field supports two values: 'OnDelete' and 'RollingUpdate'. - 'OnDelete': Updates that require a Pod restart are blocked until the Pods are manually deleted. This provides fine-grained cont"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-mysql_09-advanced-pod-management_05-gradual-rolling-update",
    "title": "Configuring a MySQL Cluster with Gradual Rolling Update in KubeBlocks",
    "content": "\n# Configuring a MySQL Cluster with Gradual Rolling Update in KubeBlocks\n\nThis guide demonstrates how to configure gradual rolling updates for MySQL clusters in KubeBlocks using the rollingUpdate strategy. Gradual rolling updates allow you to control the number of Pods updated at a time, ensuring minimal disruption during updates.\n**Key Parameters:**\n- `rollingUpdate.replicas`: Specifies the number of instances to update during each step of the rolling update. The remaining instances stay unaffected.\n  - The value can be an absolute number (e.g., 5) or a percentage of desired instances (e.g., 10%).\n  - Absolute numbers are calculated by rounding up percentages.\n  - The default is the total number of replicas, meaning all instances are updated.\n- `rollingUpdate.maxUnavailable`: Limits the maximum number of instances that can be unavailable during the update.\n  - The value can also be an absolute number (e.g., 5) or a percentage (e.g., 10%).\n  - This value cannot be 0. The default is 1.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a MySQL Semi-Synchronous Cluster\n\nDeploy a 2-node MySQL cluster with semi-synchronous replication (1 primary, 1 secondary) using the following YAML configuration:\n```yaml\nkubectl apply -f - <<EOF\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: example-mysql-cluster\n  namespace: demo\nspec:\n  clusterDef: mysql\n  topology: semisync\n  terminationPolicy: Delete\n  componentSpecs:\n    - name: mysql\n      serviceVersion",
    "path": "docs/release-1_0/kubeblocks-for-mysql/09-advanced-pod-management/05-gradual-rolling-update",
    "description": " # Configuring a MySQL Cluster with Gradual Rolling Update in KubeBlocks  This guide demonstrates how to configure gradual rolling updates for MySQL clusters in KubeBlocks using the rollingUpdate strategy. Gradual rolling updates allow you to control the number of Pods updated at a time, ensuring mi"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_04-operations_01-stop-start-restart",
    "title": "PostgreSQL Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# PostgreSQL Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a PostgreSQL cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a PostgreSQL cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Retains persistent storage (PVCs)\n3. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\nOption 1: OpsRequest API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: pg-cluster-stop-ops\n  namespace: demo\nspec:\n  clusterName: pg-cluster\n  type: Stop\n```\n\n\n\nOption 2: Cluster API Patch\n\nModify the cluster spec directly by patching the stop field:\n\n```bash\nkubectl patch cluster pg-cluster -n demo --type='json' -p='[\n  \n]'\n```\n\n\n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster pg-cluster -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME         CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\n    pg-cluster   postgresql           Delete               Stopping   6m3s\n    pg-cluster   postgresql           Delete               Stopped    6m55s\n    ```\n\n2. Verify no running pods:\n   ",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/04-operations/01-stop-start-restart",
    "description": "  # PostgreSQL Cluster Lifecycle Management  This guide demonstrates how to manage a PostgreSQL cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help optimize resourc"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a PostgreSQL Cluster",
    "content": "\n\n\n\n# Vertical Scaling for PostgreSQL Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a PostgreSQL cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for PostgreSQL instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks orchestrates scaling with minimal impact:\n1. Secondary replicas update first\n2. Primary updates last after secondaries are healthy\n3. Cluster status transitions from `Updating` to `Running`\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Secondary replicas are updated first (one at a time)\n1. Primary is updated last after secondary replicas are healthy\n1. Cluster status transitions from `Updating` to `Running`\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the postgresql component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: pg-cluster-vscale-ops\n    namespace: demo\n  spec:\n    clusterName: pg-cluster\n    type: VerticalScaling\n    verticalScaling:\n    - componentName: postgresql\n      requests:\n        cpu: '1'\n        memory: 1Gi\n      limits:\n        cpu: '1'\n        memory: 1Gi\n  ```\n  What Happens During Vertical Scaling?\n  - Secondary Pods are recreated first to ensure the primary Pod remains available.\n  - Once all secondary Pods are updated, the primary Pod is restarted with the new resource configuration.\n\n\n  You can check the progress of the scaling operation with the following command:\n\n  ```bash\n  kubectl -n demo get ops pg-cluster-vscale-ops -w\n  ```\n\n  Expected Result:\n  ```bash",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for PostgreSQL Clusters with KubeBlocks  This guide demonstrates how to vertically scale a PostgreSQL cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies compute resources (CPU "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of PostgreSQL Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for PostgreSQL Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a PostgreSQL cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running` with `secondary` role\n2. Data synced from primary to new replica\n3. Cluster status changes from `Updating` to `Running`\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the PostgreSQL cluster by adding 1 replica:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: pg-cluster-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: pg-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: postgresql\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops pg-cluster-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME           TYPE                CLUSTER      STATUS    PROGRESS   AGE\n  pg-scale-out   HorizontalScaling   pg-cluster   Running   0/1        8s\n  pg-scale-out   HorizontalScaling   pg-cluster   Running   1/1        24s\n  pg-scale-out   HorizontalScaling   pg-cluster   Succeed   1/1        24s\n  ```\n  \n\n\n  Option 2: Direct Cluster API Update\n\n  Alternatively, you can perform a direct update to the `replicas` field in the Cluster resource:\n\n  ```yaml\n  apiVersion: apps.kubeblocks.io/v1\n  kind: Cluster\n  metadata:\n    name: pg-cluster\n    namespace: demo\n  spec:\n    terminationPolicy: Delete\n    clusterDef: postgresql\n    top",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for PostgreSQL Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a PostgreSQL cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequi"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a PostgreSQL Cluster",
    "content": "\n\n\n\n# Expanding Volume in a PostgreSQL Cluster\n\nThis guide explains how to expand Persistent Volume Claims (PVCs) in a PostgreSQL cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the underlying storage class, this operation can be performed without downtime.\n\nVolume expansion allows you to increase the size of a Persistent Volume Claim (PVC) after it has been created. This feature was introduced in Kubernetes v1.11 and became generally available (GA) in Kubernetes v1.24.\n\n## Prerequisites\n\n\n\n### Check the Storage Class for Volume Expansion Support\n\nList all available storage classes and verify if volume expansion is supported by checking the `ALLOWVOLUMEEXPANSION` field:\n```bash\nkubectl get storageclass\n```\n\nExample Output:\n```bash\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4d10h\nkb-default-sc       ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   3d7h\nsc-s3-repo-2qsxfh   ru.yandex.s3.csi        Retain          Immediate              false                  3d7h\n```\nEnsure the storage class you are using has `ALLOWVOLUMEEXPANSION` set to true. If it is false, the storage class does not support volume expansion.\n\n## Deploy a PostgreSQL Replication Cluster with StorageClass\n\nKubeBlocks uses a declarative approach to manage PostgreSQL clusters. Below is an example configuration for deploying a PostgreSQL cluster with 2 replicas (1 primary, 1 secondary).\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: pg-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: postgresql\n  topology: replication\n  componentSpecs:\n    - name: postgresql\n    ",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a PostgreSQL Cluster  This guide explains how to expand Persistent Volume Claims (PVCs) in a PostgreSQL cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported b"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy PostgreSQL Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage PostgreSQL Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing PostgreSQL services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## View Network Services\nList the Services created for the PostgreSQL cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=pg-cluster -n demo\n```\n\nExample Services:\n```bash\nNAME                               TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\npg-cluster-postgresql-postgresql   ClusterIP   10.96.19.237           5432/TCP,6432/TCP   157m\n```\n\n:::note\n\nThere are two ports here 5432 and 6432, where 5432 is for postgresql and 6432 for PgBouncer.\n\n:::\n\n## Expose PostgreSQL Service\n\nExternal service addresses enable public internet access to PostgreSQL, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the PostgreSQL service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: pg-cluster-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: pg-cluster\n    expose:\n    - componentName: postgresql\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options are 'ClusterIP', 'NodePor",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/04-operations/05-manage-loadbalancer",
    "description": "    # Manage PostgreSQL Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing PostgreSQL services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_04-operations_06-minior-version-upgrade",
    "title": "Upgrading the Minor Version of a PostgreSQL Cluster in KubeBlocks",
    "content": "\n\n# Upgrading the Minor Version of a PostgreSQL Cluster in KubeBlocks\n\nThis guide walks you through the deployment and minor version upgrade of a PostgreSQL cluster managed by KubeBlocks, ensuring minimal downtime during the process.\n\nTo minimize the impact on database availability, the upgrade process starts with the replicas (secondary instances). Once the replicas are upgraded, a switchover operation promotes one of the upgraded replicas to primary. The switchover process is very fast, typically completing in a few hundred milliseconds. After the switchover, the original primary instance is upgraded, ensuring minimal disruption to the application.\n\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a PostgreSQL Replication Cluster\n\nKubeBlocks uses a declarative approach for managing PostgreSQL clusters. Below is an example configuration for deploying a PostgreSQL cluster with 2 replicas (1 primary, 1 replicas).\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: pg-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: postgresql\n  topology: replication\n  componentSpecs:\n    - name: postgresql\n      serviceVersion: 14.7.2  # use 14.7.2 here to test minor version upgrade\n      labels:\n        apps.kubeblocks.postgres.patroni/scope: pg-cluster-postgresql\n      disableExporter: true\n      replicas: 2\n      resources:\n        limits:\n          cpu: \"0.5\"\n   ",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/04-operations/06-minior-version-upgrade",
    "description": "  # Upgrading the Minor Version of a PostgreSQL Cluster in KubeBlocks  This guide walks you through the deployment and minor version upgrade of a PostgreSQL cluster managed by KubeBlocks, ensuring minimal downtime during the process.  To minimize the impact on database availability, the upgrade proc"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_04-operations_07-modify-parameters",
    "title": "Modify PostgreSQL Parameters",
    "content": "\n# Modify PostgreSQL Parameters\n\nDatabase reconfiguration involves modifying parameters, settings, or configurations to optimize performance, security, or availability. Parameter changes fall into two categories:\n\n| Type | Restart Required | Scope | Example Parameters |\n|------|------------------|-------|--------------------|\n| **Dynamic** | No | Immediate effect | `max_connections` |\n| **Static** | Yes | After restart | `shared_buffers` |\n\nFor static parameters, KubeBlocks minimizes downtime by:\n1. Modifying and restarting replica nodes first\n2. Performing a switchover to promote the updated replica as primary (typically completes in milliseconds)\n3. Restarting the original primary node\n\nThis guide demonstrates how to modify both dynamic and static parameters of a PostgreSQL cluster managed by KubeBlocks using a Reconfiguring OpsRequest.\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Check Parameter Values\n\n### Retrieve Credentials\nKubeBlocks automatically creates a secret containing the PostgreSQL postgres credentials. Retrieve the credentials with the following commands:\n```bash\nNAME=`kubectl get secrets -n demo pg-cluster-postgresql-account-postgres -o jsonpath='' | base64 -d`\nPASSWD=`kubectl get secrets -n demo pg-cluster-postgresql-account-postgres -o jsonpath='' | base64 -d`\n```\n\n### Access PostgreSQL Cluster\nTo connect to the cluster's primary node, use the PostgreSQL client:\n```bash\nkubectl exec -it -n demo pg-cluster-postgresql-0 -c postgresql -- env PGUSER=$ PGPASSWORD=$ psql\n```\n\n### Query Parameter Values\n\nOnce connected, you can query the current value of 'max_connections' and 'shared_buffers':\n```sql\npostgres=# SHOW max_connections;\n max_connections\n-----------------\n 56\n(1 row)\n\npostgres=# show pgaudit.log;\n pgaudit.log\n-------------\n ddl,read,write\n(1 row)\n\npostgres=# show shared_buffers;\n shared_buffers\n----------------\n 128MB\n(1 row)\n```\n\n## Dynamic Parameter Example: Modifying max_connections and pgaudit.",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/04-operations/07-modify-parameters",
    "description": " # Modify PostgreSQL Parameters  Database reconfiguration involves modifying parameters, settings, or configurations to optimize performance, security, or availability. Parameter changes fall into two categories:  | Type | Restart Required | Scope | Example Parameters | |------|------------------|--"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_04-operations_08-switchover",
    "title": "PostgreSQL Cluster Switchover",
    "content": "\n\n# PostgreSQL Cluster Switchover\n\nA **switchover** is a planned operation that transfers the primary role from one PostgreSQL instance to another. Unlike failover which occurs during failures, switchover provides:\n- Controlled role transitions\n- Minimal downtime (typically a few hundred milliseconds)\n- Predictable maintenance windows\n\nSwitchover is ideal for:\n- Node maintenance/upgrades\n- Workload rebalancing\n- Testing high availability\n- Planned infrastructure changes\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Check Roles\nList the Pods and their roles (primary or secondary):\n\n```bash\nkubectl get pods -n demo -l app.kubernetes.io/instance=pg-cluster -L kubeblocks.io/role\n```\n\nExample Output:\n\n```text\nNAME                      READY   STATUS    RESTARTS   AGE     ROLE\npg-cluster-postgresql-0   4/4     Running   0          9m59s   primary\npg-cluster-postgresql-1   4/4     Running   0          11m     secondary\n```\n\n## Performing a Planned Switchover\n\nTo initiate a planned switchover, create an OpsRequest resource as shown below:\n\n  Option 1: Automatic Switchover (No preferred candidate)\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: pg-switchover-ops\n    namespace: demo\n  spec:\n    clusterName: pg-cluster\n    type: Switchover\n    switchover:\n    - componentName: postgresql\n      instanceName: pg-cluster-postgresql-0\n  ```\n **Key Parameters:**\n  - `instanceName`: Specifies the instance (Pod) that is primary or leader before a switchover operation.\n\n  \n  Option 2: Targeted Switchover (Specific candidate)\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: pg-switchover-targeted\n    namespace: demo\n  spec:\n    clusterName: pg-cluster\n    type: Switchover\n    switchover:\n    - componentName: postgresql\n      # Specifies the instance whose role will be transferred.\n      # A typical usage is to transfer the leader role in a con",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/04-operations/08-switchover",
    "description": "  # PostgreSQL Cluster Switchover  A **switchover** is a planned operation that transfers the primary role from one PostgreSQL instance to another. Unlike failover which occurs during failures, switchover provides: - Controlled role transitions - Minimal downtime (typically a few hundred millisecond"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed PostgreSQL Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed PostgreSQL Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in PostgreSQL clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\nTo decommission a specific Pod (e.g., 'pg-cluster-postgresql-1'), you can use one of the following methods:\n\n\n\n  Option 1: Using OpsRequest\n\n  Create an OpsRequest to mark the Pod as offline:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: pg-cluster-decommission-ops\n    namespace: demo\n  spec:\n    clusterName: pg-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: postgresql\n      scaleIn:\n        onlineInstancesToOffline:\n          - 'pg-cluster-postgresql-1'  # Specifies the instance names that need to be taken offline\n  ```\n\n  #### Monitor the Decom",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed PostgreSQL Clusters  This guide explains how to decommission (take offline) specific Pods in PostgreSQL clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_04-operations_11-rebuild-replica",
    "title": "Recovering PostgreSQL Replica in KubeBlocks",
    "content": "\n# Rebuilding PostgreSQL Replicas in KubeBlocks\n\nThis guide demonstrates how to rebuild replicas using both in-place and non-in-place methods.\n\n**What is Replica Rebuilding**?\n\nReplica rebuilding is the process of recreating a PostgreSQL replica from scratch or from a backup while maintaining:\n- **Data Consistency**: Ensures the replica has an exact copy of primary data\n- **High Availability**: Minimizes downtime during the rebuild process\n\nDuring this process:\n1. The problematic replica is identified and isolated\n2. A new base backup is taken from the primary\n3. WAL (Write-Ahead Log) segments are streamed to catch up\n4. The replica rejoins the replication cluster\n\n**When to Rebuild a PostgreSQL Instance**?\n\nRebuilding becomes necessary in these common scenarios:\n- Replica falls too far behind primary (irrecoverable lag), or Replication slot corruption\n- WAL file gaps that can't be automatically resolved\n- Data Corruption: with storage-level corruption (disk/volume issues), inconsistent data between primary and replica, etc\n- Infrastructure Issues: Node failure, storage device failure or cross Zone/Region migration\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Connect to the Primary PostgreSQL Replcia and Write Mock Data\n\nCheck replica roles with command:\n```bash\nkubectl get pods -n demo -l app.kubernetes.io/instance=pg-cluster -L kubeblocks.io/role\n```\n\nExample Output:\n```bash\nNAME                      READY   STATUS    RESTARTS   AGE     ROLE\npg-cluster-postgresql-0   4/4     Running   0          13m     secondary\npg-cluster-postgresql-1   4/4     Running   0          12m     primary\n```\n\n### Step 1: Connect to the Primary Instance\n\nKubeBlocks automatically creates a Secret containing the PostgreSQL postgres credentials. Retrieve the PostgreSQL postgres credentials:\n\n```bash\nNAME=`kubectl get secrets -n demo pg-cluster-postgresql-account-postgres -o jsonpath='' | base64 -d`\nPASSWD=`kubectl get secrets -n demo pg-cluster-p",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/04-operations/11-rebuild-replica",
    "description": " # Rebuilding PostgreSQL Replicas in KubeBlocks  This guide demonstrates how to rebuild replicas using both in-place and non-in-place methods.  **What is Replica Rebuilding**?  Replica rebuilding is the process of recreating a PostgreSQL replica from scratch or from a backup while maintaining: - **D"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_06-custom-secret_01-custom-secret",
    "title": "Create a PostgreSQL Cluster with a Custom Root Password on KubeBlocks",
    "content": "\n# Create PostgreSQL Cluster With Custom Password on KubeBlocks\n\nThis guide demonstrates how to deploy a PostgreSQL cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.\n\n## Prerequisites\n\n\n\n## Deploying the PostgreSQL Replication Cluster\n\nKubeBlocks uses a declarative approach for managing PostgreSQL clusters. Below is an example configuration for deploying a PostgreSQL cluster with 2 nodes (1 primary, 1 replicas) and a custom root password.\n\n### Step 1: Create a Secret for the Root Account\n\nThe custom root password is stored in a Kubernetes Secret. Create the Secret by applying the following YAML:\n\n```yaml\napiVersion: v1\ndata:\n  password: Y3VzdG9tcGFzc3dvcmQ= # custompassword\n  username: cm9vdA== #root\nimmutable: true\nkind: Secret\nmetadata:\n  name: custom-pg-secret\n  namespace: demo\n```\n- password: Replace custompassword with your desired password and encode it using Base64 (`echo -n \"custompassword\" | base64`).\n- username: The default PostgreSQL postgres user is 'root', encoded as 'cm9vdA=='.\n\n\n### Step 2: Deploy the PostgreSQL Cluster\n\nApply the following manifest to deploy the PostgreSQL cluster, referencing the Secret created in Step 1 for the root account:\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: pg-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: postgresql\n  topology: replication\n  componentSpecs:\n    - name: postgresql\n      serviceVersion: 16.4.0\n      labels:\n        apps.kubeblocks.postgres.patroni/scope: pg-cluster-postgresql\n      disableExporter: true\n      replicas: 2\n      systemAccounts:\n        - name: postgres\n          secretRef:\n            name: custom-pg-secret\n            namespace: demo\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            accessModes:\n              - ReadWriteOnce\n         ",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/06-custom-secret/01-custom-secret",
    "description": " # Create PostgreSQL Cluster With Custom Password on KubeBlocks  This guide demonstrates how to deploy a PostgreSQL cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.  ## Prerequisites    ## Deploying the PostgreSQL Replication Cluster  KubeBlocks uses a declarative app"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_05-backup-restore_01-create-backuprepo",
    "title": "Create a Backup Repository for KubeBlocks",
    "content": "\n# Create a BackupRepo for KubeBlocks\n\nThis guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.\n\n## Prerequisites\n- AWS CLI configured with appropriate permissions to create S3 buckets.\n- kubectl access to your Kubernetes cluster.\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks)) and running in the kb-system namespace.\n\n## Step 1: Create S3 Bucket\n\nUse the AWS CLI to create an S3 bucket in your desired region. Replace `` with your target AWS region (e.g., `us-east-1`, `ap-southeast-1`).\n\n```bash\n aws s3api create-bucket --bucket kubeblocks-backup-repo --region  --create-bucket-configuration LocationConstraint=\n```\n\nExample (for us-west-1):\n```bash\naws s3api create-bucket \\\n  --bucket kubeblocks-backup-repo \\\n  --region us-west-1 \\\n  --create-bucket-configuration LocationConstraint=us-west-1\n```\n\nExample Output:\n\n```json\n\n```\n\nVerification:\nConfirm the bucket was created by listing its contents (it will be empty initially):\n\n```bash\naws s3 ls s3://kubeblocks-backup-repo\n```\n\n## Step 2: Create a Kubernetes Secret for AWS Credentials\n\nStore your AWS credentials securely in a Kubernetes Secret. Replace `` and `` with your actual AWS credentials:\n\n```bash\n# Create a secret to save the access key\nkubectl create secret generic s3-credential-for-backuprepo \\\n  --from-literal=accessKeyId= \\\n  --from-literal=secretAccessKey= \\\n  -n kb-system\n```\n\n## Step 3: Configure Backup Repository\n\nA BackupRepo is a custom resource that defines a storage repository for backups. In this step, you'll integrate your S3 bucket with KubeBlocks by creating a BackupRepo resource.\n\nApply the following YAML to create the BackupRepo. Replace fields(e.g., bucket name, region) with your specific settings.\n\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupRepo\nmetadata:\n  name: s3-repo\n  annotations:\n    # mark this backuprepo as default one\n    dataprotection.kubeblocks.io/is",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/05-backup-restore/01-create-backuprepo",
    "description": " # Create a BackupRepo for KubeBlocks  This guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.  ## Prerequisites - AWS CLI configured with appropriate permissions to create S3 buckets. - kubectl access to your Kubernetes cluster. -"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_05-backup-restore_02-create-full-backup",
    "title": "Create a Full Backup for a PostgreSQL Cluster on KubeBlocks",
    "content": "\n\n\n# Create a Full Backup for PostgreSQL on KubeBlocks\n\nThis guide demonstrates how to create and validate full backups for PostgreSQL clusters on KubeBlocks using the `pg-basebackup` method through both:\n- The Backup API (direct backup operations)\n- The OpsRequest API (managed backup operations with enhanced monitoring)\n\nWe will cover how to restore data from a backup in the [Restore From Full Backup](./05-restoring-from-full-backup) guide.\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Backup Prerequisites\n\nBefore creating backups, ensure:\n1. Backup repository is configured:\n   - `BackupRepo` resource exists\n   - Network connectivity between cluster and repository\n   - `BackupRepo` status shows \"Ready\"\n\n2. Cluster is ready:\n   - Cluster status is \"Running\"\n   - No ongoing operations (scaling, upgrades, etc.)\n\n## Identify Backup Configuration\n\nCheck available backup policies and schedules:\n\n```bash\n# List backup policies\nkubectl get backuppolicy -n demo -l app.kubernetes.io/instance=pg-cluster\n\n# List backup schedules\nkubectl get backupschedule -n demo -l app.kubernetes.io/instance=pg-cluster\n```\n\nExpected Output:\n```bash\nNAME                                            BACKUP-REPO   STATUS      AGE\npg-cluster-postgresql-backup-policy                           Available   58m\n\nNAME                                              STATUS      AGE\npg-cluster-postgresql-backup-schedule             Available   60m\n```\n\nView supported backup methods in the BackupPolicy CR 'pg-cluster-postgresql-backup-policy':\n\n```bash\nkubectl get backuppolicy pg-cluster-postgresql-backup-policy -n demo -oyaml | yq '.spec.backupMethods[].name'\n```\n\nExample Output:\n```bash\npg-basebackup\nwal-g\nwal-g-incremental\narchive-wal\nwal-g-archive\n```\n\n\n**List of Backup methods**\n\nKubeBlocks PostgreSQL supports these backup methods:\n\n| Feature           | Method          | Description |\n|-------------------|-----------------|-------------|\n| Full Backup       | p",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/05-backup-restore/02-create-full-backup",
    "description": "   # Create a Full Backup for PostgreSQL on KubeBlocks  This guide demonstrates how to create and validate full backups for PostgreSQL clusters on KubeBlocks using the `pg-basebackup` method through both: - The Backup API (direct backup operations) - The OpsRequest API (managed backup operations wit"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_05-backup-restore_03-scheduled-full-backup",
    "title": "Setting Up a PostgreSQL Cluster with Scheduled Backups in KubeBlocks",
    "content": "\n\n# Setting Up a PostgreSQL Cluster with Scheduled Backups in KubeBlocks\n\nThis guide demonstrates how to deploy a PostgreSQL cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.\n\n## Prerequisites\n\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Prerequisites for Backup\n\n1. Backup Repository Configured:\n   - Configured `BackupRepo`\n   - Network connectivity between cluster and repo, `BackupRepo` status is `Ready`\n\n2. Cluster is Running:\n   - Cluster must be in `Running` state\n   - No ongoing operations (scaling, upgrades etc.)\n\n## Configure Scheduled Backups\n\nKubeBlocks automatically creates a `BackupSchedule` resource when the cluster is created. Follow these steps to enable and configure scheduled backups:\n\n1. Verify the default backup schedule configuration:\n\n```bash\nkubectl get backupschedule pg-cluster-postgresql-backup-schedule  -n demo -oyaml\n```\n\nExample Output:\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupSchedule\nspec:\n  backupPolicyName: pg-cluster-postgresql-backup-policy\n  schedules:\n  - backupMethod: pg-basebackup\n    # ┌───────────── minute (0-59)\n    # │ ┌───────────── hour (0-23)\n    # │ │ ┌───────────── day of month (1-31)\n    # │ │ │ ┌───────────── month (1-12)\n    # │ │ │ │ ┌───────────── day of week (0-6) (Sunday=0)\n    # │ │ │ │ │\n    # 0 18 * * *\n    # schedule this job every day at 6:00 PM (18:00).\n    cronExpression: 0 18 * * * # update the cronExpression to your need\n    enabled: false # set to `true` to schedule base backup periodically\n    retentionPeriod: 7d # set the retention period to your need\n```\n\n2. Enable and customize the backup schedule:\n```bash\nkubectl edit backupschedule pg-cluster-postgresql-backup-schedule -n demo\n```\n\nUpdate these key parameters:\n- `enabled`: Set to `true` to activate scheduled backups\n- `cronExpression`: Configure backup frequency using cron syntax\n- `retentionPeriod`: Set how long to keep backups (e.g., `7d`, `1mo`)\n\nExa",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/05-backup-restore/03-scheduled-full-backup",
    "description": "  # Setting Up a PostgreSQL Cluster with Scheduled Backups in KubeBlocks  This guide demonstrates how to deploy a PostgreSQL cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.  ## Prerequisites    ## Deploy a PostgreSQL Cluster    ## Verifying the Deployment"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_05-backup-restore_04-scheduled-continuous-backup",
    "title": "Setting Up a PostgreSQL Cluster with Scheduled Continuous Backup in KubeBlocks",
    "content": "\n\n# Setting Up a PostgreSQL Cluster with Scheduled Continuous Backup Enabled in KubeBlocks\n\nThis guide demonstrates how to configure a PostgreSQL cluster on KubeBlocks with:\n\n- Scheduled full backups (base backups)\n- Continuous WAL (Write-Ahead Log) archiving\n- Point-In-Time Recovery (PITR) capabilities\n\nThis combination provides comprehensive data protection with minimal recovery point objectives (RPO).\n\n## What is PITR?\nPoint-In-Time Recovery (PITR) allows you to restore a database to a specific moment in time by combining full backups with continuous binlog/wal/archive log backups.\n\nFor details on restoring data from both full backups and continuous binlog backups, refer to the [Restore From PITR](restore-with-pitr.mdx)  guide.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Prerequisites for Backup\n\n1. Backup Repository Configured:\n   - Configured `BackupRepo`\n   - Network connectivity between cluster and repo, `BackupRepo` status is `Ready`\n\n2. Cluster is Running:\n   - Cluster must be in `Running` state\n   - No ongoing operations (scaling, upgrades etc.)\n\n\n## List of Backup methods\n\nKubeBlocks PostgreSQL supports these backup methods:\n\n| Feature           | Method          | Description |\n|-------------------|-----------------|-------------|\n| Full Backup       | pg-basebackup   | Uses `pg_basebackup`, a PostgreSQL utility to create a base backup |\n| Full Backup       | wal-g  | Uses `wal-g` to create a full backup (requires WAL-G configuration) |\n| Continuou",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/05-backup-restore/04-scheduled-continuous-backup",
    "description": "  # Setting Up a PostgreSQL Cluster with Scheduled Continuous Backup Enabled in KubeBlocks  This guide demonstrates how to configure a PostgreSQL cluster on KubeBlocks with:  - Scheduled full backups (base backups) - Continuous WAL (Write-Ahead Log) archiving - Point-In-Time Recovery (PITR) capabili"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_05-backup-restore_05-restoring-from-full-backup",
    "title": "Restore a PostgreSQL Cluster from Backup",
    "content": "\n\n# Restore a PostgreSQL Cluster from Backup\n\nThis guide demonstrates two methods to restore a PostgreSQL cluster from backup in KubeBlocks:\n\n1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations\n2. **OpsRequest API Method** - Enhanced operational control with progress monitoring\n\n## Prerequisites\n\n\n\n## Preparing for Restoration: Locate one Full Backup\nBefore restoring, ensure that there is a full backup available. The restoration process will use this backup to create a new PostgreSQL cluster.\n\n- Backup repository accessible from new cluster\n- Valid full backup in `Completed` state\n- Adequate CPU/memory resources\n- Sufficient storage capacity\n\nFind available full backups:\n\n```bash\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Full,app.kubernetes.io/instance=pg-cluster # get the list of full backups\n```\n\nPick ONE of the Backups whose status is `Completed`.\n\n### Step 1: Create Restored Cluster\nCreate a new cluster with restore configuration:\n\nKey parameters:\n- `kubeblocks.io/restore-from-backup` annotation\n- Backup name and namespace located from the previous steps\n\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: pg-restored\n  namespace: demo\n  annotations:\n    # NOTE: replcae  with the backup name\n    kubeblocks.io/restore-from-backup: '}'\nspec:\n  terminationPolicy: Delete\n  clusterDef: postgresql\n  topology: replication\n  componentSpecs:\n    - name: postgresql\n      serviceVersion: 16.4.0\n      disableExporter: true\n      labels:\n        apps.kubeblocks.postgres.patroni/scope: pg-restored-postgresql\n      replicas: 2\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/05-backup-restore/05-restoring-from-full-backup",
    "description": "  # Restore a PostgreSQL Cluster from Backup  This guide demonstrates two methods to restore a PostgreSQL cluster from backup in KubeBlocks:  1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations 2. **OpsRequest API Method** - Enhanced operational control with progres"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_05-backup-restore_06-restore-with-pitr",
    "title": "Restore a PostgreSQL Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks",
    "content": "\n\n# Restore a PostgreSQL Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks\n\nThis guide demonstrates how to perform Point-In-Time Recovery (PITR) for PostgreSQL clusters in KubeBlocks using:\n\n1. A full base backup\n2. Continuous WAL (Write-Ahead Log) backups\n3. Two restoration methods:\n   - Cluster Annotation (declarative approach)\n   - OpsRequest API (operational control)\n\nPITR enables recovery to any moment within the `timeRange` specified.\n\n## Prerequisites\n\n\n\n## Prepare for PITR Restoration\nTo perform a PITR restoration, both a full backup and continuous backup are required. Refer to the documentation to configure these backups if they are not already set up.\n\n- Completed full backup\n- Active continuous WAL backup\n- Backup repository accessible\n- Sufficient resources for new cluster\n\nTo identify the list of full and continuous backups, you may follow the steps:\n\n### 1. Verify Continuous Backup\nConfirm you have a continuous WAL backup, either running or completed:\n\n```bash\n# expect EXACTLY ONE continuous backup per cluster\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Continuous,app.kubernetes.io/instance=pg-cluster\n```\n\n### 2. Check Backup Time Range\nGet the valid recovery window:\n\n```bash\nkubectl get backup  -n demo -o yaml | yq '.status.timeRange'\n```\n\nExpected Output:\n```text\nstart: \"2025-05-07T09:12:47Z\"\nend: \"2025-05-07T09:22:50Z\"\n```\n\n### 3. Identify Full Backup\nFind available full backups that meet:\n- Status: Completed\n- Completion time after continuous backup start time\n\n```bash\n# expect one or more Full backups\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Full,app.kubernetes.io/instance=pg-cluster\n```\n\n:::tip\nKubeBlocks automatically selects the most recent qualifying full backup as the base.\nMake sure there is a full backup meets the condition: its `stopTime`/`completionTimestamp` must **AFTER** Continuous backup's `startTime`, otherwise PITR restoration will fail.\n:::\n\n## Option 1: Clu",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/05-backup-restore/06-restore-with-pitr",
    "description": "  # Restore a PostgreSQL Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks  This guide demonstrates how to perform Point-In-Time Recovery (PITR) for PostgreSQL clusters in KubeBlocks using:  1. A full base backup 2. Continuous WAL (Write-Ahead Log) backups 3. Two restoration method"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_07-tls_01-tls-overview",
    "title": "Deploying a PostgreSQL Cluster with TLS on KubeBlocks",
    "content": "\n\n# Deploying a PostgreSQL Cluster with TLS on KubeBlocks\n\nThis guide demonstrates how to deploy a PostgreSQL cluster with TLS encryption using KubeBlocks. Transport Layer Security (TLS) ensures secure communication between PostgreSQL clients and servers by encrypting data in transit, protecting sensitive information from interception. You'll learn how to:\n\n- Deploy a PostgreSQL cluster with TLS enabled\n- Establish secure connections using different TLS modes\n- Verify the TLS configuration\n- Clean up resources after testing\n\n## Prerequisites\n\n\n\n## Deploying the PostgreSQL Replication Cluster\n\nKubeBlocks uses a declarative approach for managing PostgreSQL clusters. Below is a configuration example for deploying a PostgreSQL cluster with TLS enabled (1 primary, 1 replica):\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: pg-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: postgresql\n  topology: replication\n  componentSpecs:\n    - name: postgresql\n      serviceVersion: 16.4.0\n      tls: true  # Enable TLS encryption\n      issuer:\n        name: KubeBlocks  # Use KubeBlocks' built-in certificate authority\n      labels:\n        apps.kubeblocks.postgres.patroni/scope: pg-cluster-postgresql\n      disableExporter: true\n      replicas: 2\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n\n**Key Configuration Fields**:\n- `tls: true`: Enables TLS encryption for all connections\n- `issuer: KubeBlocks`: Uses KubeBlocks' built-in certificate authority (alternatively: `UserProvided` for custom certificates)\n\n## Verifying the Deployment\n\nMonitor the cluster status until it reaches the `Running` state:\n```bash\nkubectl get cluster pg-cluster -n d",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/07-tls/01-tls-overview",
    "description": "  # Deploying a PostgreSQL Cluster with TLS on KubeBlocks  This guide demonstrates how to deploy a PostgreSQL cluster with TLS encryption using KubeBlocks. Transport Layer Security (TLS) ensures secure communication between PostgreSQL clients and servers by encrypting data in transit, protecting sen"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_07-tls_02-tls-custom-cert",
    "title": "Deploy a PostgreSQL Cluster with Custom TLS Certificates on KubeBlocks",
    "content": "\n\n# Deploy a PostgreSQL Cluster with Custom TLS Certificates on KubeBlocks\n\nThis guide demonstrates how to deploy a PostgreSQL cluster with **custom TLS certificates** using KubeBlocks. By providing your own certificates, you maintain complete control over the security configuration for encrypted client-server communication.\n\n## Prerequisites\n\n\n\n## Generate Certificates\n\nGenerate the required certificates using OpenSSL:\n\n1. **Root Certificate (CA)**\n```bash\n# Generate CA private key (password protected)\nopenssl genrsa -aes256 -out ca-key.pem 4096\n\n# Create self-signed root certificate (10-year validity)\nopenssl req -x509 -new -nodes -key ca-key.pem -sha256 -days 3650 -out ca.pem\n# Enter certificate details (e.g., Common Name = \"PostgreSQL Root CA\")\n```\n\n2. **Server Certificate**\n```bash\n# Generate server private key\nopenssl genrsa -out server-key.pem 4096\n\n# Create Certificate Signing Request\nopenssl req -new -key server-key.pem -out server-req.pem\n# Enter server details (Common Name must match PostgreSQL server address)\n\n# Sign server certificate with CA (10-year validity)\nopenssl x509 -req -in server-req.pem -CA ca.pem -CAkey ca-key.pem \\\n  -CAcreateserial -out server-cert.pem -days 3650 -sha256\n```\n\n:::note\n\nThe Common Name (CN) must match your PostgreSQL server address (e.g., service name `pg-cluster-postgresql-postgresql`).\n\n:::\n\n3. **Verify Certificates**\n```bash\nopenssl verify -CAfile ca.pem server-cert.pem\n# Example Output: server-cert.pem: OK\n```\n\n## Create Kubernetes Secret\n\nStore certificates in a Kubernetes Secret for cluster access:\n\n```bash\nkubectl create secret generic postgresql-tls-secret \\\n  --namespace=demo \\\n  --from-file=ca.crt=ca.pem \\\n  --from-file=tls.crt=server-cert.pem \\\n  --from-file=tls.key=server-key.pem \\\n  --type=kubernetes.io/tls\n```\n\n## Deploy PostgreSQL Cluster\n\nDeploy a 2-node PostgreSQL cluster (1 primary, 1 replica) with TLS:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: pg-cluster\n  namespace: demo\ns",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/07-tls/02-tls-custom-cert",
    "description": "  # Deploy a PostgreSQL Cluster with Custom TLS Certificates on KubeBlocks  This guide demonstrates how to deploy a PostgreSQL cluster with **custom TLS certificates** using KubeBlocks. By providing your own certificates, you maintain complete control over the security configuration for encrypted cl"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for PostgreSQL Clusters with the Prometheus Operator",
    "content": "\n\n# PostgreSQL Monitoring with Prometheus Operator\n\nThis guide demonstrates how to configure comprehensive monitoring for PostgreSQL clusters in KubeBlocks using:\n\n1. Prometheus Operator for metrics collection\n2. Built-in PostgreSQL exporter for metrics exposure\n3. Grafana for visualization\n\n## Prerequisites\n\n\n\n## Install Monitoring Stack\n\n### 1. Install Prometheus Operator\nDeploy the kube-prometheus-stack using Helm:\n\n```bash\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  -n monitoring \\\n  --create-namespace\n```\n\n### 2. Verify Installation\nCheck all components are running:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-jmxg8                1/1     Running   0          2m1s\n```\n\n\n## Deploy a PostgreSQL Cluster\n\n\n\n**Key Monitoring Configuration**\n- `disableExporter: false` enables the built-in metrics exporter\n- Exporter runs as sidecar container in each PostgreSQL pod\n- Scrapes PostgreSQL metrics on port 9187\n\n## Verifying the Deployment\nMonitor the cluster status until it transitions to the Running state",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/08-monitoring/01-integrate-with-prometheus-operator",
    "description": "  # PostgreSQL Monitoring with Prometheus Operator  This guide demonstrates how to configure comprehensive monitoring for PostgreSQL clusters in KubeBlocks using:  1. Prometheus Operator for metrics collection 2. Built-in PostgreSQL exporter for metrics exposure 3. Grafana for visualization  ## Prer"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql__tpl__create-pg-replication-cluster",
    "title": "_create-pg-replication-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing PostgreSQL clusters. Below is an example configuration for deploying a PostgreSQL cluster with 2 replicas (1 primary, 1 replicas).\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: pg-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: postgresql\n  topology: replication\n  componentSpecs:\n    - name: postgresql\n      serviceVersion: 16.4.0\n      labels:\n        apps.kubeblocks.postgres.patroni/scope: pg-cluster-postgresql\n      disableExporter: true\n      replicas: 2\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/_tpl/_create-pg-replication-cluster",
    "description": "KubeBlocks uses a declarative approach for managing PostgreSQL clusters. Below is an example configuration for deploying a PostgreSQL cluster with 2 replicas (1 primary, 1 replicas).  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps.kubeblocks.io/v1 kind: Clust"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-postgresql__tpl__verify-pg-replication-cluster",
    "title": "_verify-pg-replication-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster pg-cluster -n demo -w\n```\n\nExpected Output:\n\n```bash\nNAME         CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\npg-cluster   postgresql           Delete               Creating   50s\npg-cluster   postgresql           Delete               Running    4m2s\n```\nOnce the cluster status becomes Running, your PostgreSQL cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::",
    "path": "docs/release-1_0/kubeblocks-for-postgresql/_tpl/_verify-pg-replication-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster pg-cluster -n demo -w ```  Expected Output:  ```bash NAME         CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE pg-cluster   postgresql           Delete               Creating   50s pg-cluste"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant_04-operations_01-stop-start-restart",
    "title": "Qdrant Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# Qdrant Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a Qdrant Cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a Qdrant Cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Retains persistent storage (PVCs)\n3. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\nOption 1: OpsRequest API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: qdrant-cluster-stop-ops\n  namespace: demo\nspec:\n  clusterName: qdrant-cluster\n  type: Stop\n```\n\n\n\nOption 2: Cluster API Patch\n\nModify the cluster spec directly by patching the stop field:\n\n\n```bash\nkubectl patch cluster qdrant-cluster -n demo --type='json' -p='[\n\n]'\n```\n\n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster qdrant-cluster -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME             CLUSTER-DEFINITION  TERMINATION-POLICY   STATUS     AGE\n    qdrant-cluster   qdrant              Delete               Stopping   6m3s\n    qdrant-cluster   qdrant              Delete               Stopped    6m55s\n    ```\n\n2. Verify no running po",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/04-operations/01-stop-start-restart",
    "description": "  # Qdrant Cluster Lifecycle Management  This guide demonstrates how to manage a Qdrant Cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help optimize resource usage "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a Qdrant Cluster",
    "content": "\n\n\n\n# Vertical Scaling for Qdrant Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a Qdrant Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for Qdrant instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks ensures minimal impact during scaling operations by following a controlled, role-aware update strategy:\n**Role-Aware Replicas (Primary/Secondary Replicas)**\n- Secondary replicas update first – Non-leader pods are upgraded to minimize disruption.\n- Primary updates last – Only after all secondaries are healthy does the primary pod restart.\n- Cluster state progresses from Updating → Running once all replicas are stable.\n\n**Role-Unaware Replicas (Ordinal-Based Scaling)**\nIf replicas have no defined roles, updates follow Kubernetes pod ordinal order:\n- Highest ordinal first (e.g., pod-2 → pod-1 → pod-0) to ensure deterministic rollouts.\n\n## Prerequisites\n\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Pods are updated in pod ordinal order, from highest to lowest, (e.g., pod-2 → pod-1 → pod-0)\n1. Cluster status transitions from `Updating` to `Running`\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the qdrant component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: qdrant-cluster-vscale-ops\n    namespace: demo\n  spec:\n    clusterName: qdrant-cluster\n    type: VerticalScaling\n    verticalScaling:\n    - componentName: qdrant\n      requests:\n        cpu: '1'\n        memory: 1Gi\n      limits:\n        cpu: '1'\n        memory: 1Gi\n  ```\n\n ",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for Qdrant Clusters with KubeBlocks  This guide demonstrates how to vertically scale a Qdrant Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies compute resources (CPU and memo"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of Qdrant Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for Qdrant Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a Qdrant cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running`.\n2. Cluster status changes from `Updating` to `Running`\n\n:::note\n\nQdrant uses the **Raft consensus protocol** to maintain consistency regarding the cluster topology and the collections structure.\nBetter to have an odd number of replicas, such as 3, 5, 7, to avoid split-brain scenarios, after scaling out/in the cluster.\n\n:::\n\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the Qdrant cluster by adding 1 replica to qdrant component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: qdrant-cluster-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: qdrant-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: qdrant\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops qdrant-cluster-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME                           TYPE                CLUSTER          STATUS    PROGRESS   AGE\n  qdrant-cluster-scale-out-ops   HorizontalScaling   qdrant-cluster   Running   0/1        9s\n  qdrant-cluster-scale-out-ops   HorizontalScaling   qdrant-cluster   Running   1/1        16s\n  qdrant-cluster-scale-out-ops   HorizontalScaling   qdrant-cluster   Succeed   1/1        16s\n  ```\n  \n\n\n  Option 2: Direct Cluster",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for Qdrant Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a Qdrant cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequisites   "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a Qdrant Cluster",
    "content": "\n\n\n\n# Expanding Volume in a Qdrant Cluster\n\nThis guide explains how to expand Persistent Volume Claims (PVCs) in a Qdrant cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the underlying storage class, this operation can be performed without downtime.\n\nVolume expansion allows you to increase the size of a Persistent Volume Claim (PVC) after it has been created. This feature was introduced in Kubernetes v1.11 and became generally available (GA) in Kubernetes v1.24.\n\n## Prerequisites\n\n\n\n### Check the Storage Class for Volume Expansion Support\n\nList all available storage classes and verify if volume expansion is supported by checking the `ALLOWVOLUMEEXPANSION` field:\n```bash\nkubectl get storageclass\n```\n\nExample Output:\n```bash\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4d10h\nkb-default-sc       ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   3d7h\nsc-s3-repo-2qsxfh   ru.yandex.s3.csi        Retain          Immediate              false                  3d7h\n```\nEnsure the storage class you are using has `ALLOWVOLUMEEXPANSION` set to true. If it is false, the storage class does not support volume expansion.\n\n## Deploy a Qdrant Cluster with StorageClass\n\nKubeBlocks uses a declarative approach to manage Qdrant clusters. Below is an example configuration for deploying a Qdrant cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: qdrant-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: qdrant\n  topology: cluster\n  componentSpecs:\n    - name: qdrant\n      serviceVersion: 1.10.0\n      replicas: 3\n      resources:\n     ",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a Qdrant Cluster  This guide explains how to expand Persistent Volume Claims (PVCs) in a Qdrant cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the un"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy Qdrant Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage Qdrant Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing Qdrant services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## View Network Services\nList the Services created for the Qdrant cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=qdrant-cluster -n demo\n```\n\nExample Services:\n```bash\nNAME                           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\nqdrant-cluster-qdrant-qdrant   ClusterIP   10.96.111.81           6333/TCP,6334/TCP   28m\n```\n\n## Expose Qdrant Service\n\nExternal service addresses enable public internet access to Qdrant, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the Qdrant service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: qdrant-cluster-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: qdrant-cluster\n    expose:\n    - componentName: qdrant\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options are 'ClusterIP', 'NodePort', and 'LoadBalancer'.\n        serviceType: LoadBalancer\n        # Contains cloud provider related parameters if ServiceType is LoadBala",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/04-operations/05-manage-loadbalancer",
    "description": "    # Manage Qdrant Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing Qdrant services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant_04-operations_06-minior-version-upgrade",
    "title": "Upgrading the Minor Version of a Qdrant Cluster in KubeBlocks",
    "content": "\n\n# Upgrading the Minor Version of a Qdrant Cluster in KubeBlocks\n\nThis guide walks you through the deployment and minor version upgrade of a Qdrant Cluster managed by KubeBlocks, ensuring minimal downtime during the process.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Deploy a Qdrant Cluster\n\nKubeBlocks uses a declarative approach for managing Qdrant Clusters. Below is an example configuration for deploying a Qdrant Cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: qdrant-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: qdrant\n  topology: cluster\n  componentSpecs:\n    - name: qdrant\n      serviceVersion: 1.10.0\n      replicas: 3\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n\n## Verifying the Deployment\nMonitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster qdrant-cluster -n demo -w\n```\n\nExample Output:\n\n```bash\nNAME             CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nqdrant-cluster   qdrant               Delete               Creating   49",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/04-operations/06-minior-version-upgrade",
    "description": "  # Upgrading the Minor Version of a Qdrant Cluster in KubeBlocks  This guide walks you through the deployment and minor version upgrade of a Qdrant Cluster managed by KubeBlocks, ensuring minimal downtime during the process.  ## Prerequisites  Before proceeding, ensure the following: - Environment "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed Qdrant Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed Qdrant Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in Qdrant clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\nTo decommission a specific Pod (e.g., 'qdrant-cluster-qdrant-1'), you can use one of the following methods:\n\n\n\n  Option 1: Using OpsRequest\n\n  Create an OpsRequest to mark the Pod as offline:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: qdrant-cluster-decommission-ops\n    namespace: demo\n  spec:\n    clusterName: qdrant-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: qdrant\n      scaleIn:\n        onlineInstancesToOffline:\n          - 'qdrant-cluster-qdrant-1'  # Specifies the instance names that need to be taken offline\n  ```\n\n  #### Monitor the Decommissioni",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed Qdrant Clusters  This guide explains how to decommission (take offline) specific Pods in Qdrant clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workloa"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant_05-backup-restore_01-create-backuprepo",
    "title": "Create a Backup Repository for KubeBlocks",
    "content": "\n# Create a BackupRepo for KubeBlocks\n\nThis guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.\n\n## Prerequisites\n- AWS CLI configured with appropriate permissions to create S3 buckets.\n- kubectl access to your Kubernetes cluster.\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks)) and running in the kb-system namespace.\n\n## Step 1: Create S3 Bucket\n\nUse the AWS CLI to create an S3 bucket in your desired region. Replace `` with your target AWS region (e.g., `us-east-1`, `ap-southeast-1`).\n\n```bash\n aws s3api create-bucket --bucket kubeblocks-backup-repo --region  --create-bucket-configuration LocationConstraint=\n```\n\nExample (for us-west-1):\n```bash\naws s3api create-bucket \\\n  --bucket kubeblocks-backup-repo \\\n  --region us-west-1 \\\n  --create-bucket-configuration LocationConstraint=us-west-1\n```\n\nExample Output:\n\n```json\n\n```\n\nVerification:\nConfirm the bucket was created by listing its contents (it will be empty initially):\n\n```bash\naws s3 ls s3://kubeblocks-backup-repo\n```\n\n## Step 2: Create a Kubernetes Secret for AWS Credentials\n\nStore your AWS credentials securely in a Kubernetes Secret. Replace `` and `` with your actual AWS credentials:\n\n```bash\n# Create a secret to save the access key\nkubectl create secret generic s3-credential-for-backuprepo \\\n  --from-literal=accessKeyId= \\\n  --from-literal=secretAccessKey= \\\n  -n kb-system\n```\n\n## Step 3: Configure Backup Repository\n\nA BackupRepo is a custom resource that defines a storage repository for backups. In this step, you'll integrate your S3 bucket with KubeBlocks by creating a BackupRepo resource.\n\nApply the following YAML to create the BackupRepo. Replace fields(e.g., bucket name, region) with your specific settings.\n\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupRepo\nmetadata:\n  name: s3-repo\n  annotations:\n    # mark this backuprepo as default one\n    dataprotection.kubeblocks.io/is",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/05-backup-restore/01-create-backuprepo",
    "description": " # Create a BackupRepo for KubeBlocks  This guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.  ## Prerequisites - AWS CLI configured with appropriate permissions to create S3 buckets. - kubectl access to your Kubernetes cluster. -"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant_05-backup-restore_02-create-full-backup",
    "title": "Create a Full Backup for a Qdrant Cluster on KubeBlocks",
    "content": "\n\n\n# Create a Full Backup for Qdrant on KubeBlocks\n\nThis guide demonstrates how to create and validate full backups for Qdrant clusters on KubeBlocks using the `pg-basebackup` method through both:\n- The Backup API (direct backup operations)\n- The OpsRequest API (managed backup operations with enhanced monitoring)\n\nWe will cover how to restore data from a backup in the [Restore From Full Backup](./05-restoring-from-full-backup) guide.\n\n## Prerequisites\n\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Backup Prerequisites\n\nBefore creating backups, ensure:\n1. Backup repository is configured:\n   - `BackupRepo` resource exists\n   - Network connectivity between cluster and repository\n   - `BackupRepo` status shows \"Ready\"\n\n2. Cluster is ready:\n   - Cluster status is \"Running\"\n   - No ongoing operations (scaling, upgrades, etc.)\n\n## Identify Backup Configuration\n\nCheck available backup policies and schedules:\n\n```bash\n# List backup policies\nkubectl get backuppolicy -n demo -l app.kubernetes.io/instance=qdrant-cluster\n\n# List backup schedules\nkubectl get backupschedule -n demo -l app.kubernetes.io/instance=qdrant-cluster\n```\n\nExpected Output:\n```bash\nNAME                                  BACKUP-REPO   STATUS      AGE\nqdrant-cluster-qdrant-backup-policy                 Available   36m\n\nNAME                                    STATUS      AGE\nqdrant-cluster-qdrant-backup-schedule   Available   36m\n```\n\nView supported backup methods in the BackupPolicy CR 'qdrant-cluster-qdrant-backup-policy':\n\n```bash\nkubectl get backuppolicy qdrant-cluster-qdrant-backup-policy -n demo -oyaml | yq '.spec.backupMethods[].name'\n```\n**List of Backup methods**\n\nKubeBlocks Qdrant supports these backup methods:\n\n| Feature           | Method          | Description |\n|-------------|--------|------------|\n| Full Backup | datafile | uses HTTP API `snapshot` to create snapshot for all collections. |\n\n## Backup via Backup API\n\n### 1. Create On-Demand Backup\n\nThe `datafile` method backup ",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/05-backup-restore/02-create-full-backup",
    "description": "   # Create a Full Backup for Qdrant on KubeBlocks  This guide demonstrates how to create and validate full backups for Qdrant clusters on KubeBlocks using the `pg-basebackup` method through both: - The Backup API (direct backup operations) - The OpsRequest API (managed backup operations with enhanc"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant_05-backup-restore_03-scheduled-full-backup",
    "title": "Setting Up a Qdrant Cluster with Scheduled Backups in KubeBlocks",
    "content": "\n\n# Setting Up a Qdrant Cluster with Scheduled Backups in KubeBlocks\n\nThis guide demonstrates how to deploy a Qdrant cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.\n\n## Prerequisites\n\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Prerequisites for Backup\n\n1. Backup Repository Configured:\n   - Configured `BackupRepo`\n   - Network connectivity between cluster and repo, `BackupRepo` status is `Ready`\n\n2. Cluster is Running:\n   - Cluster must be in `Running` state\n   - No ongoing operations (scaling, upgrades etc.)\n\n## Configure Scheduled Backups\n\nKubeBlocks automatically creates a `BackupSchedule` resource when the cluster is created. Follow these steps to enable and configure scheduled backups:\n\n1. Verify the default backup schedule configuration:\n\n```bash\nkubectl get backupschedule qdrant-cluster-qdrant-backup-schedule  -n demo -oyaml\n```\n\nExample Output:\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupSchedule\nspec:\n  backupPolicyName: qdrant-cluster-Qdrant-backup-policy\n  schedules:\n  - backupMethod: datafile\n    # ┌───────────── minute (0-59)\n    # │ ┌───────────── hour (0-23)\n    # │ │ ┌───────────── day of month (1-31)\n    # │ │ │ ┌───────────── month (1-12)\n    # │ │ │ │ ┌───────────── day of week (0-6) (Sunday=0)\n    # │ │ │ │ │\n    # 0 18 * * *\n    # schedule this job every day at 6:00 PM (18:00).\n    cronExpression: 0 18 * * * # update the cronExpression to your need\n    enabled: true # set to `true` to schedule base backup periodically\n    retentionPeriod: 7d # set the retention period to your need\n```\n\n2. Enable and customize the backup schedule:\n```bash\nkubectl edit backupschedule qdrant-cluster-qdrant-backup-schedule -n demo\n```\n\nUpdate these key parameters:\n- `enabled`: Set to `true` to activate scheduled backups\n- `cronExpression`: Configure backup frequency using cron syntax\n- `retentionPeriod`: Set how long to keep backups (e.g., `7d`, `1mo`)\n\nExample configuration",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/05-backup-restore/03-scheduled-full-backup",
    "description": "  # Setting Up a Qdrant Cluster with Scheduled Backups in KubeBlocks  This guide demonstrates how to deploy a Qdrant cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.  ## Prerequisites    ## Deploy a Qdrant Cluster    ## Verifying the Deployment    ## Prere"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant_05-backup-restore_05-restoring-from-full-backup",
    "title": "Restore a Qdrant Cluster from Backup",
    "content": "\n\n# Restore a Qdrant Cluster from Backup\n\nThis guide demonstrates two methods to restore a Qdrant cluster from backup in KubeBlocks:\n\n1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations\n2. **OpsRequest API Method** - Enhanced operational control with progress monitoring\n\n## Prerequisites\n\n\n\n## Preparing for Restoration: Locate one Full Backup\nBefore restoring, ensure that there is a full backup available. The restoration process will use this backup to create a new Qdrant cluster.\n\n- Backup repository accessible from new cluster\n- Valid full backup in `Completed` state\n- Adequate CPU/memory resources\n- Sufficient storage capacity\n\nFind available full backups:\n\n```bash\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Full,app.kubernetes.io/instance=qdrant-cluster # get the list of full backups\n```\n\nPick ONE of the Backups whose status is `Completed`.\n\n## Option 1: Cluster Annotation Restoration\n\n### Step 1: Create Restored Cluster\nCreate a new cluster with restore configuration:\n\nKey parameters:\n- `kubeblocks.io/restore-from-backup` annotation\n- Backup name and namespace located from the previous steps\n\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: qdrant-cluster-restored\n  namespace: demo\n  annotations:\n    # NOTE: replace  with your backup\n    kubeblocks.io/restore-from-backup: '}'\nspec:\n  terminationPolicy: Delete\n  clusterDef: qdrant\n  topology: cluster\n  componentSpecs:\n    - name: qdrant\n      serviceVersion: 1.10.0\n      replicas: 3\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n\n### Step 2: Monitor Restoration\nTrack restore progress with:\n\n```bash\n# ",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/05-backup-restore/05-restoring-from-full-backup",
    "description": "  # Restore a Qdrant Cluster from Backup  This guide demonstrates two methods to restore a Qdrant cluster from backup in KubeBlocks:  1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations 2. **OpsRequest API Method** - Enhanced operational control with progress monito"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for Qdrant Clusters with the Prometheus Operator",
    "content": "\n\n# Qdrant Monitoring with Prometheus Operator\n\nThis guide demonstrates how to configure comprehensive monitoring for Qdrant clusters in KubeBlocks using:\n\n1. Prometheus Operator for metrics collection\n2. Built-in Qdrant exporter for metrics exposure\n3. Grafana for visualization\n\n## Prerequisites\n\n\n\n## Install Monitoring Stack\n\n### 1. Install Prometheus Operator\nDeploy the kube-prometheus-stack using Helm:\n\n```bash\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  -n monitoring \\\n  --create-namespace\n```\n\n### 2. Verify Installation\nCheck all components are running:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-jmxg8                1/1     Running   0          2m1s\n```\n\n\n## Deploy a Qdrant Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Configure Metrics Collection\n\n### 1. Verify Exporter Endpoint\n\n```bash\nkubectl -n demo exec -it pods/qdrant-cluster-qdrant-0 -c kbagent -- \\\n  curl -s http://127.0.0.1:6333/metrics | head -n 50\n```\n\n### 2. Create PodMonitor\n```yaml\napiVersion: monitoring.coreos.com/v1\nkind: PodM",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/08-monitoring/01-integrate-with-prometheus-operator",
    "description": "  # Qdrant Monitoring with Prometheus Operator  This guide demonstrates how to configure comprehensive monitoring for Qdrant clusters in KubeBlocks using:  1. Prometheus Operator for metrics collection 2. Built-in Qdrant exporter for metrics exposure 3. Grafana for visualization  ## Prerequisites   "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant__tpl__create-cluster",
    "title": "_create-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing Qdrant Clusters.\nBelow is an example configuration for deploying a Qdrant Cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: qdrant-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: qdrant\n  topology: cluster\n  componentSpecs:\n    - name: qdrant\n      serviceVersion: 1.10.0\n      replicas: 3\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/_tpl/_create-cluster",
    "description": "KubeBlocks uses a declarative approach for managing Qdrant Clusters. Below is an example configuration for deploying a Qdrant Cluster with 3 replicas.  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps.kubeblocks.io/v1 kind: Cluster metadata:   name: qdrant-clus"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-qdrant__tpl__verify-cluster",
    "title": "_verify-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster qdrant-cluster -n demo -w\n```\n\nExpected Output:\n\n```bash\nkubectl get cluster qdrant-cluster -n demo\nNAME             CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nqdrant-cluster   qdrant              Delete               Creating   49s\nqdrant-cluster   qdrant              Delete               Running    62s\n```\n\nCheck the pod status and roles:\n```bash\nkubectl get pods -l app.kubernetes.io/instance=qdrant-cluster -n demo\n```\n\nExpected Output:\n```bash\nNAME                      READY   STATUS    RESTARTS   AGE\nqdrant-cluster-qdrant-0   2/2     Running   0          1m43s\nqdrant-cluster-qdrant-1   2/2     Running   0          1m28s\nqdrant-cluster-qdrant-2   2/2     Running   0          1m14s\n```\n\nOnce the cluster status becomes Running, your Qdrant cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::\n",
    "path": "docs/release-1_0/kubeblocks-for-qdrant/_tpl/_verify-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster qdrant-cluster -n demo -w ```  Expected Output:  ```bash kubectl get cluster qdrant-cluster -n demo NAME             CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE qdrant-cluster   qdrant     "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-rabbitmq_04-operations_01-stop-start-restart",
    "title": "RabbitMQ  Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# RabbitMQ  Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a RabbitMQ  Cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a RabbitMQ  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a RabbitMQ  Cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Retains persistent storage (PVCs)\n3. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\nOption 1: OpsRequest API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: rabbitmq-cluster-stop-ops\n  namespace: demo\nspec:\n  clusterName: rabbitmq-cluster\n  type: Stop\n```\n\n\n\nOption 2: Cluster API Patch\n\nModify the cluster spec directly by patching the stop field:\n\n```bash\nkubectl patch cluster rabbitmq-cluster -n demo --type='json' -p='[\n\n]'\n```\n\n\n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster rabbitmq-cluster -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME               CLUSTER-DEFINITION    TERMINATION-POLICY   STATUS     AGE\n    rabbitmq-cluster   rabbitmq              Delete               Stopping   6m3s\n    rabbitmq-cluster   rabbitmq              Delete               Stopped    6m55s\n",
    "path": "docs/release-1_0/kubeblocks-for-rabbitmq/04-operations/01-stop-start-restart",
    "description": "  # RabbitMQ  Cluster Lifecycle Management  This guide demonstrates how to manage a RabbitMQ  Cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help optimize resource "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-rabbitmq_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a RabbitMQ  Cluster",
    "content": "\n\n\n\n# Vertical Scaling for RabbitMQ  Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a RabbitMQ  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for RabbitMQ instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks ensures minimal impact during scaling operations by following a controlled, role-aware update strategy:\n**Role-Aware Replicas (Primary/Secondary Replicas)**\n- Secondary replicas update first – Non-leader pods are upgraded to minimize disruption.\n- Primary updates last – Only after all secondaries are healthy does the primary pod restart.\n- Cluster state progresses from Updating → Running once all replicas are stable.\n\n**Role-Unaware Replicas (Ordinal-Based Scaling)**\nIf replicas have no defined roles, updates follow Kubernetes pod ordinal order:\n- Highest ordinal first (e.g., pod-2 → pod-1 → pod-0) to ensure deterministic rollouts.\n\n## Prerequisites\n\n\n\n## Deploy a RabbitMQ  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Pods are updated in pod ordinal order, from highest to lowest, (e.g., pod-2 → pod-1 → pod-0)\n1. Cluster status transitions from `Updating` to `Running`\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the rabbitmq component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: rabbitmq-cluster-vscale-ops\n    namespace: demo\n  spec:\n    clusterName: rabbitmq-cluster\n    type: VerticalScaling\n    verticalScaling:\n    - componentName: rabbitmq\n      requests:\n        cpu: '1'\n        memory: 1Gi\n      limits:\n        cpu: '1'\n        m",
    "path": "docs/release-1_0/kubeblocks-for-rabbitmq/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for RabbitMQ  Clusters with KubeBlocks  This guide demonstrates how to vertically scale a RabbitMQ  Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies compute resources (CPU an"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-rabbitmq_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of RabbitMQ Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for RabbitMQ Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a RabbitMQ cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a RabbitMQ  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running`.\n2. Cluster status changes from `Updating` to `Running`\n\n:::note\n\nRabbitMQ quorum queue are designed based on the **Raft consensus algorithm**.\nBetter to have an odd number of replicas, such as 3, 5, 7, to avoid split-brain scenarios, after scaling out/in the cluster.\n\n:::\n\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the RabbitMQ cluster by adding 1 replica to rabbitmq component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: rabbitmq-cluster-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: rabbitmq-cluster\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: rabbitmq\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops rabbitmq-cluster-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME                             TYPE                CLUSTER          STATUS    PROGRESS   AGE\n  rabbitmq-cluster-scale-out-ops   HorizontalScaling   rabbitmq-cluster   Running   0/1        9s\n  rabbitmq-cluster-scale-out-ops   HorizontalScaling   rabbitmq-cluster   Running   1/1        16s\n  rabbitmq-cluster-scale-out-ops   HorizontalScaling   rabbitmq-cluster   Succeed   1/1        16s\n  ```\n  \n\n\n  Option 2: Direct Cluster API Update\n\n  Alte",
    "path": "docs/release-1_0/kubeblocks-for-rabbitmq/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for RabbitMQ Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a RabbitMQ cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequisite"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-rabbitmq_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a RabbitMQ Cluster",
    "content": "\n\n\n\n# Expanding Volume in a RabbitMQ Cluster\n\nThis guide explains how to expand Persistent Volume Claims (PVCs) in a RabbitMQ cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the underlying storage class, this operation can be performed without downtime.\n\nVolume expansion allows you to increase the size of a Persistent Volume Claim (PVC) after it has been created. This feature was introduced in Kubernetes v1.11 and became generally available (GA) in Kubernetes v1.24.\n\n## Prerequisites\n\n\n\n### Check the Storage Class for Volume Expansion Support\n\nList all available storage classes and verify if volume expansion is supported by checking the `ALLOWVOLUMEEXPANSION` field:\n```bash\nkubectl get storageclass\n```\n\nExample Output:\n```bash\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4d10h\nkb-default-sc       ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   3d7h\nsc-s3-repo-2qsxfh   ru.yandex.s3.csi        Retain          Immediate              false                  3d7h\n```\nEnsure the storage class you are using has `ALLOWVOLUMEEXPANSION` set to true. If it is false, the storage class does not support volume expansion.\n\n## Deploy a RabbitMQ  Cluster with StorageClass\n\nKubeBlocks uses a declarative approach to manage RabbitMQ clusters. Below is an example configuration for deploying a RabbitMQ cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: rabbitmq-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: rabbitmq\n  topology: cluster\n  componentSpecs:\n    - name: rabbitmq\n      serviceVersion: 1.10.0\n      replicas: 3\n     ",
    "path": "docs/release-1_0/kubeblocks-for-rabbitmq/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a RabbitMQ Cluster  This guide explains how to expand Persistent Volume Claims (PVCs) in a RabbitMQ cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by th"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-rabbitmq_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy RabbitMQ Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage RabbitMQ Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing RabbitMQ services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n## Deploy a RabbitMQ  Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## View Network Services\nList the Services created for the RabbitMQ cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=rabbitmq-cluster -n demo\n```\n\nExample Services:\n```bash\nNAME                        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)              AGE\nrabbitmq-cluster-rabbitmq   ClusterIP   10.96.6.67           5672/TCP,15672/TCP   33m\n```\n\n## Expose RabbitMQ Service\n\nExternal service addresses enable public internet access to RabbitMQ, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the RabbitMQ service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: rabbitmq-cluster-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: rabbitmq-cluster\n    expose:\n    - componentName: rabbitmq\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options are 'ClusterIP', 'NodePort', and 'LoadBalancer'.\n        serviceType: LoadBalancer\n        ports:\n          - name: managment\n            port: 156",
    "path": "docs/release-1_0/kubeblocks-for-rabbitmq/04-operations/05-manage-loadbalancer",
    "description": "    # Manage RabbitMQ Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing RabbitMQ services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, ma"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-rabbitmq_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed RabbitMQ Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed RabbitMQ Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in RabbitMQ clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a RabbitMQ Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\n\nBefore decommissioning a specific pod from a component, make sure this component has more than one replicas.\nIf not, please scale out the component ahead.\n\nE.g. you can patch the cluster CR with command, to declare there are 3 replicas in component querynode.\n\n```bash\nkubectl patch cluster milvus-cluster -n demo --type='json' -p='[\n  \n]'\n```\n\n\nTo decommission a specific Pod (e.g., 'rabbitmq-cluster-rabbitmq-1'), you can use one of the following methods:\n\n\n\n  Option 1: Using OpsRequest\n\n  Create an OpsRequest to mark the Pod as offline:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: ra",
    "path": "docs/release-1_0/kubeblocks-for-rabbitmq/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed RabbitMQ Clusters  This guide explains how to decommission (take offline) specific Pods in RabbitMQ clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for wor"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-rabbitmq_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for RabbitMQ Clusters with the Prometheus Operator",
    "content": "\n\n# RabbitMQ Monitoring with Prometheus Operator\n\nThis guide demonstrates how to configure comprehensive monitoring for RabbitMQ clusters in KubeBlocks using:\n\n1. Prometheus Operator for metrics collection\n2. Built-in RabbitMQ exporter for metrics exposure\n3. Grafana for visualization\n\n## Prerequisites\n\n\n\n## Install Monitoring Stack\n\n### 1. Install Prometheus Operator\nDeploy the kube-prometheus-stack using Helm:\n\n```bash\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  -n monitoring \\\n  --create-namespace\n```\n\n### 2. Verify Installation\nCheck all components are running:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-jmxg8                1/1     Running   0          2m1s\n```\n\n\n## Deploy a RabbitMQ Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Configure Metrics Collection\n\n### 1. Verify Exporter Endpoint\n\n```bash\n# prot-forward\nkubectl -n demo port-forward pods/rabbitmq-cluster-rabbitmq-0  15692:15692\n# check metrics\ncurl -s http://127.0.0.1:15692/metrics | head -n 50\n```\n\n### 2. Create PodMonitor\n```yaml\napiV",
    "path": "docs/release-1_0/kubeblocks-for-rabbitmq/08-monitoring/01-integrate-with-prometheus-operator",
    "description": "  # RabbitMQ Monitoring with Prometheus Operator  This guide demonstrates how to configure comprehensive monitoring for RabbitMQ clusters in KubeBlocks using:  1. Prometheus Operator for metrics collection 2. Built-in RabbitMQ exporter for metrics exposure 3. Grafana for visualization  ## Prerequisi"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-rabbitmq__tpl__create-cluster",
    "title": "_create-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing RabbitMQ  Clusters.\nBelow is an example configuration for deploying a RabbitMQ  Cluster with 3 replicas.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: rabbitmq-cluster\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: rabbitmq\n  topology: clustermode\n  componentSpecs:\n    - name: rabbitmq\n      serviceVersion: 3.13.7\n      replicas: 3\n      resources:\n        limits:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n        requests:\n          cpu: \"0.5\"\n          memory: \"0.5Gi\"\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n",
    "path": "docs/release-1_0/kubeblocks-for-rabbitmq/_tpl/_create-cluster",
    "description": "KubeBlocks uses a declarative approach for managing RabbitMQ  Clusters. Below is an example configuration for deploying a RabbitMQ  Cluster with 3 replicas.  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps.kubeblocks.io/v1 kind: Cluster metadata:   name: rabbi"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-rabbitmq__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/release-1_0/kubeblocks-for-rabbitmq/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-rabbitmq__tpl__verify-cluster",
    "title": "_verify-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster rabbitmq-cluster -n demo -w\n```\n\nExpected Output:\n\n```bash\nkubectl get cluster rabbitmq-cluster -n demo\nNAME               CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\nrabbitmq-cluster   rabbitmq             Delete               Creating   15s\nrabbitmq-cluster   rabbitmq             Delete               Running    83s\n```\n\nCheck the pod status and roles:\n```bash\nkubectl get pods -l app.kubernetes.io/instance=rabbitmq-cluster -n demo\n```\n\nExpected Output:\n```bash\nNAME                          READY   STATUS    RESTARTS   AGE\nrabbitmq-cluster-rabbitmq-0   2/2     Running   0          106s\nrabbitmq-cluster-rabbitmq-1   2/2     Running   0          82s\nrabbitmq-cluster-rabbitmq-2   2/2     Running   0          47s\n```\n\nOnce the cluster status becomes Running, your RabbitMQ cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::\n",
    "path": "docs/release-1_0/kubeblocks-for-rabbitmq/_tpl/_verify-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster rabbitmq-cluster -n demo -w ```  Expected Output:  ```bash kubectl get cluster rabbitmq-cluster -n demo NAME               CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE rabbitmq-cluster   rab"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_03-topologies_01-standlone",
    "title": "Deploying a Redis Standalone Cluster with KubeBlocks",
    "content": "\n# Deploying a Redis Standalone Cluster with KubeBlocks\n\nA standalone Redis deployment consists of a single Redis server instance running independently without any replication or clustering. It is the simplest and most lightweight deployment model.\n\n**Use Cases**\n- Development & testing environments.\n- Small applications with low traffic.\n\n## Prerequisites\n\n\n\n## Deploying the Redis Standalone Cluster\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: redis-standalone\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: redis    # set to reids\n  topology: standalone # set topology to standalone\n  componentSpecs:\n  - name: redis\n    replicas: 1       # set replica to 1\n    serviceVersion: 7.2.4\n    resources:\n      limits:\n        cpu: \"0.5\"\n        memory: \"0.5Gi\"\n      requests:\n        cpu: \"0.5\"\n        memory: \"0.5Gi\"\n    volumeClaimTemplates:\n      - name: data\n        spec:\n          accessModes:\n            - ReadWriteOnce\n          resources:\n            requests:\n              storage: 20Gi\n```\n\n**Key Configuration Details**:\n- `clusterDef: redis`: Specifies the ClusterDefinition CR for the cluster.\n- `topology: standalone`: Configures the cluster to use standalone topology.\n- `componentSpecs`: Defines the components in the cluster:\n  - Component 'redis':\n    - `serviceVersion: 7.2.4`: Specifies the version of the Redis service to be deployed.\n\n\n## Verifying the Deployment\n\n### Check the Cluster Status\nOnce the cluster is deployed, check its status:\n```bash\nkubectl get cluster redis-standalone  -n demo -w\n```\nExpected Output:\n```bash\nNAME               CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS    AGE\nredis-standalone   redis                Delete               Running   34s\n```\n\n### Verify Component Status\n```bash\nkubectl get component redis-standalone-redis -n demo\n```\nExpected Output:\n```bash\nNAME                     DEFINITION              SERVICE-VERSION   STATUS    AGE\nredis-standalone-redis   redis-7-1.0.0   ",
    "path": "docs/release-1_0/kubeblocks-for-redis/03-topologies/01-standlone",
    "description": " # Deploying a Redis Standalone Cluster with KubeBlocks  A standalone Redis deployment consists of a single Redis server instance running independently without any replication or clustering. It is the simplest and most lightweight deployment model.  **Use Cases** - Development & testing environments"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_03-topologies_02-replication",
    "title": "Deploying a Redis Replication Cluster with KubeBlocks",
    "content": "\n# Deploying a Redis Replication Cluster with KubeBlocks\n\nRedis Replication involves a primary (master) node that handles writes and one or more replica (slave) nodes that replicate data from the master for read scaling and failover.\n\n**Use Cases**\n- Read-heavy applications (e.g., analytics workload).\n- High-availability setups with Redis Sentinel for automatic failover.\n\n## Prerequisites\n\n\n\n## Deploying the Redis Replication Cluster\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: redis-replication\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: redis\n  topology: replication\n  componentSpecs:\n    - name: redis\n      serviceVersion: \"7.2.4\"\n      disableExporter: false\n      replicas: 2\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n    - name: redis-sentinel\n      replicas: 3\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n\n**Key Configuration Details**:\n- `clusterDef: redis`: Specifies the ClusterDefinition CR for the cluster.\n- `topology: replication`: Configures the cluster to use replication topology.\n- `componentSpecs`: Defines the components in the cluster:\n  - Component 'redis':\n    - `serviceVersion: 7.2.4`: Specifies the version of the Redis service to be deployed.\n  - Component 'redis-sentinel':\n    - Redis Sentinel is a high availability solution for Redis. Recommended to ",
    "path": "docs/release-1_0/kubeblocks-for-redis/03-topologies/02-replication",
    "description": " # Deploying a Redis Replication Cluster with KubeBlocks  Redis Replication involves a primary (master) node that handles writes and one or more replica (slave) nodes that replicate data from the master for read scaling and failover.  **Use Cases** - Read-heavy applications (e.g., analytics workload"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_03-topologies_03-sharding",
    "title": "Deploying a Redis Sharding Cluster with KubeBlocks",
    "content": "\n# Deploying a Redis Sharding Cluste (Cluster Mode) with KubeBlocks\n\nRedis Cluster distributes data across multiple nodes (shards) using hash-based partitioning, allowing horizontal scaling for both reads and writes.\n\n**Use Cases**\n- Large-scale applications requiring high throughput.\n- Distributed caching and session storage.\n- Write-heavy workloads (e.g., real-time analytics).\n\n## Prerequisites\n\n\n\n## Deploying the Redis Sharding Cluster\n\nTo create a redis sharding cluster (cluster mode)  with 3 shards, and 2 replica for each shard:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: redis-sharding\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  shardings:\n  - name: shard\n    shards: 3\n    template:\n      name: redis\n      componentDef: redis-cluster-7\n      disableExporter: true\n      replicas: 2\n      resources:\n        limits:\n          cpu: '1'\n          memory: 1.1Gi\n        requests:\n          cpu: '1'\n          memory: 1.1Gi\n      serviceVersion: 7.2.4\n      volumeClaimTemplates:\n      - name: data\n        spec:\n          accessModes:\n          - ReadWriteOnce\n          resources:\n            requests:\n              storage: 20Gi\n      services:\n      - name: redis-advertised # This is a per-pod svc, and will be used to parse advertised endpoints\n        podService: true\n        #  - NodePort\n        #  - LoadBalancer\n        serviceType: NodePort\n```\n\n**Key Configuration Details**:\n- `shardings`: Specifies a list of ShardingSpec objects that configure the sharding topology for components of a Cluster.\n\n## Verifying the Deployment\n\n### Check the Cluster Status\nOnce the cluster is deployed, check its status:\n```bash\nkubectl get cluster redis-sharding  -n demo -w\n```\nExpected Output:\n```bash\nNAME             CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS    AGE\nredis-sharding                        Delete               Running   103s\n```\n\n### Verify Component and Pod Status\n\nGet all componets working for this cluster:\n```bash\nkub",
    "path": "docs/release-1_0/kubeblocks-for-redis/03-topologies/03-sharding",
    "description": " # Deploying a Redis Sharding Cluste (Cluster Mode) with KubeBlocks  Redis Cluster distributes data across multiple nodes (shards) using hash-based partitioning, allowing horizontal scaling for both reads and writes.  **Use Cases** - Large-scale applications requiring high throughput. - Distributed "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_04-operations_01-stop-start-restart",
    "title": "Redis Replication Cluster Lifecycle Management (Stop, Start, Restart)",
    "content": "\n\n# Redis Replication Cluster Lifecycle Management\n\nThis guide demonstrates how to manage a Redis Replication Cluster's operational state in **KubeBlocks**, including:\n\n- Stopping the cluster to conserve resources\n- Starting a stopped cluster\n- Restarting cluster components\n\nThese operations help optimize resource usage and reduce operational costs in Kubernetes environments.\n\nLifecycle management operations in KubeBlocks:\n\n| Operation | Effect | Use Case |\n|-----------|--------|----------|\n| Stop | Suspends cluster, retains storage | Cost savings, maintenance |\n| Start | Resumes cluster operation | Restore service after pause |\n| Restart | Recreates pods for component | Configuration changes, troubleshooting |\n\n## Prerequisites\n\n\n\n## Deploy a Redis Replication Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Cluster Lifecycle Operations\n\n### Stopping the Cluster\n\nStopping a Redis Replication Cluster in KubeBlocks will:\n\n1. Terminates all running pods\n2. Retains persistent storage (PVCs)\n3. Maintains cluster configuration\n\nThis operation is ideal for:\n- Temporary cost savings\n- Maintenance windows\n- Development environment pauses\n\n\n\nOption 1: OpsRequest API\n\nCreate a Stop operation request:\n\n```yaml\napiVersion: operations.kubeblocks.io/v1alpha1\nkind: OpsRequest\nmetadata:\n  name: redis-replication-stop-ops\n  namespace: demo\nspec:\n  clusterName: redis-replication\n  type: Stop\n```\n\n\n\nOption 2: Cluster API Patch\n\nModify the cluster spec directly by patching the stop field:\n\n```bash\nkubectl patch cluster redis-replication -n demo --type='json' -p='[\n,\n\n]'\n```\n\n\n\n\n\n### Verifying Cluster Stop\n\nTo confirm a successful stop operation:\n\n1. Check cluster status transition:\n    ```bash\n    kubectl get cluster redis-replication -n demo -w\n    ```\n    Example Output:\n    ```bash\n    NAME                CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     AGE\n    redis-replication   redis                Delete               Stopping   6m3s\n    redis-replication   redis                ",
    "path": "docs/release-1_0/kubeblocks-for-redis/04-operations/01-stop-start-restart",
    "description": "  # Redis Replication Cluster Lifecycle Management  This guide demonstrates how to manage a Redis Replication Cluster's operational state in **KubeBlocks**, including:  - Stopping the cluster to conserve resources - Starting a stopped cluster - Restarting cluster components  These operations help op"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_04-operations_02-vertical-scaling",
    "title": "Vertical Scaling in a Redis Replication Cluster",
    "content": "\n\n\n\n# Vertical Scaling for Redis Replication Clusters with KubeBlocks\n\nThis guide demonstrates how to vertically scale a Redis Replication Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.\n\nVertical scaling modifies compute resources (CPU and memory) for Redis instances while maintaining replica count. Key characteristics:\n\n- **Non-disruptive**: When properly configured, maintains availability during scaling\n- **Granular**: Adjust CPU, memory, or both independently\n- **Reversible**: Scale up or down as needed\n\nKubeBlocks orchestrates scaling with minimal impact:\n1. Secondary replicas update first\n2. Primary updates last after secondaries are healthy\n3. Cluster status transitions from `Updating` to `Running`\n\n## Prerequisites\n\n\n\n## Deploy a Redis Replication Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Vertical Scale\n\n**Expected Workflow**:\n\n1. Secondary replicas are updated first (one at a time)\n1. Primary is updated last after secondary replicas are healthy\n1. Cluster status transitions from `Updating` to `Running`\n\n  Option 1: Using VerticalScaling OpsRequest\n\n  Apply the following YAML to scale up the resources for the redis component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: redis-replication-vscale-ops\n    namespace: demo\n  spec:\n    clusterName: redis-replication\n    type: VerticalScaling\n    verticalScaling:\n    - componentName: redis\n      requests:\n        cpu: '1'\n        memory: 1Gi\n      limits:\n        cpu: '1'\n        memory: 1Gi\n  ```\n  What Happens During Vertical Scaling?\n  - Secondary Pods are recreated first to ensure the primary Pod remains available.\n  - Once all secondary Pods are updated, the primary Pod is restarted with the new resource configuration.\n\n\n  You can check the progress of the scaling operation with the following command:\n\n  ```bash\n  kubectl -n demo get ops redis-replication-vscale-ops -w\n  ```\n\n ",
    "path": "docs/release-1_0/kubeblocks-for-redis/04-operations/02-vertical-scaling",
    "description": "    # Vertical Scaling for Redis Replication Clusters with KubeBlocks  This guide demonstrates how to vertically scale a Redis Replication Cluster managed by KubeBlocks by adjusting compute resources (CPU and memory) while maintaining the same number of replicas.  Vertical scaling modifies compute r"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_04-operations_03-horizontal-scaling",
    "title": "Horizontal Scaling of Redis Clusters with KubeBlocks",
    "content": "\n\n\n\n# Horizontal Scaling for Redis Clusters with KubeBlocks\n\nThis guide explains how to perform horizontal scaling (scale-out and scale-in) on a Redis cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.\n\n## Prerequisites\n\n\n\n## Deploy a Redis Replication Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n\n## Scale-out (Add Replicas)\n\n**Expected Workflow**:\n\n1. New pod is provisioned, and transitions from `Pending` to `Running` with `secondary` role\n2. Data synced from primary to new replica\n3. Cluster status changes from `Updating` to `Running`\n\n\n\n  Option 1: Using Horizontal Scaling OpsRequest\n\n  Scale out the Redis cluster by adding 1 replica to redis component:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: redis-replication-scale-out-ops\n    namespace: demo\n  spec:\n    clusterName: redis-replication\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: redis\n      # Specifies the replica changes for scaling in components\n      scaleOut:\n        # Specifies the replica changes for the component.\n        # add one more replica to current component\n        replicaChanges: 1\n  ```\n\n  Monitor the progress of the scaling operation:\n\n  ```bash\n  kubectl get ops redis-replication-scale-out-ops -n demo -w\n  ```\n\n  Expected Result:\n  ```bash\n  NAME                              TYPE                CLUSTER             STATUS    PROGRESS   AGE\n  redis-replication-scale-out-ops   HorizontalScaling   redis-replication   Running   0/1        9s\n  redis-replication-scale-out-ops   HorizontalScaling   redis-replication   Running   1/1        20s\n  redis-replication-scale-out-ops   HorizontalScaling   redis-replication   Succeed   1/1        20s\n  ```\n  \n\n\n  Option 2: Direct Cluster API Update\n\n  Alternatively, you can perform a direct update to the `replicas` field in the Cluster resource:\n\n  ```yaml\n  apiVersion: apps.kubeblocks.io/v1\n  kind: Cl",
    "path": "docs/release-1_0/kubeblocks-for-redis/04-operations/03-horizontal-scaling",
    "description": "    # Horizontal Scaling for Redis Clusters with KubeBlocks  This guide explains how to perform horizontal scaling (scale-out and scale-in) on a Redis cluster managed by KubeBlocks. You'll learn how to use both **OpsRequest** and direct **Cluster API** updates to achieve this.  ## Prerequisites    #"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_04-operations_04-volume-expansion",
    "title": "Expanding Volume in a Redis Cluster",
    "content": "\n\n\n\n# Expanding Volume in a Redis Cluster\n\nThis guide explains how to expand Persistent Volume Claims (PVCs) in a Redis cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the underlying storage class, this operation can be performed without downtime.\n\nVolume expansion allows you to increase the size of a Persistent Volume Claim (PVC) after it has been created. This feature was introduced in Kubernetes v1.11 and became generally available (GA) in Kubernetes v1.24.\n\n## Prerequisites\n\n\n\n### Check the Storage Class for Volume Expansion Support\n\nList all available storage classes and verify if volume expansion is supported by checking the `ALLOWVOLUMEEXPANSION` field:\n```bash\nkubectl get storageclass\n```\n\nExample Output:\n```bash\nNAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\ngp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  4d10h\nkb-default-sc       ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   3d7h\nsc-s3-repo-2qsxfh   ru.yandex.s3.csi        Retain          Immediate              false                  3d7h\n```\nEnsure the storage class you are using has `ALLOWVOLUMEEXPANSION` set to true. If it is false, the storage class does not support volume expansion.\n\n## Deploy a Redis Replication Cluster with StorageClass\n\nKubeBlocks uses a declarative approach to manage Redis clusters. Below is an example configuration for deploying a Redis cluster with 2 replicas (1 primary, 1 secondary).\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: redis-replication\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: redis\n  topology: replication\n  componentSpecs:\n    - name: redis\n      serviceVersion: \"7.2.4\"\n  ",
    "path": "docs/release-1_0/kubeblocks-for-redis/04-operations/04-volume-expansion",
    "description": "    # Expanding Volume in a Redis Cluster  This guide explains how to expand Persistent Volume Claims (PVCs) in a Redis cluster managed by **KubeBlocks**. Volume expansion enables dynamic storage capacity increases, allowing your database to scale seamlessly as data grows. When supported by the unde"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_04-operations_05-manage-loadbalancer",
    "title": "Create and Destroy Redis Service Using the Declarative Cluster API in KubeBlocks",
    "content": "\n\n\n\n# Manage Redis Services Using the Declarative Cluster API in KubeBlocks\n\nThis guide provides step-by-step instructions for exposing Redis services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage internal services, and properly disable external exposure when no longer needed.\n\n## Prerequisites\n\n\n\n## Deploy a Redis Replication Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## View Network Services\nList the Services created for the Redis cluster:\n```bash\nkubectl get service -l app.kubernetes.io/instance=redis-replication -n demo\n```\n\nExample Services:\n```bash\nNAME                                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE\nredis-replication-redis-redis                     ClusterIP   10.96.102.140           6379/TCP    31s\nredis-replication-redis-sentinel-redis-sentinel   ClusterIP   10.96.157.4             26379/TCP   51s\n```\n\n## Expose Redis Service\n\nExternal service addresses enable public internet access to Redis, while internal service addresses restrict access to the user's VPC.\n\n### Service Types Comparison\n\n| Type | Use Case | Cloud Cost | Security |\n|------|----------|------------|----------|\n| ClusterIP | Internal service communication | Free | Highest |\n| NodePort | Development/testing | Low | Moderate |\n| LoadBalancer | Production external access | High | Managed via security groups |\n\n\n\n\n  Option 1: Using OpsRequest\n\n  To expose the Redis service externally using a LoadBalancer, create an OpsRequest resource:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: redis-replication-expose-enable-ops\n    namespace: demo\n  spec:\n    type: Expose\n    clusterName: redis-replication\n    expose:\n    - componentName: redis\n      services:\n      - name: internet\n        # Determines how the Service is exposed. Defaults to 'ClusterIP'.\n        # Valid options are 'ClusterIP', 'NodeP",
    "path": "docs/release-1_0/kubeblocks-for-redis/04-operations/05-manage-loadbalancer",
    "description": "    # Manage Redis Services Using the Declarative Cluster API in KubeBlocks  This guide provides step-by-step instructions for exposing Redis services managed by KubeBlocks, both externally and internally. You'll learn to configure external access using cloud provider LoadBalancer services, manage i"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_04-operations_07-modify-parameters",
    "title": "Modify Redis Parameters",
    "content": "\n# Modify Redis Parameters\n\nDatabase reconfiguration involves modifying parameters, settings, or configurations to optimize performance, security, or availability. Parameter changes fall into two categories:\n\n| Type | Restart Required | Scope | Example Parameters |\n|------|------------------|-------|--------------------|\n| **Dynamic** | No | Immediate effect | `max_connections` |\n| **Static** | Yes | After restart | `shared_buffers` |\n\nFor static parameters, KubeBlocks minimizes downtime by:\n1. Modifying and restarting replica nodes first\n2. Performing a switchover to promote the updated replica as primary (typically completes in milliseconds)\n3. Restarting the original primary node\n\n:::note\n\nKubeBlocks Redis Addon does not implement any dynamic reload action for `Dynamic Parameters`, thus changes on any parameters will cause a restart.\n\n:::\n\nThis guide demonstrates how to modify static parameters of a Redis cluster managed by KubeBlocks using a Reconfiguring OpsRequest.\n\n## Prerequisites\n\n\n\n## Deploy a Redis Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Check Parameter Values\n\n### Retrieve Credentials\nKubeBlocks automatically creates a secret containing the Redis root credentials. Retrieve the credentials with the following commands:\n```bash\nNAME=`kubectl get secrets -n demo redis-replication-redis-account-default -o jsonpath='' | base64 -d`\nPASSWD=`kubectl get secrets -n demo redis-replication-redis-account-default -o jsonpath='' | base64 -d`\n```\n\n### Access Redis Cluster\nTo connect to the cluster's primary node, use the Redis client:\n```bash\nkubectl exec -it -n demo redis-replication-redis-0 -c redis -- redis-cli -a $\n```\n\n### Query Parameter Values\n\nOnce connected, you can query the current value of 'max_connections' and 'shared_buffers':\n```sql\n127.0.0.1:6379> CONFIG GET aof-timestamp-enabled\n1) \"aof-timestamp-enabled\"\n2) \"no\"\n```\n\n## Static Parameter Example: Modifying aof-timestamp-enabled\n\nCreate a Reconfigure OpsRequest. Apply the following OpsRequest YAML t",
    "path": "docs/release-1_0/kubeblocks-for-redis/04-operations/07-modify-parameters",
    "description": " # Modify Redis Parameters  Database reconfiguration involves modifying parameters, settings, or configurations to optimize performance, security, or availability. Parameter changes fall into two categories:  | Type | Restart Required | Scope | Example Parameters | |------|------------------|-------"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_04-operations_08-switchover",
    "title": "Redis Cluster Switchover",
    "content": "\n\n# Redis Cluster Switchover\n\nA **switchover** is a planned operation that transfers the primary role from one Redis instance to another. Unlike failover which occurs during failures, switchover provides:\n- Controlled role transitions\n- Minimal downtime (typically a few hundred milliseconds)\n- Predictable maintenance windows\n\nSwitchover is ideal for:\n- Node maintenance/upgrades\n- Workload rebalancing\n- Testing high availability\n- Planned infrastructure changes\n\n## Prerequisites\n\n\n\n## Deploy a Redis Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Check Roles\nList the Pods and their roles (primary or secondary):\n\n```bash\nkubectl get pods -n demo -l app.kubernetes.io/instance=redis-replication,apps.kubeblocks.io/component-name=redis -L kubeblocks.io/role\n```\n\nExample Output:\n\n```text\nNAME                      READY   STATUS    RESTARTS   AGE     ROLE\nredis-replication-redis-0   4/4     Running   0          9m59s   primary\nredis-replication-redis-1   4/4     Running   0          11m     secondary\n```\n\n## Performing a Planned Switchover\n\nTo initiate a planned switchover, create an OpsRequest resource as shown below:\n\n  Option 1: Automatic Switchover (No preferred candidate)\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: redis-switchover-ops\n    namespace: demo\n  spec:\n    clusterName: redis-replication\n    type: Switchover\n    switchover:\n    - componentName: redis\n      instanceName: redis-replication-redis-0\n  ```\n **Key Parameters:**\n  - `instanceName`: Specifies the instance (Pod) that is primary or leader before a switchover operation.\n\n  \n  Option 2: Targeted Switchover (Specific candidate)\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: redis-switchover-targeted\n    namespace: demo\n  spec:\n    clusterName: redis-replication\n    type: Switchover\n    switchover:\n    - componentName: redis\n      # Specifies the instance whose role will be transferred.\n      # A typic",
    "path": "docs/release-1_0/kubeblocks-for-redis/04-operations/08-switchover",
    "description": "  # Redis Cluster Switchover  A **switchover** is a planned operation that transfers the primary role from one Redis instance to another. Unlike failover which occurs during failures, switchover provides: - Controlled role transitions - Minimal downtime (typically a few hundred milliseconds) - Predi"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_04-operations_09-decommission-a-specific-replica",
    "title": "Decommission a Specific Pod in KubeBlocks-Managed Redis Clusters",
    "content": "\n\n\n\n# Decommission a Specific Pod in KubeBlocks-Managed Redis Clusters\n\nThis guide explains how to decommission (take offline) specific Pods in Redis clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload rebalancing, node maintenance, or addressing failures.\n\n## Why Decommission Pods with KubeBlocks?\n\nIn traditional StatefulSet-based deployments, Kubernetes lacks the ability to decommission specific Pods. StatefulSets ensure the order and identity of Pods, and scaling down always removes the Pod with the highest ordinal number (e.g., scaling down from 3 replicas removes `Pod-2` first). This limitation prevents precise control over which Pod to take offline, which can complicate maintenance, workload distribution, or failure handling.\n\nKubeBlocks overcomes this limitation by enabling administrators to decommission specific Pods directly. This fine-grained control ensures high availability and allows better resource management without disrupting the entire cluster.\n\n## Prerequisites\n\n\n\n## Deploy a Redis Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Decommission a Pod\n\n**Expected Workflow**:\n\n1. Replica specified in `onlineInstancesToOffline` is removed\n2. Pod terminates gracefully\n3. Cluster transitions from `Updating` to `Running`\n\nTo decommission a specific Pod (e.g., 'redis-replication-redis-1'), you can use one of the following methods:\n\n\n\n  Option 1: Using OpsRequest\n\n  Create an OpsRequest to mark the Pod as offline:\n\n  ```yaml\n  apiVersion: operations.kubeblocks.io/v1alpha1\n  kind: OpsRequest\n  metadata:\n    name: redis-replication-decommission-ops\n    namespace: demo\n  spec:\n    clusterName: redis-replication\n    type: HorizontalScaling\n    horizontalScaling:\n    - componentName: redis\n      scaleIn:\n        onlineInstancesToOffline:\n          - 'redis-replication-redis-1'  # Specifies the instance names that need to be taken offline\n  ```\n\n  #### Monitor the Decommi",
    "path": "docs/release-1_0/kubeblocks-for-redis/04-operations/09-decommission-a-specific-replica",
    "description": "    # Decommission a Specific Pod in KubeBlocks-Managed Redis Clusters  This guide explains how to decommission (take offline) specific Pods in Redis clusters managed by KubeBlocks. Decommissioning provides precise control over cluster resources while maintaining availability. Use this for workload "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_05-backup-restore_01-create-backuprepo",
    "title": "Create a Backup Repository for KubeBlocks",
    "content": "\n# Create a BackupRepo for KubeBlocks\n\nThis guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.\n\n## Prerequisites\n- AWS CLI configured with appropriate permissions to create S3 buckets.\n- kubectl access to your Kubernetes cluster.\n- KubeBlocks installed ([installation guide](../user_docs/overview/install-kubeblocks)) and running in the kb-system namespace.\n\n## Step 1: Create S3 Bucket\n\nUse the AWS CLI to create an S3 bucket in your desired region. Replace `` with your target AWS region (e.g., `us-east-1`, `ap-southeast-1`).\n\n```bash\n aws s3api create-bucket --bucket kubeblocks-backup-repo --region  --create-bucket-configuration LocationConstraint=\n```\n\nExample (for us-west-1):\n```bash\naws s3api create-bucket \\\n  --bucket kubeblocks-backup-repo \\\n  --region us-west-1 \\\n  --create-bucket-configuration LocationConstraint=us-west-1\n```\n\nExample Output:\n\n```json\n\n```\n\nVerification:\nConfirm the bucket was created by listing its contents (it will be empty initially):\n\n```bash\naws s3 ls s3://kubeblocks-backup-repo\n```\n\n## Step 2: Create a Kubernetes Secret for AWS Credentials\n\nStore your AWS credentials securely in a Kubernetes Secret. Replace `` and `` with your actual AWS credentials:\n\n```bash\n# Create a secret to save the access key\nkubectl create secret generic s3-credential-for-backuprepo \\\n  --from-literal=accessKeyId= \\\n  --from-literal=secretAccessKey= \\\n  -n kb-system\n```\n\n## Step 3: Configure Backup Repository\n\nA BackupRepo is a custom resource that defines a storage repository for backups. In this step, you'll integrate your S3 bucket with KubeBlocks by creating a BackupRepo resource.\n\nApply the following YAML to create the BackupRepo. Replace fields(e.g., bucket name, region) with your specific settings.\n\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupRepo\nmetadata:\n  name: s3-repo\n  annotations:\n    # mark this backuprepo as default one\n    dataprotection.kubeblocks.io/is",
    "path": "docs/release-1_0/kubeblocks-for-redis/05-backup-restore/01-create-backuprepo",
    "description": " # Create a BackupRepo for KubeBlocks  This guide walks you through creating and configuring a BackupRepo in KubeBlocks using an S3 bucket for storing backup data.  ## Prerequisites - AWS CLI configured with appropriate permissions to create S3 buckets. - kubectl access to your Kubernetes cluster. -"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_05-backup-restore_02-create-full-backup",
    "title": "Create a Full Backup for a Redis Cluster on KubeBlocks",
    "content": "\n\n\n# Create a Full Backup for Redis on KubeBlocks\n\nThis guide demonstrates how to create and validate full backups for Redis clusters on KubeBlocks using the `pg-basebackup` method through both:\n- The Backup API (direct backup operations)\n- The OpsRequest API (managed backup operations with enhanced monitoring)\n\nWe will cover how to restore data from a backup in the [Restore From Full Backup](./05-restoring-from-full-backup) guide.\n\n## Prerequisites\n\n\n\n## Deploy a Redis Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Backup Prerequisites\n\nBefore creating backups, ensure:\n1. Backup repository is configured:\n   - `BackupRepo` resource exists\n   - Network connectivity between cluster and repository\n   - `BackupRepo` status shows \"Ready\"\n\n2. Cluster is ready:\n   - Cluster status is \"Running\"\n   - No ongoing operations (scaling, upgrades, etc.)\n\n## Identify Backup Configuration\n\nCheck available backup policies and schedules:\n\n```bash\n# List backup policies\nkubectl get backuppolicy -n demo -l app.kubernetes.io/instance=redis-replication\n\n# List backup schedules\nkubectl get backupschedule -n demo -l app.kubernetes.io/instance=redis-replication\n```\n\nExpected Output:\n```bash\nNAME                                    BACKUP-REPO   STATUS      AGE\nredis-replication-redis-backup-policy                 Available   17m\n\nNAME                                              STATUS      AGE\nredis-replication-redis-backup-schedule           Available   60m\n```\n\nView supported backup methods in the BackupPolicy CR 'redis-replication-redis-backup-policy':\n\n```bash\nkubectl get backuppolicy redis-replication-redis-backup-policy -n demo -oyaml | yq '.spec.backupMethods[].name'\n```\n**List of Backup methods**\n\nKubeBlocks Redis supports these backup methods:\n\n| Feature           | Method          | Description |\n|-------------|--------|------------|\n| Full Backup | datafile  | Uses `redis-cli BGSAVE` command to backup data |\n| Continuous Backup | aof | Continuously perform incremental backups by arc",
    "path": "docs/release-1_0/kubeblocks-for-redis/05-backup-restore/02-create-full-backup",
    "description": "   # Create a Full Backup for Redis on KubeBlocks  This guide demonstrates how to create and validate full backups for Redis clusters on KubeBlocks using the `pg-basebackup` method through both: - The Backup API (direct backup operations) - The OpsRequest API (managed backup operations with enhanced"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_05-backup-restore_03-scheduled-full-backup",
    "title": "Setting Up a Redis Cluster with Scheduled Backups in KubeBlocks",
    "content": "\n\n# Setting Up a Redis Cluster with Scheduled Backups in KubeBlocks\n\nThis guide demonstrates how to deploy a Redis cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.\n\n## Prerequisites\n\n\n\n## Deploy a Redis Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Prerequisites for Backup\n\n1. Backup Repository Configured:\n   - Configured `BackupRepo`\n   - Network connectivity between cluster and repo, `BackupRepo` status is `Ready`\n\n2. Cluster is Running:\n   - Cluster must be in `Running` state\n   - No ongoing operations (scaling, upgrades etc.)\n\n## Configure Scheduled Backups\n\nKubeBlocks automatically creates a `BackupSchedule` resource when the cluster is created. Follow these steps to enable and configure scheduled backups:\n\n1. Verify the default backup schedule configuration:\n\n```bash\nkubectl get backupschedule redis-replication-redis-backup-schedule  -n demo -oyaml\n```\n\nExample Output:\n```yaml\napiVersion: dataprotection.kubeblocks.io/v1alpha1\nkind: BackupSchedule\nspec:\n  backupPolicyName: redis-replication-redis-backup-policy\n  schedules:\n  - backupMethod: datafile\n    # ┌───────────── minute (0-59)\n    # │ ┌───────────── hour (0-23)\n    # │ │ ┌───────────── day of month (1-31)\n    # │ │ │ ┌───────────── month (1-12)\n    # │ │ │ │ ┌───────────── day of week (0-6) (Sunday=0)\n    # │ │ │ │ │\n    # 0 18 * * *\n    # schedule this job every day at 6:00 PM (18:00).\n    cronExpression: 0 18 * * * # update the cronExpression to your need\n    enabled: false # set to `true` to schedule base backup periodically\n    retentionPeriod: 7d # set the retention period to your need\n```\n\n2. Enable and customize the backup schedule:\n```bash\nkubectl edit backupschedule redis-replication-redis-backup-schedule -n demo\n```\n\nUpdate these key parameters:\n- `enabled`: Set to `true` to activate scheduled backups\n- `cronExpression`: Configure backup frequency using cron syntax\n- `retentionPeriod`: Set how long to keep backups (e.g., `7d`, `1mo`)\n\nExample configura",
    "path": "docs/release-1_0/kubeblocks-for-redis/05-backup-restore/03-scheduled-full-backup",
    "description": "  # Setting Up a Redis Cluster with Scheduled Backups in KubeBlocks  This guide demonstrates how to deploy a Redis cluster using KubeBlocks and configure scheduled backups with retention in an S3 repository.  ## Prerequisites    ## Deploy a Redis Cluster    ## Verifying the Deployment    ## Prerequi"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_05-backup-restore_04-scheduled-continuous-backup",
    "title": "Setting Up a Redis Cluster with Scheduled Continuous Backup in KubeBlocks",
    "content": "\n# Setting Up a Redis Cluster with Scheduled Continuous Backup Enabled in KubeBlocks\n\nThis guide demonstrates how to configure a Redis cluster on KubeBlocks with:\n\n- Scheduled full backups (base backups)\n- Continuous WAL (Write-Ahead Log) archiving\n- Point-In-Time Recovery (PITR) capabilities\n\nThis combination provides comprehensive data protection with minimal recovery point objectives (RPO).\n\n## What is PITR?\nPoint-In-Time Recovery (PITR) allows you to restore a database to a specific moment in time by combining full backups with continuous binlog/wal/archive log backups.\n\nFor details on restoring data from both full backups and continuous binlog backups, refer to the [Restore From PITR](restore-with-pitr.mdx)  guide.\n\n## Prerequisites\n\nBefore proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```\n\n## Prerequisites for Backup\n\n1. Backup Repository Configured:\n   - Configured `BackupRepo`\n   - Network connectivity between cluster and repo, `BackupRepo` status is `Ready`\n\n2. Cluster is Running:\n   - Cluster must be in `Running` state\n   - No ongoing operations (scaling, upgrades etc.)\n\n## List of Backup methods\n\nKubeBlocks Redis supports these backup methods:\n\n| Feature           | Method          | Description |\n|-------------|--------|------------|\n| Full Backup | datafile  | Uses `redis-cli BGSAVE` command to backup data |\n| Continuous Backup | aof | Continuously perform incremental backups by archiving Append-Only Files (AOF) |\n\n## Deploy a Redis Cluster\n\n\n\n## Verifying the Deployment\n\n\n\n## Enabl",
    "path": "docs/release-1_0/kubeblocks-for-redis/05-backup-restore/04-scheduled-continuous-backup",
    "description": " # Setting Up a Redis Cluster with Scheduled Continuous Backup Enabled in KubeBlocks  This guide demonstrates how to configure a Redis cluster on KubeBlocks with:  - Scheduled full backups (base backups) - Continuous WAL (Write-Ahead Log) archiving - Point-In-Time Recovery (PITR) capabilities  This "
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_05-backup-restore_05-restoring-from-full-backup",
    "title": "Restore a Redis Cluster from Backup",
    "content": "\n\n# Restore a Redis Cluster from Backup\n\nThis guide demonstrates two methods to restore a Redis cluster from backup in KubeBlocks:\n\n1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations\n2. **OpsRequest API Method** - Enhanced operational control with progress monitoring\n\n## Prerequisites\n\n\n\n## Preparing for Restoration: Locate one Full Backup\nBefore restoring, ensure that there is a full backup available. The restoration process will use this backup to create a new Redis cluster.\n\n- Backup repository accessible from new cluster\n- Valid full backup in `Completed` state\n- Adequate CPU/memory resources\n- Sufficient storage capacity\n\nFind available full backups:\n\n```bash\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Full,app.kubernetes.io/instance=redis-replication # get the list of full backups\n```\n\nPick ONE of the Backups whose status is `Completed`.\n\n## Option 1: Cluster Annotation Restoration\n\n### Step 1: Create Restored Cluster\nCreate a new cluster with restore configuration:\n\nKey parameters:\n- `kubeblocks.io/restore-from-backup` annotation\n- Backup name and namespace located from the previous steps\n\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: redis-replication-restore\n  namespace: demo\n  annotations:\n    kubeblocks.io/restore-from-backup: '}'\nspec:\n  terminationPolicy: Delete\n  clusterDef: redis\n  topology: replication\n  componentSpecs:\n    - name: redis\n      serviceVersion: \"7.2.4\"\n      disableExporter: false\n      replicas: 2\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n    - name: redis-sentinel\n      replicas: 3\n      resources:\n        limits:\n       ",
    "path": "docs/release-1_0/kubeblocks-for-redis/05-backup-restore/05-restoring-from-full-backup",
    "description": "  # Restore a Redis Cluster from Backup  This guide demonstrates two methods to restore a Redis cluster from backup in KubeBlocks:  1. **Cluster Annotation Method** - Simple declarative approach using YAML annotations 2. **OpsRequest API Method** - Enhanced operational control with progress monitori"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_05-backup-restore_06-restore-with-pitr",
    "title": "Restore a Redis Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks",
    "content": "\n\n# Restore a Redis Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks\n\nThis guide demonstrates how to perform Point-In-Time Recovery (PITR) for Redis clusters in KubeBlocks using:\n\n1. A full base backup\n2. Continuous WAL (Write-Ahead Log) backups\n3. Two restoration methods:\n   - Cluster Annotation (declarative approach)\n   - OpsRequest API (operational control)\n\nPITR enables recovery to any moment within the `timeRange` specified.\n\n## Prerequisites\n\n\n\n## Prepare for PITR Restoration\nTo perform a PITR restoration, both a full backup and continuous backup are required. Refer to the documentation to configure these backups if they are not already set up.\n\n- Completed full backup\n- Active continuous WAL backup\n- Backup repository accessible\n- Sufficient resources for new cluster\n\nTo identify the list of full and continuous backups, you may follow the steps:\n\n### 1. Verify Continuous Backup\nConfirm you have a continuous WAL backup, either running or completed:\n\n```bash\n# expect EXACTLY ONE continuous backup per cluster\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Continuous,app.kubernetes.io/instance=redis-replication\n```\n\n### 2. Check Backup Time Range\nGet the valid recovery window:\n\n```bash\nkubectl get backup  -n demo -o yaml | yq '.status.timeRange'\n```\n\nExpected Output:\n```text\nstart: \"2025-05-07T09:12:47Z\"\nend: \"2025-05-07T09:22:50Z\"\n```\n\n### 3. Identify Full Backup\nFind available full backups that meet:\n- Status: Completed\n- Completion time after continuous backup start time\n\n```bash\n# expect one or more Full backups\nkubectl get backup -n demo -l dataprotection.kubeblocks.io/backup-type=Full,app.kubernetes.io/instance=redis-replication\n```\n\n:::tip\nKubeBlocks automatically selects the most recent qualifying full backup as the base.\nMake sure there is a full backup meets the condition: its `stopTime`/`completionTimestamp` must **AFTER** Continuous backup's `startTime`, otherwise PITR restoration will fail.\n:::\n\n## Option 1:",
    "path": "docs/release-1_0/kubeblocks-for-redis/05-backup-restore/06-restore-with-pitr",
    "description": "  # Restore a Redis Cluster from Backup with Point-In-Time-Recovery(PITR) on KubeBlocks  This guide demonstrates how to perform Point-In-Time Recovery (PITR) for Redis clusters in KubeBlocks using:  1. A full base backup 2. Continuous WAL (Write-Ahead Log) backups 3. Two restoration methods:    - Cl"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_06-custom-secret_01-custom-secret",
    "title": "Create a Redis Cluster with a Custom Root Password on KubeBlocks",
    "content": "\n# Create Redis Cluster With Custom Password on KubeBlocks\n\nThis guide demonstrates how to deploy a Redis cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.\n\n## Prerequisites\n\n\n\n## Deploying the Redis Replication Cluster\n\nKubeBlocks uses a declarative approach for managing Redis clusters. Below is an example configuration for deploying a Redis cluster with 2 nodes (1 primary, 1 replicas) and a custom root password.\n\n### Step 1: Create a Secret for the Defaults Account\n\nThe custom root password is stored in a Kubernetes Secret. Create the Secret by applying the following YAML:\n\n```yaml\napiVersion: v1\ndata:\n  password: Y3VzdG9tcGFzc3dvcmQ= # custompassword\n  username: cm9vdA== #root\nimmutable: true\nkind: Secret\nmetadata:\n  name: custom-secret\n  namespace: demo\n```\n- password: Replace custompassword with your desired password and encode it using Base64 (`echo -n \"custompassword\" | base64`).\n- username: The default Redis default user is 'default', encoded as 'cm9vdA=='.\n\n\n### Step 2: Deploy the Redis Cluster\n\nApply the following manifest to deploy the Redis cluster, referencing the Secret created in Step 1 for the root account:\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: redis-replication\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: redis\n  topology: replication\n  componentSpecs:\n    - name: redis\n      serviceVersion: \"7.2.4\"\n      disableExporter: false\n      replicas: 2\n      systemAccounts:  # override systemaccount password\n        - name: default\n          secretRef:\n            name: custom-secret\n            namespace: demo\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage:",
    "path": "docs/release-1_0/kubeblocks-for-redis/06-custom-secret/01-custom-secret",
    "description": " # Create Redis Cluster With Custom Password on KubeBlocks  This guide demonstrates how to deploy a Redis cluster in KubeBlocks with a custom root password stored in a Kubernetes Secret.  ## Prerequisites    ## Deploying the Redis Replication Cluster  KubeBlocks uses a declarative approach for manag"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis_08-monitoring_01-integrate-with-prometheus-operator",
    "title": "Observability for Redis Clusters with the Prometheus Operator",
    "content": "\n\n# Redis Monitoring with Prometheus Operator\n\nThis guide demonstrates how to configure comprehensive monitoring for Redis clusters in KubeBlocks using:\n\n1. Prometheus Operator for metrics collection\n2. Built-in Redis exporter for metrics exposure\n3. Grafana for visualization\n\n## Prerequisites\n\n\n\n## Install Monitoring Stack\n\n### 1. Install Prometheus Operator\nDeploy the kube-prometheus-stack using Helm:\n\n```bash\nhelm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n  -n monitoring \\\n  --create-namespace\n```\n\n### 2. Verify Installation\nCheck all components are running:\n```bash\nkubectl get pods -n monitoring\n```\n\nExpected Output:\n```bash\nNAME                                                     READY   STATUS    RESTARTS   AGE\nalertmanager-prometheus-kube-prometheus-alertmanager-0   2/2     Running   0          114s\nprometheus-grafana-75bb7d6986-9zfkx                      3/3     Running   0          2m\nprometheus-kube-prometheus-operator-7986c9475-wkvlk      1/1     Running   0          2m\nprometheus-kube-state-metrics-645c667b6-2s4qx            1/1     Running   0          2m\nprometheus-prometheus-kube-prometheus-prometheus-0       2/2     Running   0          114s\nprometheus-prometheus-node-exporter-47kf6                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-6ntsl                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-gvtxs                1/1     Running   0          2m1s\nprometheus-prometheus-node-exporter-jmxg8                1/1     Running   0          2m1s\n```\n\n\n## Deploy a Redis Cluster\n\n\n\n**Key Monitoring Configuration**\n- `disableExporter: false` enables the built-in metrics exporter\n- Exporter runs as sidecar container in each Redis pod\n- Scrapes Redis metrics on port 9187\n\n## Verifying the Deployment\nMonitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster ",
    "path": "docs/release-1_0/kubeblocks-for-redis/08-monitoring/01-integrate-with-prometheus-operator",
    "description": "  # Redis Monitoring with Prometheus Operator  This guide demonstrates how to configure comprehensive monitoring for Redis clusters in KubeBlocks using:  1. Prometheus Operator for metrics collection 2. Built-in Redis exporter for metrics exposure 3. Grafana for visualization  ## Prerequisites    ##"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis__tpl__create-redis-replication-cluster",
    "title": "_create-redis-replication-cluster.mdx",
    "content": "KubeBlocks uses a declarative approach for managing Redis Replication Clusters.\nBelow is an example configuration for deploying a Redis Replication Cluster with two components, redis and redis sentinel.\n\nApply the following YAML configuration to deploy the cluster:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1\nkind: Cluster\nmetadata:\n  name: redis-replication\n  namespace: demo\nspec:\n  terminationPolicy: Delete\n  clusterDef: redis\n  topology: replication\n  componentSpecs:\n    - name: redis\n      serviceVersion: \"7.2.4\"\n      disableExporter: false\n      replicas: 2\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n    - name: redis-sentinel\n      replicas: 3\n      resources:\n        limits:\n          cpu: '0.5'\n          memory: 0.5Gi\n        requests:\n          cpu: '0.5'\n          memory: 0.5Gi\n      volumeClaimTemplates:\n        - name: data\n          spec:\n            storageClassName: \"\"\n            accessModes:\n              - ReadWriteOnce\n            resources:\n              requests:\n                storage: 20Gi\n```\n",
    "path": "docs/release-1_0/kubeblocks-for-redis/_tpl/_create-redis-replication-cluster",
    "description": "KubeBlocks uses a declarative approach for managing Redis Replication Clusters. Below is an example configuration for deploying a Redis Replication Cluster with two components, redis and redis sentinel.  Apply the following YAML configuration to deploy the cluster:  ```yaml apiVersion: apps.kubebloc"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis__tpl__prerequisites",
    "title": "_prerequisites.mdx",
    "content": "Before proceeding, ensure the following:\n- Environment Setup:\n    - A Kubernetes cluster is up and running.\n    - The kubectl CLI tool is configured to communicate with your cluster.\n    - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/install-kubeblocks) are installed. Follow the installation instructions here.\n- Namespace Preparation: To keep resources isolated, create a dedicated namespace for this tutorial:\n\n```bash\nkubectl create ns demo\nnamespace/demo created\n```",
    "path": "docs/release-1_0/kubeblocks-for-redis/_tpl/_prerequisites",
    "description": "Before proceeding, ensure the following: - Environment Setup:     - A Kubernetes cluster is up and running.     - The kubectl CLI tool is configured to communicate with your cluster.     - [KubeBlocks CLI](../../user_docs/references/install-kbcli) and [KubeBlocks Operator](../../user_docs/overview/i"
  },
  {
    "id": "docs_en_release-1_0_kubeblocks-for-redis__tpl__verify-redis-replication-cluster",
    "title": "_verify-redis-replication-cluster.mdx",
    "content": "Monitor the cluster status until it transitions to the Running state:\n```bash\nkubectl get cluster redis-replication -n demo -w\n```\n\nExpected Output:\n\n```bash\nNAME                CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS    AGE\nredis-replication   redis                Delete               Running   3m49s\n```\n\nCheck the pod status and roles:\n```bash\nkubectl get pods -l app.kubernetes.io/instance=redis-replication -L  kubeblocks.io/role -n demo\n```\n\nExpected Output:\n```bash\nNAME                                 READY   STATUS    RESTARTS   AGE     ROLE\nredis-replication-redis-0            3/3     Running   0          3m38s   primary\nredis-replication-redis-1            3/3     Running   0          3m16s   secondary\nredis-replication-redis-sentinel-0   2/2     Running   0          4m35s\nredis-replication-redis-sentinel-1   2/2     Running   0          4m17s\nredis-replication-redis-sentinel-2   2/2     Running   0          3m59s\n```\n\nOnce the cluster status becomes Running, your Redis cluster is ready for use.\n\n:::tip\nIf you are creating the cluster for the very first time, it may take some time to pull images before running.\n\n:::\n",
    "path": "docs/release-1_0/kubeblocks-for-redis/_tpl/_verify-redis-replication-cluster",
    "description": "Monitor the cluster status until it transitions to the Running state: ```bash kubectl get cluster redis-replication -n demo -w ```  Expected Output:  ```bash NAME                CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS    AGE redis-replication   redis                Delete               Runn"
  },
  {
    "id": "docs_en_release-1_0_user_docs_concepts_concept",
    "title": "Concepts",
    "content": "\n# Concepts\n\nYou've already seen the benefits of using unified APIs to represent various databases in the section [\"How Unified APIs Reduce Your Learning Curve\"](./../overview/introduction). If you take a closer look at those examples, you'll notice two key concepts in the sample YAML files: **Cluster** and **Component**. For instance, `test-mysql` is a Cluster that includes a Component called `mysql` (with a componentDef of `apecloud-mysql`). Similarly, `test-redis` is also a Cluster, and it includes two Components: one called `redis` (with a componentDef of `redis-7`), which has two replicas, and another called `redis-sentinel` (with a componentDef of `redis-sentinel`), which has three replicas.\n\nIn this document, we will explain the reasons behind these two concepts and provide a brief introduction to the underlying API (i.e., CRD).\n\n## Motivation of KubeBlocks’ Layered API\nIn KubeBlocks, to support the management of various databases through a unified API, we need to abstract the topologies and characteristics of different databases.\n\nWe’ve observed that database systems deployed in production environments often use a topology composed of multiple components. For example, a production MySQL cluster might include several Proxy nodes (such as ProxySQL, MaxScale, Vitess, WeScale, etc.) alongside multiple MySQL server nodes (like MySQL Community Edition, Percona, MariaDB, ApeCloud MySQL, etc.) to achieve higher availability and read-write separation. Similarly, Redis deployments typically consist of a primary node and multiple read replicas, managed for high availability via Sentinel. Some users even use twemproxy for horizontal sharding to achieve greater capacity and throughput.\n\nThis modular approach is even more pronounced in distributed database systems, where the entire system is divided into distinct components with clear and singular responsibilities, such as data storage, query processing, transaction management, logging, and metadata management. These comp",
    "path": "docs/release-1_0/user_docs/concepts/concept",
    "description": " # Concepts  You've already seen the benefits of using unified APIs to represent various databases in the section [\"How Unified APIs Reduce Your Learning Curve\"](./../overview/introduction). If you take a closer look at those examples, you'll notice two key concepts in the sample YAML files: **Clust"
  },
  {
    "id": "docs_en_release-1_0_user_docs_overview_install-kubeblocks",
    "title": "Installation",
    "content": "\n\n\n# KubeBlocks\n\nThis guide covers KubeBlocks deployment on existing Kubernetes clusters. Choose your preferred installation method:\n\n- **Helm** (recommended for production)\n- **kbcli** (simplified CLI experience)\n\n\n## Prerequisites\n\n### Resource Requirements\n| Component     | Database   | Recommendation |\n|--------------|------------|---------------|\n| **Control Plane** | - | 1 node (4 cores, 4GB RAM, 50GB storage) |\n| **Data Plane**    | MySQL | 2 nodes (2 cores, 4GB RAM, 50GB storage) |\n|                   | PostgreSQL | 2 nodes (2 cores, 4GB RAM, 50GB storage) |\n|                   | Redis | 2 nodes (2 cores, 4GB RAM, 50GB storage) |\n|                   | MongoDB | 3 nodes (2 cores, 4GB RAM, 50GB storage) |\n\n- **Control Plane**: Nodes running KubeBlocks components\n- **Data Plane**: Nodes hosting database instances\n\n\n### System Requirements\n\nBefore installation, verify your environment meets these requirements:\n\n- Kubernetes cluster (v1.21+ recommended) - [create test cluster](../references/prepare-a-local-k8s-cluster) if needed\n- `kubectl` v1.21+ installed and configured with cluster access\n- Helm installed ([installation guide](https://helm.sh/docs/intro/install/))\n- Snapshot Controller installed ([installation guide](../references/install-snapshot-controller))\n\n## Install KubeBlocks\n\n\n\n```bash\n# Step 1: Install CRDs\nkubectl create -f https://github.com/apecloud/kubeblocks/releases/download/}/kubeblocks_crds.yaml\n\n# Step 2: Configure Helm Repository\nhelm repo add kubeblocks https://apecloud.github.io/helm-charts\nhelm repo update\n\n# Step 3: Deploy KubeBlocks\nhelm install kubeblocks kubeblocks/kubeblocks --namespace kb-system --create-namespace --version=}\n```\n\n\n:::note\n\nIf you are using K8s \\\n\n\n**Before You Begin**:\n- Install [KubeBlocks CLI](../../user_docs/references/install-kbcli)\n- Ensure kubectl is configured with cluster access\n\n```bash\nkbcli kubeblocks install --version=} --create-namespace\n```\n\n**Need a different version?**\n\nList available versions or fi",
    "path": "docs/release-1_0/user_docs/overview/install-kubeblocks",
    "description": "   # KubeBlocks  This guide covers KubeBlocks deployment on existing Kubernetes clusters. Choose your preferred installation method:  - **Helm** (recommended for production) - **kbcli** (simplified CLI experience)   ## Prerequisites  ### Resource Requirements | Component     | Database   | Recommend"
  },
  {
    "id": "docs_en_release-1_0_user_docs_overview_introduction",
    "title": "Introduction",
    "content": "\n\n# Introduction\n\n## What is KubeBlocks\n\nKubeBlocks is an open-source Kubernetes operator for databases (more specifically, for stateful applications, including databases and middleware like message queues), enabling users to run and manage multiple types of databases on Kubernetes. As far as we know, most database operators typically manage only one specific type of database. For example:\n- CloudNativePG, Zalando, CrunchyData, StackGres operator can manage PostgreSQL\n- Strimzi manages Kafka\n- Oracle and Percona MySQL operator manage MySQL\n\nIn contrast, KubeBlocks is designed to be a **general-purpose database operator**. This means that when designing the KubeBlocks API, we didn’t tie it to any specific database. Instead, we abstracted the common features of various databases, resulting in a universal, engine-agnostic API. Consequently, the operator implementation developed around this abstract API is also agnostic to the specific database engine.\n\n![Design of KubeBlocks, a general purpose database operator](/img/docs/en/kubeblocks_general_purpose_arch.png)\n\nIn above diagram, Cluster, Component, and InstanceSet are all CRDs provided by KubeBlocks. If you'd like to learn more about them, please refer to [concepts](../concepts/concept).\n\nKubeBlocks offers an Addon API to support the integration of various databases. For instance, we currently have the following KubeBlocks Addons for mainstream open-source database engines:\n- MySQL\n- PostgreSQL\n- Redis\n- MongoDB\n- Kafka\n- RabbitMQ\n- Minio\n- Elasticsearch\n- StarRocks\n- Qdrant\n- Milvus\n- ZooKeeper\n- etcd\n- ...\n\nFor a detailed list of Addons and their features, please refer to [supported addons](supported-addons.md).\n\nThe unified API makes KubeBlocks an excellent choice if you need to run multiple types of databases on Kubernetes. It can significantly reduce the learning curve associated with mastering multiple operators.\n\n## How unified APIs reduce your learning curve\n\nHere is an example of how to use KubeBlocks' Cluste",
    "path": "docs/release-1_0/user_docs/overview/introduction",
    "description": "  # Introduction  ## What is KubeBlocks  KubeBlocks is an open-source Kubernetes operator for databases (more specifically, for stateful applications, including databases and middleware like message queues), enabling users to run and manage multiple types of databases on Kubernetes. As far as we kno"
  },
  {
    "id": "docs_en_release-1_0_user_docs_overview_supported-addons",
    "title": "Supported Addons",
    "content": "\n# Supported Addons\n\nKubeBlocks uses Addons to extend support for various database engines. And there are currently over 30 Addons available in the KubeBlocks repository, which can be further categorized as follows sections.\n\nFor installing and enabling Addons, refer to [Addon installation tutorial](./../references/install-addons).\n\n## Relational Databases\n\nMySQL and PostgreSQL are the two most popular open-source relational databases in the world, and they have branches/variants.\n\n### MySQL and its variants\n\n**Addon List**\n\n| Addons          | Description                                                                                                                                                                                                                                                                                                                                                             |\n|:----------------|:---------------|\n| mysql           | This addon uses the community edition MySQL image officially released by Oracle.                                                    |\n| mariadb         | MariaDB is a high performance open source relational database management system that is widely used for web and application servers. |\n\n**Supported Features**\n\n:::note\n\nThe versions listed below may not be up-to-date, and some supported versions might be missing. For the latest addon versions, please refer to the [KubeBlocks addon GitHub repo](https://github.com/apecloud/kubeblocks-addons).\n\n:::\n\n| Addon (v0.9.0)     | Supported Versions                     | Vscale | Hscale | Volumeexpand | Stop/Start | Restart | Expose | Backup/Restore | Logs | Config | Upgrade (DB engine version) | Account | Failover | Switchover |\n|:------------------:|:--------------------------------------:|:------:|:------:|:------------:|:----------:|:-------:|:------:|:--------------:|:----:|:------:|:---------------------------:|:-------:|:--------:|:----------:|\n| mysql              | •",
    "path": "docs/release-1_0/user_docs/overview/supported-addons",
    "description": " # Supported Addons  KubeBlocks uses Addons to extend support for various database engines. And there are currently over 30 Addons available in the KubeBlocks repository, which can be further categorized as follows sections.  For installing and enabling Addons, refer to [Addon installation tutorial]"
  },
  {
    "id": "docs_en_release-1_0_user_docs_references_install-addons",
    "title": "Install Addons",
    "content": "\n\n# Install Addons\n\nWith the release of KubeBlocks v0.8.0, Addons are decoupled from KubeBlocks and some Addons are not installed by default. If you want to use these Addons, install Addons first by index. Or if you uninstalled some Addons, you can follow the steps in this tutorial to install them again.\n\nThis tutorial takes elasticsearch as an example. You can replace elasticsearch with the Addon you need.\n\nThe official index repo is [KubeBlocks index](https://github.com/apecloud/block-index). Addons are maintained in the [KubeBlocks Addon repo](https://github.com/apecloud/kubeblocks-addons).\n\n:::note\n\nMake sure the major version of Addons and KubeBlocks are the same.\n\nFor example, you can install an Addon v0.9.0 with KubeBlocks v0.9.2, but using mismatched major versions, such as an Addon v0.8.0 with KubeBlocks v0.9.2, may lead to errors.\n\n:::\n\n\n\n1. (Optional) Add the KubeBlocks repo. If you install KubeBlocks with Helm, just run `helm repo update`.\n\n   ```bash\n   helm repo add kubeblocks https://apecloud.github.io/helm-charts\n   helm repo update\n   ```\n\n2. View the Addon versions.\n\n   ```bash\n   helm search repo kubeblocks/elasticsearch --devel --versions\n   ```\n\n3. Install the Addon (take elasticsearch as example). Specify a version with `--version`.\n\n   ```bash\n   helm install kb-addon-es kubeblocks/elasticsearch --namespace kb-system --create-namespace --version \n   ```\n\n4. Verify whether this Addon is installed.\n\n   The STATUS is `deployed` and this Addon is installed successfully.\n\n   ```bash\n   helm list -A\n   >\n   NAME             NAMESPACE\tREVISION\tUPDATED                                STATUS  \t CHART                   APP VERSION\n   ...\n   kb-addon-es      kb-system\t1       \t2024-11-27 10:04:59.730127 +0800 CST   deployed\t elasticsearch-0.9.0     8.8.2\n   ```\n\n5. (Optional) You can run the command below to uninstall the Addon.\n\n   If you have created a related cluster, delete the cluster first.\n\n   ```bash\n   helm uninstall kb-addon-es --namespace kb-sy",
    "path": "docs/release-1_0/user_docs/references/install-addons",
    "description": "  # Install Addons  With the release of KubeBlocks v0.8.0, Addons are decoupled from KubeBlocks and some Addons are not installed by default. If you want to use these Addons, install Addons first by index. Or if you uninstalled some Addons, you can follow the steps in this tutorial to install them a"
  },
  {
    "id": "docs_en_release-1_0_user_docs_references_install-kbcli",
    "title": "Install and Configure the KubeBlocks CLI (kbcli)",
    "content": "\n\n# KubeBlocks Command Line (kbcli)\n\nkbcli is the official command-line tool for managing KubeBlocks clusters. It provides:\n- Cluster lifecycle management (create, scale, delete)\n- Configuration and troubleshooting tools\n- Version compatibility checks\n- Shell auto-completion support\n\n## Prerequisites\n\nBefore installing kbcli, ensure your system meets these requirements:\n\n- **All platforms**:\n  - Network access to download packages\n  - Administrator/sudo privileges\n- **Windows**:\n  - PowerShell 5.0 or later\n- **macOS/Linux**:\n  - curl or wget installed\n  - Homebrew (for macOS brew installation)\n\n## Install kbcli\n\n**Supported Platforms**\n\nkbcli is available for:\n- **macOS** (Intel and Apple Silicon)\n- **Windows** (x86-64)\n- **Linux** (x86-64 and ARM64)\n\n\nChoose your preferred installation method:\n\n- **curl** (recommended for most users)\n- **Homebrew** (macOS package manager)\n\n**Option 1: Install with curl**\n\nTo install the latest stable version:\n\n```bash\ncurl -fsSL https://kubeblocks.io/installer/install_cli.sh | bash\n```\n\nTo install a specific version:\n\n1. Check the available versions in [kbcli Release](https://github.com/apecloud/kbcli/releases/).\n2. Specify a version with `-s` and run the command below.\n\n   ```bash\n   curl -fsSL https://kubeblocks.io/installer/install_cli.sh | bash -s \n   ```\n\n:::note\n\n**Version Compatibility**\n\n- Always match kbcli version with your KubeBlocks deployment\n- Latest stable is recommended for new installations\n- Existing deployments require version matching:\n  - KubeBlocks v1.0.0 → kbcli v1.0.x\n  - KubeBlocks v0.9.x → kbcli v0.9.x\n- Mismatches may cause operational issues\n:::\n\n2. Run `kbcli version` to check the version of kbcli and ensure that it is successfully installed.\n\n:::tip\n**Troubleshooting**\nIf installation fails:\n1. Verify network connectivity\n2. Check firewall/proxy settings\n:::\n\n**Option 2: Install with Homebrew**\n\n1. Install ApeCloud tap, the Homebrew package of ApeCloud.\n\n   ```bash\n   brew tap apecloud/tap\n   ```\n\n2. I",
    "path": "docs/release-1_0/user_docs/references/install-kbcli",
    "description": "  # KubeBlocks Command Line (kbcli)  kbcli is the official command-line tool for managing KubeBlocks clusters. It provides: - Cluster lifecycle management (create, scale, delete) - Configuration and troubleshooting tools - Version compatibility checks - Shell auto-completion support  ## Prerequisite"
  },
  {
    "id": "docs_en_release-1_0_user_docs_references_install-snapshot-controller",
    "title": "Install Snapshot Controller",
    "content": "\n# Install Snapshot Controller\n\nThe Snapshot Controller manages CSI Volume Snapshots, enabling creation, restoration, and deletion of Persistent Volume (PV) snapshots. KubeBlocks' DataProtection Controller leverages this component for database snapshot operations.\n\n**Step 1: Check Prerequisites**\nVerify if required CRDs exist:\n\n```bash\nkubectl get crd volumesnapshotclasses.snapshot.storage.k8s.io\nkubectl get crd volumesnapshots.snapshot.storage.k8s.io\nkubectl get crd volumesnapshotcontents.snapshot.storage.k8s.io\n```\n\nIf your cluster lacks these CRDs, you'll need to install them first:\n\n```bash\n# v8.2.0 is the latest version of the external-snapshotter, you can replace it with the version you need.\nkubectl create -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml\nkubectl create -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml\nkubectl create -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v8.2.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml\n```\n\n\n:::note\n\n**Optional Installation**\n\nIf snapshot backups aren't required, you can install just the CRDs and skip following steps.\n\n:::\n\n\n**Step 2: Deploy Snapshot Controller**\n\nInstall using Helm with these steps:\n\n```bash\nhelm repo add piraeus-charts https://piraeus.io/helm-charts/\nhelm repo update\n# Update the namespace to an appropriate value for your environment (e.g. kb-system)\nhelm install snapshot-controller piraeus-charts/snapshot-controller -n kb-system --create-namespace\n```\n\nFor advanced configuration options, see the [Snapshot Controller documentation](https://artifacthub.io/packages/helm/piraeus-charts/snapshot-controller#configuration).\n\n**Step 3: Verify Deployment**\n\nCheck if the snapshot-controller Pod is running:\n\n```bash\nkubectl get pods -n kb-system | grep snapshot-contr",
    "path": "docs/release-1_0/user_docs/references/install-snapshot-controller",
    "description": " # Install Snapshot Controller  The Snapshot Controller manages CSI Volume Snapshots, enabling creation, restoration, and deletion of Persistent Volume (PV) snapshots. KubeBlocks' DataProtection Controller leverages this component for database snapshot operations.  **Step 1: Check Prerequisites** Ve"
  },
  {
    "id": "docs_en_release-1_0_user_docs_references_kubernetes_and_operator_101",
    "title": "Kubernetes and Operator 101",
    "content": "\n# Kubernetes and Operator 101\n\n## K8s\n\nWhat is Kubernetes? Some say it's a container orchestration system, others describe it as a distributed operating system, while some view it as a multi-cloud PaaS (Platform as a Service) platform, and others consider it a platform for building PaaS solutions.\n\nThis article will introduce the key concepts and building blocks within Kubernetes.\n\n## K8s Control Plane\n\nThe Kubernetes Control Plane is the brain and heart of Kubernetes. It manages the overall operation of the cluster, including processing API requests, storing configuration data, and ensuring the cluster's desired state. Key components include the API Server (which handles communication), etcd (which stores all cluster data), the Controller Manager (which enforces the desired state), the Scheduler (which assigns workloads to Nodes), and the Cloud Controller Manager (which manages cloud-specific integrations, such as load balancers, storage, and networking). Together, these components orchestrate the deployment, scaling, and management of containers across the cluster.\n\n## Node\n\nSome describe Kubernetes as a distributed operating system, capable of managing many Nodes. A Node is a physical or virtual machine that acts as a worker within the cluster. Each Node runs essential services, including the container runtime (such as Docker or containerd), the kubelet, and the kube-proxy. The kubelet ensures that containers are running as specified in a Pod, the smallest deployable unit in Kubernetes. The kube-proxy handles network routing, maintaining network rules, and enabling communication between Pods and services. Nodes provide the computational resources needed to run containerized applications and are managed by the Kubernetes Master, which distributes tasks, monitors Node health, and maintains the desired state of the cluster.\n\n:::note\n\nIn certain contexts, the term \"Node\" can be confusing when discussing Kubernetes (K8s) alongside databases. In Kubernetes, a \"Node\" r",
    "path": "docs/release-1_0/user_docs/references/kubernetes_and_operator_101",
    "description": " # Kubernetes and Operator 101  ## K8s  What is Kubernetes? Some say it's a container orchestration system, others describe it as a distributed operating system, while some view it as a multi-cloud PaaS (Platform as a Service) platform, and others consider it a platform for building PaaS solutions. "
  },
  {
    "id": "docs_en_release-1_0_user_docs_references_prepare-a-local-k8s-cluster",
    "title": "Create a test Kubernetes cluster",
    "content": "\n\n# Create a test Kubernetes cluster\n\nThis tutorial introduces how to create a local Kubernetes test cluster using Minikube, K3d, and Kind. These tools make it easy to try out KubeBlocks on your local host, offering a great solution for development, testing, and experimentation without the complexity of creating a full production-grade cluster.\n\n## Before you start\n\nMake sure you have the following tools installed on your local host:\n\n- Docker: All three tools rely on Docker to create containerized Kubernetes clusters.\n- kubectl: The Kubernetes command-line tool for interacting with clusters. Refer to the [kubectl installation guide](https://kubernetes.io/docs/tasks/tools/)\n\n\n\n## Create a Kubernetes cluster using Kind\n\nKind stands for Kubernetes IN Docker. It runs Kubernetes clusters within Docker containers, making it an ideal tool for local Kubernetes testing.\n\n1. Install Kind. For details, you can refer to [Kind Quick Start](https://kind.sigs.k8s.io/docs/user/quick-start/).\n\n\n\n   ```bash\n   brew install kind\n   ```\n\n   \n\n\n   ```bash\n   # For AMD64 / x86_64\n   [ $(uname -m) = x86_64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.24.0/kind-linux-amd64\n   # For ARM64\n   [ $(uname -m) = aarch64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.24.0/kind-linux-arm64\n   chmod +x ./kind\n   sudo cp ./kind /usr/local/bin/kind\n   rm -rf kind\n   ```\n\n   \n\n\n   You can use chocolatey to install Kind.\n\n   ```bash\n   choco install kind\n   ```\n\n   \n\n   \n\n2. Create a Kind cluster.\n\n   ```bash\n   kind create cluster --name mykindcluster\n   ```\n\n   This command creates a single-node Kubernetes cluster running in a Docker container.\n\n3. Check whether the cluster is started and running.\n\n   ```bash\n   kubectl get nodes\n   >\n   NAME                          STATUS   ROLES           AGE   VERSION\n   mykindcluster-control-plane   Ready    control-plane   25s   v1.31.0\n   ```\n\n   You can see a node named `mykindcluster-control-plane` from the output, which means the cluster is cre",
    "path": "docs/release-1_0/user_docs/references/prepare-a-local-k8s-cluster",
    "description": "  # Create a test Kubernetes cluster  This tutorial introduces how to create a local Kubernetes test cluster using Minikube, K3d, and Kind. These tools make it easy to try out KubeBlocks on your local host, offering a great solution for development, testing, and experimentation without the complexit"
  },
  {
    "id": "docs_en_release-1_0_user_docs_references_terminology",
    "title": "Terminology",
    "content": "\n# Terminology\n\n### Addon\n\nAn addon is an efficient and open extension mechanism. With the KubeBlocks addon, developers can quickly add a new database engine to KubeBlocks and obtain specific foundational management functionalities of that database engine, including but not limited to lifecycle management, data backup and recovery, metrics and log collection, etc.\n### ActionSet\n\nAn ActionSet declares a set of commands to perform backup and restore operations using specific tools, such as commands to backup MySQL using xtrabackup, as well as commands to restore data from the backup.\n\n### BackupPolicy\n\nA BackupPolicy represents a backup strategy for a Cluster, including details such as the backup repository (BackupRepo), backup targets, and backup methods. Multiple backup methods can be defined within a backup policy, with each method referring to a corresponding ActionSet. When creating a backup, the backup policy and backup method can be specified for the backup process.\n\n### BackupRepo\n\nBackupRepo is the storage repository for backup data. Its principle involves using a CSI driver to upload backup data to various storage systems, such as object storage systems like S3, GCS, as well as storage servers like FTP, NFS, and others.\n\n### BackupSchedule\n\nBackupSchedule declares the configuration for automatic backups in a Cluster, including backup frequency, retention period, backup policy, and backup method. The BackupSchedule Controller creates a CronJob to automatically backup the Cluster based on the configuration specified in the Custom Resource (CR).\n\n### Cluster\n\nCluster is composed by [components](#component-is-the-fundamental-assembly-component-used-to-build-a-data-storage-and-processing-system-a-component-utilizes-a-statefulset-either-native-to-kubernetes-or-specified-by-the-customer-such-as-openkruise-to-manage-one-to-multiple-pods).\n\n### Component\n\nA component is the fundamental assembly component used to build a data storage and processing system. A Component",
    "path": "docs/release-1_0/user_docs/references/terminology",
    "description": " # Terminology  ### Addon  An addon is an efficient and open extension mechanism. With the KubeBlocks addon, developers can quickly add a new database engine to KubeBlocks and obtain specific foundational management functionalities of that database engine, including but not limited to lifecycle mana"
  },
  {
    "id": "docs_en_release-1_0_user_docs_troubleshooting_handle-a-cluster-exception",
    "title": "FAQs",
    "content": "\n\n# FAQs\n\n### List of K8s Resources created by KubeBlocks when creating a Cluster\n\nTo get the full list of associated resources created by KubeBlocks for given cluster:\n\n```bash\nkubectl get cmp,its,po -l app.kubernetes.io/instance= -n demo # cluster and worload\nkubectl get backuppolicy,backupschedule,backup -l app.kubernetes.io/instance= -n demo # data protection resources\nkubectl get componentparameter,parameter -l app.kubernetes.io/instance= -n demo # configuration resources\nkubectl get opsrequest -l app.kubernetes.io/instance= -n demo # opsrequest resources\nkubectl get svc,secret,cm,pvc -l app.kubernetes.io/instance= -n demo # k8s native resources\n```\n\nFor troubleshooting,\n\n1. describe resource such as Cluster, Component, e.g.\n```bash\nkubectl describe TYPE NAME\n```\n\n2. check database instance logs\n```bash\nkubectl logs  -c \n```\n\n3. check KubeBlocks logs\n```bash\nkubectl -n kb-system logs deployments/kubeblocks -f\n```\n\n### How to get the detail of each backup method\n\nDetails of each backup method are defined in `ActionSet` in KubeBlocks.\n\nFor example, To get the `ActionSet` which defines the behavior of backup method named `wal-g-archive` in PostgreSQL, for instance:\n\n```bash\nkubectl -n demo get bp pg-cluster-postgresql-backup-policy -oyaml | yq '.spec.backupMethods[] | select(.name==\"wal-g-archive\") | .actionSetName'\n```\n\nActionSet defined:\n\n- backup type\n- both backup and restore procedures\n- environment variables used in procedures\n\nAnd you may check details of each ActionsSet to find out how backup and restore will be performed.\n\n\n### How to Check Compatible versions\n\nVersions and it compatibility rules are embedded in `ComponentVersion` CR in KubeBlocks.\nTo the the list of compatible versions:\n\n```bash\nkubectl get cmpv postgresql -ojson | jq '.spec.compatibilityRules'\n```\n\n\n\nExample Output\n\n```json\n[\n  ,\n  \n]\n```\n\n\n\nReleases are grouped by component definitions, and each group has a list of compatible releases.\nIn this example, it shows you can upgrade from ver",
    "path": "docs/release-1_0/user_docs/troubleshooting/handle-a-cluster-exception",
    "description": "  # FAQs  ### List of K8s Resources created by KubeBlocks when creating a Cluster  To get the full list of associated resources created by KubeBlocks for given cluster:  ```bash kubectl get cmp,its,po -l app.kubernetes.io/instance= -n demo # cluster and worload kubectl get backuppolicy,backupschedul"
  },
  {
    "id": "docs_en_release-1_0_user_docs_troubleshooting_known-issues",
    "title": "Known Issues",
    "content": "\n\n# Known Issues\n\n## Issue 1: KubeBlocks creates enormous number of secrets\n\n### Problem Description\nKubeBlocks keeps creating an enormous number of secrets for each cluster and never stops. You may see the following information in **KubeBlocks** logs:\n\n```bash\nINFO reconcile object *v1.ServiceAccount with action UPDATE OK\n```\n\n### Affected Version\n- KubeBlocks v1.0.0\n- Kubernetes versions \\≤ 1.24\n\n### Root Cause\nBefore Kubernetes version 1.24, Kubernetes automatically generated Secret-based tokens for ServiceAccounts, as documented in [Kubernetes Service Account Tokens](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/).\n\n### Solution\nUpgrade KubeBlocks to `v1.0.0-beta.3` or later.\n\n---\n\n## Issue 2: PostgreSQL fails to start with special characters in password\n\n### Problem Description\nPostgreSQL may fail to start when the password contains certain special characters. By checking POD logs:\n```bash\nFile \"/usr/lib/python3/dist-packages/yaml/scanner.py\", line 116, in check_token\n    self.fetch_more_tokens()\n  File \"/usr/lib/python3/dist-packages/yaml/scanner.py\", line 231, in fetch_more_tokens\n    return self.fetch_anchor()\n  File \"/usr/lib/python3/dist-packages/yaml/scanner.py\", line 621, in fetch_anchor\n    self.tokens.append(self.scan_anchor(AnchorToken))\n  File \"/usr/lib/python3/dist-packages/yaml/scanner.py\", line 929, in scan_anchor\n    raise ScannerError(\"while scanning an %s\" % name, start_mark,\nyaml.scanner.ScannerError: while scanning an anchor\n  in \"\", line 45, column 17:\n          password: &amp;JgE#F5x&amp;eNwis*2dW!7&amp ...\n                    ^\n```\n\n### Affected Version\n- All KubeBlocks versions with PostgreSQL clusters\n\n### Solution\nUse passwords that do not contain special characters that may cause parsing issues in PostgreSQL configuration files.",
    "path": "docs/release-1_0/user_docs/troubleshooting/known-issues",
    "description": "  # Known Issues  ## Issue 1: KubeBlocks creates enormous number of secrets  ### Problem Description KubeBlocks keeps creating an enormous number of secrets for each cluster and never stops. You may see the following information in **KubeBlocks** logs:  ```bash INFO reconcile object *v1.ServiceAccou"
  },
  {
    "id": "docs_en_release-1_0_user_docs_upgrade_upgrade-to-0_8",
    "title": "Upgrade to v0.8",
    "content": "\n\n# Upgrade to KubeBlocks v0.8\n\nIn this tutorial, you will learn how to upgrade to KubeBlocks v0.8.\n\n:::note\n\nExecute `helm -n kb-system list | grep kubeblocks` or `kbcli version` to check the current KubeBlocks version you are running, and then upgrade it.\n\n:::\n\n## Upgrade from KubeBlocks v0.7\n\n\n\n1. Set keepAddons.\n\n    KubeBlocks v0.8 streamlines the default installed engines and separates the addons from KubeBlocks operators to KubeBlocks-Addons repo, such as greptime, influxdb, neon, oracle-mysql, orioledb, tdengine, mariadb, nebula, risingwave, starrocks, tidb, and zookeeper. To avoid deleting addon resources that are already in use during the upgrade, execute the following commands:\n\n- Check the current KubeBlocks version.\n\n    ```bash\n    helm -n kb-system list | grep kubeblocks\n    ```\n\n- Set the value of keepAddons as true.\n\n    ```bash\n    helm repo add kubeblocks https://apecloud.github.io/helm-charts\n    helm repo update kubeblocks\n    helm -n kb-system upgrade kubeblocks kubeblocks/kubeblocks --version \\ --set keepAddons=true\n    ```\n\n    Replace \\ with your current KubeBlocks version, such as 0.7.2.\n\n- Check addons.\n\n    Execute the following command to ensure that the addon annotations contain `\"helm.sh/resource-policy\": \"keep\"`.\n\n    ```bash\n    kubectl get addon -o json | jq '.items[] | '\n    ```\n\n2. Install CRD.\n\n    To reduce the size of Helm chart, KubeBlocks v0.8 removes CRD from the Helm chart. Before upgrading, you need to install CRD.\n\n    ```bash\n    kubectl replace -f https://github.com/apecloud/kubeblocks/releases/download/v0.8.1/kubeblocks_crds.yaml\n    ```\n\n3. Upgrade KubeBlocks.\n\n    ```bash\n    helm -n kb-system upgrade kubeblocks kubeblocks/kubeblocks --version 0.8.1 --set dataProtection.image.datasafed.tag=0.1.0\n    ```\n\n:::note\n\nTo avoid affecting existing database clusters, when upgrading to KubeBlocks v0.8, the versions of already-installed addons will not be upgraded by default. If you want to upgrade the addons to the versions b",
    "path": "docs/release-1_0/user_docs/upgrade/upgrade-to-0_8",
    "description": "  # Upgrade to KubeBlocks v0.8  In this tutorial, you will learn how to upgrade to KubeBlocks v0.8.  :::note  Execute `helm -n kb-system list | grep kubeblocks` or `kbcli version` to check the current KubeBlocks version you are running, and then upgrade it.  :::  ## Upgrade from KubeBlocks v0.7    1"
  },
  {
    "id": "docs_en_release-1_0_user_docs_upgrade_upgrade-to-0_9_0",
    "title": "Upgrade to v0.9.0",
    "content": "\n\n# Upgrade to KubeBlocks v0.9.0\n\nIn this tutorial, you will learn how to upgrade to KubeBlocks v0.9.0.\n\n:::note\n\nExecute `helm -n kb-system list | grep kubeblocks` or `kbcli version` to check the current KubeBlocks version you are running, and then upgrade KubeBlocks.\n\n:::\n\n## Compatibility\n\nKubeBlocks 0.9.0 is compatible with KubeBlocks 0.8 APIs, but compatibility with APIs from versions prior to v0.8 is not guaranteed. If you are using Addons from KubeBlocks 0.7 or earlier (0.6, etc), DO [upgrade KubeBlocks and all Addons to v0.8 first](../upgrade/upgrade-to-0_8) to ensure service availability before upgrading to v0.9.0.\n\n## Upgrade from KubeBlocks v0.8\n\n\n\n1. Add the `\"helm.sh/resource-policy\": \"keep\"` for Addons.\n\n    KubeBlocks v0.8 streamlines the default installed engines. To avoid deleting Addon resources that are already in use during the upgrade, execute the following commands first.\n\n    - Add the `\"helm.sh/resource-policy\": \"keep\"` for Addons. You can replace `-l app.kubernetes.io/name=kubeblocks` with your actual filter name.\n\n         ```bash\n         kubectl annotate addons.extensions.kubeblocks.io -l app.kubernetes.io/name=kubeblocks helm.sh/resource-policy=keep\n         ```\n\n    - Check Addons.\n\n         Execute the following command to ensure that the Addon annotations contain `\"helm.sh/resource-policy\": \"keep\"`.\n\n         ```bash\n         kubectl get addon -o json | jq '.items[] | '\n         ```\n\n2. Delete the incompatible OpsDefinition.\n\n   ```bash\n   kubectl delete opsdefinitions.apps.kubeblocks.io kafka-quota kafka-topic kafka-user-acl switchover\n   ```\n\n3. Install the StorageProvider CRD before the upgrade.\n\n    If the network is slow, it's recommended to download the CRD YAML file on your localhost before further operations.\n\n    ```bash\n    kubectl create -f https://github.com/apecloud/kubeblocks/releases/download/v0.9.0/dataprotection.kubeblocks.io_storageproviders.yaml\n    ```\n\n4. Upgrade KubeBlocks.\n\n    ```bash\n    helm -n kb-system upgr",
    "path": "docs/release-1_0/user_docs/upgrade/upgrade-to-0_9_0",
    "description": "  # Upgrade to KubeBlocks v0.9.0  In this tutorial, you will learn how to upgrade to KubeBlocks v0.9.0.  :::note  Execute `helm -n kb-system list | grep kubeblocks` or `kbcli version` to check the current KubeBlocks version you are running, and then upgrade KubeBlocks.  :::  ## Compatibility  KubeBl"
  },
  {
    "id": "docs_en_release-1_0_user_docs_upgrade_upgrade-to-v09-version",
    "title": "Upgrade to v0.9.x",
    "content": "\n\n# Upgrade to KubeBlocks v0.9.x\n\n:::note\n\n- Before upgrading, check your current KubeBlocks version:\n\n   Run `helm -n kb-system list | grep kubeblocks` or `kbcli version`.\n- For upgrading to different versions:\n\n   - For v0.9.2 and v0.9.1, follow this upgrade tutorial, replacing the version number with v0.9.2 or v0.9.1 respectively.\n   - [v0.9.0 upgrade guide](./upgrade-to-0_9_0)\n   - [v0.8.x upgrade guide](./upgrade-to-0_8).\n\n   Installing the latest version is recommended for better performance and features.\n\n:::\n\n## Compatibility\n\nKubeBlocks v0.9.3 is compatible with KubeBlocks v0.8 APIs, but compatibility with APIs from versions prior to v0.8 is not guaranteed. If you are using Addons from KubeBlocks v0.7 or earlier (v0.6, etc), DO [upgrade KubeBlocks and all Addons to v0.8 first](./upgrade-to-0_8) to ensure service availability before upgrading to v0.9.\n\nIf you are upgrading from v0.8 to v0.9, it's recommended to enable webhook to ensure the availability.\n\n## Upgrade from KubeBlocks v0.9.x\n\n\n\n1. View Addon and check whether the `\"helm.sh/resource-policy\": \"keep\"` annotation exists.\n\n    KubeBlocks streamlines the default installed engines. Add the `\"helm.sh/resource-policy\": \"keep\"` annotation to avoid deleting Addon resources that are already in use during the upgrade.\n\n    Check whether the `\"helm.sh/resource-policy\": \"keep\"` annotation is added.\n\n    ```bash\n    kubectl get addon -o json | jq '.items[] | '\n    ```\n\n    If the annotation doesn't exist, run the command below to add it. You can replace `-l app.kubernetes.io/name=kubeblocks` with your actual filter name.\n\n    ```bash\n    kubectl annotate addons.extensions.kubeblocks.io -l app.kubernetes.io/name=kubeblocks helm.sh/resource-policy=keep\n    ```\n\n2. Install CRD.\n\n    To reduce the size of Helm chart, KubeBlocks v0.8 removes CRD from the Helm chart. Before upgrading, you need to install CRD.\n\n    ```bash\n    kubectl replace -f https://github.com/apecloud/kubeblocks/releases/download/v0.9.3/kubeblock",
    "path": "docs/release-1_0/user_docs/upgrade/upgrade-to-v09-version",
    "description": "  # Upgrade to KubeBlocks v0.9.x  :::note  - Before upgrading, check your current KubeBlocks version:     Run `helm -n kb-system list | grep kubeblocks` or `kbcli version`. - For upgrading to different versions:     - For v0.9.2 and v0.9.1, follow this upgrade tutorial, replacing the version number "
  },
  {
    "id": "docs_en_preview_user_docs_concepts_backup-and-restore_introduction",
    "title": "Introduction",
    "content": "\n# Introduction\n\nKubeBlocks provides the backup and restore function to ensure the safety and reliability of your data. The backup and restore function of KubeBlocks relies on BackupRepo and before using the full backup and restore function, you need to [configure BackupRepo first](./backup/backup-repo.md).\n\nKubeBlocks adopts physical backup which takes the physical files in a database as the backup object. You can choose one backup option based on your demands to back up the cluster data on demand or by schedule.\n\n* [On-demand backup](./backup/on-demand-backup.md): Based on different backup options, on-demand backup can be further divided into backup tool and snapshot backup.\n  * Backup tool: You can use the backup tool of the corresponding data product, such as MySQL XtraBackup and PostgreSQL pg_basebackup. KubeBlocks supports configuring backup tools for different data products.\n  * Snapshot backup: If your data is stored in a cloud disk that supports snapshots, you can create a data backup by snapshots. Snapshot backup is usually faster than a backup tool, and thus is recommended.\n\n* [Scheduled backup](./backup/scheduled-backup.md): You can specify retention time, backup method, time, and other parameters to customize your backup schedule.\n\nAs for the restore function, KubeBlocks supports restoring data from the backup set.\n\n* Restore\n  * [Restore data from the backup set](./restore/restore-data-from-backup-set.md)\n\nFollow the steps below to back up and restore your cluster.\n\n1. [Configure BackupRepo](./backup/backup-repo.md).\n2. [Configure BackupPolicy](./backup/configure-backuppolicy.md).\n3. Backup your cluster [on demand](./backup/on-demand-backup.md) or [by schedule](./backup/scheduled-backup.md).\n4. Restore your data by [PITR](./restore/pitr.md) or from the [backup set](./restore/restore-data-from-backup-set.md).\n",
    "path": "docs/preview/user_docs/concepts/backup-and-restore/introduction",
    "description": " # Introduction  KubeBlocks provides the backup and restore function to ensure the safety and reliability of your data. The backup and restore function of KubeBlocks relies on BackupRepo and before using the full backup and restore function, you need to [configure BackupRepo first](./backup/backup-r"
  },
  {
    "id": "docs_en_preview_user_docs_concepts_instance-template_how-to-use-instance-template",
    "title": "Apply instance template",
    "content": "\n# Apply instance template\n\nInstance templates can be applied to many scenarios. In this section, we take a RisingWave cluster as an example.\n\nKubeBlocks supports the management of RisingWave clusters. The RisingWave addon is contributed by the RisingWave official team. For RisingWave to function optimally, it relies on an external storage solution, such as AWS S3 or Alibaba Cloud OSS, to serve as its state backend. When creating a RisingWave cluster, it is necessary to configure credentials and other information for the external storage to ensure normal operation, and this information may vary for each cluster.\n\nIn the official image of RisingWave, this information can be injected via environment variables. Therefore, in KubeBlocks 0.9, we can configure corresponding environment variables in the instance template and set the values of these environment variables each time a cluster is created, so as to inject credential information into the container of RisingWave.\n\n## An example\n\nIn the default template of RisingWave addon, [the environment variables](https://github.com/apecloud/kubeblocks-addons/blob/main/addons/risingwave/templates/cmpd-compute.yaml#L26) are configured as follows:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1alpha1\nkind: ComponentDefinition\nmetadata:\n  name: risingwave\n# ...\nspec:\n#...\n  runtime:\n    containers:\n      - name: compute\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        command:\n        - /risingwave/bin/risingwave\n        - compute-node\n        env:\n        - name: RUST_BACKTRACE\n          value: \"1\"\n        - name: RW_CONFIG_PATH\n          value: /risingwave/config/risingwave.toml\n        - name: RW_LISTEN_ADDR\n          value: 0.0.0.0:5688\n        - name: RW_ADVERTISE_ADDR\n          value: $(KB_POD_FQDN):5688\n        - name: RW_META_ADDR\n          value: load-balance+http://$(metaSvc)-headless:5690\n        - name: RW_METR",
    "path": "docs/preview/user_docs/concepts/instance-template/how-to-use-instance-template",
    "description": " # Apply instance template  Instance templates can be applied to many scenarios. In this section, we take a RisingWave cluster as an example.  KubeBlocks supports the management of RisingWave clusters. The RisingWave addon is contributed by the RisingWave official team. For RisingWave to function op"
  },
  {
    "id": "docs_en_preview_user_docs_concepts_instance-template_introduction",
    "title": "Introduction",
    "content": "\n# Introduction\n\n## What is an instance template\n\nAn *instance* serves as the fundamental unit in KubeBlocks, comprising a Pod along with several auxiliary objects. To simplify, you can initially think of it as a Pod, and henceforth, we'll consistently refer to it as an \"Instance.\"\n\nStarting from version 0.9, we're able to establish multiple instance templates for a particular component within a cluster. These instance templates include several fields such as Name, Replicas, Annotations, Labels, Env, Tolerations, NodeSelector, etc. These fields will ultimately override the corresponding ones in the default template (originating from ClusterDefinition and ComponentDefinition) to generate the final template for rendering the instance.\n\n## Why do we introduce the instance template\n\nIn KubeBlocks, a *Cluster* is composed of several *Components*, where each *Component* ultimately oversees multiple *Pods* and auxiliary objects.\n\nPrior to version 0.9, these pods were rendered from a shared PodTemplate, as defined in either ClusterDefinition or ComponentDefinition. However, this design can’t meet the following demands:\n\n - For Clusters rendered from the same addon, setting separate scheduling configurations such as *NodeName*, *NodeSelector*, or *Tolerations*.\n - For Components rendered from the same addon, adding custom *Annotations*, *Labels*, or ENV to the Pods they manage.\n - For Pods managed by the same Component, configuring different *CPU*, *Memory*, and other *Resource Requests* and *Limits*.\n\nWith various similar requirements emerging, the Cluster API introduced the Instance Template feature from version 0.9 onwards to cater to these needs.\n",
    "path": "docs/preview/user_docs/concepts/instance-template/introduction",
    "description": " # Introduction  ## What is an instance template  An *instance* serves as the fundamental unit in KubeBlocks, comprising a Pod along with several auxiliary objects. To simplify, you can initially think of it as a Pod, and henceforth, we'll consistently refer to it as an \"Instance.\"  Starting from ve"
  },
  {
    "id": "docs_en_preview_user_docs_concepts_in-place-update_ignore-vertical-scale",
    "title": "Enable in-place update",
    "content": "\n# Enable in-place update\n\nIn Kubernetes versions below 1.27, we have seen support for in-place updates of Resources in many Kubernetes distributions. Different distributions may adopt different approaches to implement this feature.\n\nTo accommodate these Kubernetes distributions, KubeBlocks has introduced the `IgnorePodVerticalScaling` feature switch. When this feature is enabled, KubeBlocks ignores updates to CPU and Memory in Resources during instance updates, ensuring that the Resources configuration of the final rendered Pod remains consistent with the Resources configuration of the currently running Pod.\n",
    "path": "docs/preview/user_docs/concepts/in-place-update/ignore-vertical-scale",
    "description": " # Enable in-place update  In Kubernetes versions below 1.27, we have seen support for in-place updates of Resources in many Kubernetes distributions. Different distributions may adopt different approaches to implement this feature.  To accommodate these Kubernetes distributions, KubeBlocks has intr"
  },
  {
    "id": "docs_en_preview_user_docs_concepts_in-place-update_overview",
    "title": "Introduction",
    "content": "\n# Overview\n\nIn its earlier versions, KubeBlocks ultimately generated Workloads as StatefulSets. For StatefulSets, any change in the segment of PodTemplate may result in the update of all pods, and the method of update is called `Recreate`, that deletes all current pods and creates a new one. This is obviously not the best practice for database management, which has a high requirement on system availability.\nTo address this issue, KubeBlocks introduced the instance in-place update feature starting from version 0.9, reducing the impact on system availability during instance updates.\n\n## Which fields of an instance support in-place updates?\n\nIn principle, KubeBlocks instance in-place updates leverage [the Kubernetes Pod API's in-place update capability](https://kubernetes.io/docs/concepts/workloads/pods/#pod-update-and-replacement). Therefore, the specific supported fields are as follows:\n\n* `annotations`\n* `labels`\n* `spec.activeDeadlineSeconds`\n* `spec.initContainers[*].image`\n* `spec.containers[*].image`\n* `spec.tolerations (only supports adding Toleration)`\n\nStarting from Kubernetes version 1.27, support for in-place updates of CPU and Memory can be further enabled through the `InPlacePodVerticalScaling` feature switch. KubeBlocks also supports the `InPlacePodVerticalScaling` feature switch which further supports the following capabilities:\n\nFor Kubernetes versions equal to or greater than 1.27 with InPlacePodVerticalScaling enabled, the following fields' in-place updates are supported:\n\n* `spec.containers[*].resources.requests[\"cpu\"]`\n* `spec.containers[*].resources.requests[\"memory\"]`\n* `spec.containers[*].resources.limits[\"cpu\"]`\n* `spec.containers[*].resources.limits[\"memory\"]`\n\nIt is important to note that after successful resource resizing, some applications may need to be restarted to recognize the new resource configuration. In such cases, further configuration of container `restartPolicy` is required in ClusterDefinition or ComponentDefinition.\n\nFor PVC, ",
    "path": "docs/preview/user_docs/concepts/in-place-update/overview",
    "description": " # Overview  In its earlier versions, KubeBlocks ultimately generated Workloads as StatefulSets. For StatefulSets, any change in the segment of PodTemplate may result in the update of all pods, and the method of update is called `Recreate`, that deletes all current pods and creates a new one. This i"
  },
  {
    "id": "docs_en_preview_user_docs_references_api-reference_add-on",
    "title": "Add-On API Reference",
    "content": "\n\n\nPackages:\n\n\n\n\nextensions.kubeblocks.io/v1alpha1\n\n\n\nextensions.kubeblocks.io/v1alpha1\n\n\nResource Types:\n\n\nAddon\n\n\n\nAddon\n\n\n\n\n\nAddon is the Schema for the add-ons API.\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`extensions.kubeblocks.io/v1alpha1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`Addon`\n\n\n\n\n\n\n\n`metadata`\n\n\nKubernetes meta/v1.ObjectMeta\n\n\n\n\n\n\nRefer to the Kubernetes API documentation for the fields of the\n`metadata` field.\n\n\n\n\n\n\n\n\n`spec`\n\n\nAddonSpec\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`description`\n\nstring\n\n\n\n\n\n(Optional)\n\n\nSpecifies the description of the add-on.\n\n\n\n\n\n\n\n\n\n`type`\n\n\nAddonType\n\n\n\n\n\n\n\n\nDefines the type of the add-on. The only valid value is &lsquo;helm&rsquo;.\n\n\n\n\n\n\n\n\n\n`version`\n\nstring\n\n\n\n\n\n(Optional)\n\n\nIndicates the version of the add-on.\n\n\n\n\n\n\n\n\n\n`provider`\n\nstring\n\n\n\n\n\n(Optional)\n\n\nSpecifies the provider of the add-on.\n\n\n\n\n\n\n\n\n\n`helm`\n\n\nHelmTypeInstallSpec\n\n\n\n\n\n\n(Optional)\n\n\nRepresents the Helm installation specifications. This is only processed\nwhen the type is set to &lsquo;helm&rsquo;.\n\n\n\n\n\n\n\n\n\n`defaultInstallValues`\n\n\n[]AddonDefaultInstallSpecItem\n\n\n\n\n\n\n\n\nSpecifies the default installation parameters.\n\n\n\n\n\n\n\n\n\n`install`\n\n\nAddonInstallSpec\n\n\n\n\n\n\n(Optional)\n\n\nDefines the installation parameters.\n\n\n\n\n\n\n\n\n\n`installable`\n\n\nInstallableSpec\n\n\n\n\n\n\n(Optional)\n\n\nRepresents the installable specifications of the add-on. This includes\nthe selector and auto-install settings.\n\n\n\n\n\n\n\n\n\n`cliPlugins`\n\n\n[]CliPlugin\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies the CLI plugin installation specifications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`status`\n\n\nAddonStatus\n\n\n\n\n\n\n\n\n\n\n\n\n\nAddonDefaultInstallSpecItem\n\n\n\n\n\n(Appears on:AddonSpec)\n\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`AddonInstallSpec`\n\n\nAddonInstallSpec\n\n\n\n\n\n\n\n\n\n(Members of `AddonInstallSpec` are embedded into this type.)\n\n\n\n\n\n\n\n\n\n\n`selectors`\n\n\n[]SelectorRequirement\n\n\n\n\n\n\n(Optional)\n\n\nIndicates the default selectors for add-on installations. If multiple selectors are provided,\nall selectors must evaluate to true.\n\n\n\n\n\n\n\n\nAddonInstallExtraItem\n\n\n\n\n\n(Appears on:AddonInstal",
    "path": "docs/preview/user_docs/references/api-reference/add-on",
    "description": "   Packages:     extensions.kubeblocks.io/v1alpha1    extensions.kubeblocks.io/v1alpha1   Resource Types:   Addon    Addon      Addon is the Schema for the add-ons API.       Field Description         `apiVersion` string    `extensions.kubeblocks.io/v1alpha1`         `kind` string    `Addon`        "
  },
  {
    "id": "docs_en_preview_user_docs_references_api-reference_cluster",
    "title": "Cluster API Reference",
    "content": "\n\n\nPackages:\n\n\n\n\napps.kubeblocks.io/v1\n\n\n\n\napps.kubeblocks.io/v1alpha1\n\n\n\n\napps.kubeblocks.io/v1beta1\n\n\n\n\nworkloads.kubeblocks.io/v1\n\n\n\n\nworkloads.kubeblocks.io/v1alpha1\n\n\n\napps.kubeblocks.io/v1\n\n\nResource Types:\n\n\nCluster\n\n\n\nClusterDefinition\n\n\n\nComponent\n\n\n\nComponentDefinition\n\n\n\nComponentVersion\n\n\n\nServiceDescriptor\n\n\n\nShardingDefinition\n\n\n\nSidecarDefinition\n\n\n\nCluster\n\n\n\n\n\nCluster offers a unified management interface for a wide variety of database and storage systems:\n\n\n\nRelational databases: MySQL, PostgreSQL, MariaDB\n\n\nNoSQL databases: Redis, MongoDB\n\n\nKV stores: ZooKeeper, etcd\n\n\nAnalytics systems: ElasticSearch, OpenSearch, ClickHouse, Doris, StarRocks, Solr\n\n\nMessage queues: Kafka, Pulsar\n\n\nDistributed SQL: TiDB, OceanBase\n\n\nVector databases: Qdrant, Milvus, Weaviate\n\n\nObject storage: Minio\n\n\n\n\nKubeBlocks utilizes an abstraction layer to encapsulate the characteristics of these diverse systems.\nA Cluster is composed of multiple Components, each defined by vendors or KubeBlocks Addon developers via ComponentDefinition,\narranged in Directed Acyclic Graph (DAG) topologies.\nThe topologies, defined in a ClusterDefinition, coordinate reconciliation across Cluster&rsquo;s lifecycle phases:\nCreating, Running, Updating, Stopping, Stopped, Deleting.\nLifecycle management ensures that each Component operates in harmony, executing appropriate actions at each lifecycle stage.\n\n\n\nFor sharded-nothing architecture, the Cluster supports managing multiple shards,\neach shard managed by a separate Component, supporting dynamic resharding.\n\n\n\nThe Cluster object is aimed to maintain the overall integrity and availability of a database cluster,\nserves as the central control point, abstracting the complexity of multiple-component management,\nand providing a unified interface for cluster-wide operations.\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`apps.kubeblocks.io/v1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`Cluster`\n\n\n\n\n\n\n\n`metadata`\n\n\nKubernetes meta/v1.ObjectMeta\n\n\n\n\n\n\nRefer to t",
    "path": "docs/preview/user_docs/references/api-reference/cluster",
    "description": "   Packages:     apps.kubeblocks.io/v1     apps.kubeblocks.io/v1alpha1     apps.kubeblocks.io/v1beta1     workloads.kubeblocks.io/v1     workloads.kubeblocks.io/v1alpha1    apps.kubeblocks.io/v1   Resource Types:   Cluster    ClusterDefinition    Component    ComponentDefinition    ComponentVersion "
  },
  {
    "id": "docs_en_preview_user_docs_references_api-reference_dataprotection",
    "title": "Dataprotection API Reference",
    "content": "\n\n\nPackages:\n\n\n\n\ndataprotection.kubeblocks.io/v1alpha1\n\n\n\ndataprotection.kubeblocks.io/v1alpha1\n\n\nResource Types:\n\n\nActionSet\n\n\n\nBackup\n\n\n\nBackupPolicy\n\n\n\nBackupRepo\n\n\n\nBackupSchedule\n\n\n\nRestore\n\n\n\nStorageProvider\n\n\n\nActionSet\n\n\n\n\n\nActionSet is the Schema for the actionsets API\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`dataprotection.kubeblocks.io/v1alpha1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`ActionSet`\n\n\n\n\n\n\n\n`metadata`\n\n\nKubernetes meta/v1.ObjectMeta\n\n\n\n\n\n\nRefer to the Kubernetes API documentation for the fields of the\n`metadata` field.\n\n\n\n\n\n\n\n\n`spec`\n\n\nActionSetSpec\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`backupType`\n\n\nBackupType\n\n\n\n\n\n\n\n\nSpecifies the backup type. Supported values include:\n\n\n\n`Full` for a full backup.\n\n\n`Incremental` back up data that have changed since the last backup (either full or incremental).\n\n\n`Differential` back up data that has changed since the last full backup.\n\n\n`Continuous` back up transaction logs continuously, such as MySQL binlog, PostgreSQL WAL, etc.\n\n\n`Selective` back up data more precisely, use custom parameters, such as specific databases or tables.\n\n\n\n\nContinuous backup is essential for implementing Point-in-Time Recovery (PITR).\n\n\n\n\n\n\n\n\n\n`parametersSchema`\n\n\nActionSetParametersSchema\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies the schema of parameters in backups and restores before their usage.\n\n\n\n\n\n\n\n\n\n`env`\n\n\n[]Kubernetes core/v1.EnvVar\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies a list of environment variables to be set in the container.\n\n\n\n\n\n\n\n\n\n`envFrom`\n\n\n[]Kubernetes core/v1.EnvFromSource\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies a list of sources to populate environment variables in the container.\nThe keys within a source must be a C_IDENTIFIER. Any invalid keys will be\nreported as an event when the container starts. If a key exists in multiple\nsources, the value from the last source will take precedence. Any values\ndefined by an Env with a duplicate key will take precedence.\n\n\n\nThis field cannot be updated.\n\n\n\n\n\n\n\n\n\n`backup`\n\n\nBackupActionSpec\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies the ba",
    "path": "docs/preview/user_docs/references/api-reference/dataprotection",
    "description": "   Packages:     dataprotection.kubeblocks.io/v1alpha1    dataprotection.kubeblocks.io/v1alpha1   Resource Types:   ActionSet    Backup    BackupPolicy    BackupRepo    BackupSchedule    Restore    StorageProvider    ActionSet      ActionSet is the Schema for the actionsets API       Field Descripti"
  },
  {
    "id": "docs_en_preview_user_docs_references_api-reference_operations",
    "title": "Operations API Reference",
    "content": "\n\n\nPackages:\n\n\n\n\noperations.kubeblocks.io/v1alpha1\n\n\n\noperations.kubeblocks.io/v1alpha1\nResource Types:\n\n\nOpsDefinition\n\n\n\nOpsRequest\n\n\n\nOpsDefinition\n\n\n\n\n\nOpsDefinition is the Schema for the OpsDefinitions API.\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`operations.kubeblocks.io/v1alpha1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`OpsDefinition`\n\n\n\n\n\n\n\n`metadata`\n\n\nKubernetes meta/v1.ObjectMeta\n\n\n\n\n\n\nRefer to the Kubernetes API documentation for the fields of the\n`metadata` field.\n\n\n\n\n\n\n\n\n`spec`\n\n\nOpsDefinitionSpec\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`preConditions`\n\n\n[]PreCondition\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies the preconditions that must be met to run the actions for the operation.\nif set, it will check the condition before the Component runs this operation.\nExample:\n\n\n\n preConditions:\n - rule:\n     expression: '&#123;&#123; eq .component.status.phase &quot;Running&quot; &#125;&#125;'\n     message: Component is not in Running status.\n\n\n\n\n\n\n\n\n\n\n`podInfoExtractors`\n\n\n[]PodInfoExtractor\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies a list of PodInfoExtractor, each designed to select a specific Pod and extract selected runtime info\nfrom its PodSpec.\nThe extracted information, such as environment variables, volumes and tolerations, are then injected into\nJobs or Pods that execute the OpsActions defined in `actions`.\n\n\n\n\n\n\n\n\n\n`componentInfos`\n\n\n[]ComponentInfo\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies a list of ComponentDefinition for Components associated with this OpsDefinition.\nIt also includes connection credentials (address and account) for each Component.\n\n\n\n\n\n\n\n\n\n`parametersSchema`\n\n\nParametersSchema\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies the schema for validating the data types and value ranges of parameters in OpsActions before their usage.\n\n\n\n\n\n\n\n\n\n`actions`\n\n\n[]OpsAction\n\n\n\n\n\n\n\n\nSpecifies a list of OpsAction where each customized action is executed sequentially.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`status`\n\n\nOpsDefinitionStatus\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpsRequest\n\n\n\n\n\nOpsRequest is the Schema for the opsrequests API\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersi",
    "path": "docs/preview/user_docs/references/api-reference/operations",
    "description": "   Packages:     operations.kubeblocks.io/v1alpha1    operations.kubeblocks.io/v1alpha1 Resource Types:   OpsDefinition    OpsRequest    OpsDefinition      OpsDefinition is the Schema for the OpsDefinitions API.       Field Description         `apiVersion` string    `operations.kubeblocks.io/v1alpha"
  },
  {
    "id": "docs_en_preview_user_docs_references_api-reference_parameters",
    "title": "Parameters API Reference",
    "content": "\n\n\nPackages:\n\n\n\n\nparameters.kubeblocks.io/v1alpha1\n\n\n\nparameters.kubeblocks.io/v1alpha1\nResource Types:\n\n\nComponentParameter\n\n\n\nParamConfigRenderer\n\n\n\nParameter\n\n\n\nParametersDefinition\n\n\n\nComponentParameter\n\n\n\n\n\nComponentParameter is the Schema for the componentparameters API\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`parameters.kubeblocks.io/v1alpha1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`ComponentParameter`\n\n\n\n\n\n\n\n`metadata`\n\n\nKubernetes meta/v1.ObjectMeta\n\n\n\n\n\n\nRefer to the Kubernetes API documentation for the fields of the\n`metadata` field.\n\n\n\n\n\n\n\n\n`spec`\n\n\nComponentParameterSpec\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`clusterName`\n\nstring\n\n\n\n\n\n(Optional)\n\n\nSpecifies the name of the Cluster that this configuration is associated with.\n\n\n\n\n\n\n\n\n\n`componentName`\n\nstring\n\n\n\n\n\n\n\nRepresents the name of the Component that this configuration pertains to.\n\n\n\n\n\n\n\n\n\n`configItemDetails`\n\n\n[]ConfigTemplateItemDetail\n\n\n\n\n\n\n(Optional)\n\n\nConfigItemDetails is an array of ConfigTemplateItemDetail objects.\n\n\n\nEach ConfigTemplateItemDetail corresponds to a configuration template,\nwhich is a ConfigMap that contains multiple configuration files.\nEach configuration file is stored as a key-value pair within the ConfigMap.\n\n\n\nThe ConfigTemplateItemDetail includes information such as:\n\n\n\nThe configuration template (a ConfigMap)\n\n\nThe corresponding ConfigConstraint (constraints and validation rules for the configuration)\n\n\nVolume mounts (for mounting the configuration files)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`status`\n\n\nComponentParameterStatus\n\n\n\n\n\n\n\n\n\n\n\n\n\nParamConfigRenderer\n\n\n\n\n\nParamConfigRenderer is the Schema for the paramconfigrenderers API\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`parameters.kubeblocks.io/v1alpha1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`ParamConfigRenderer`\n\n\n\n\n\n\n\n`metadata`\n\n\nKubernetes meta/v1.ObjectMeta\n\n\n\n\n\n\nRefer to the Kubernetes API documentation for the fields of the\n`metadata` field.\n\n\n\n\n\n\n\n\n`spec`\n\n\nParamConfigRendererSpec\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`componentDef`\n\nstring\n\n\n\n\n\n\n\nSpecifies the ComponentDefinit",
    "path": "docs/preview/user_docs/references/api-reference/parameters",
    "description": "   Packages:     parameters.kubeblocks.io/v1alpha1    parameters.kubeblocks.io/v1alpha1 Resource Types:   ComponentParameter    ParamConfigRenderer    Parameter    ParametersDefinition    ComponentParameter      ComponentParameter is the Schema for the componentparameters API       Field Description"
  },
  {
    "id": "docs_en_release-0_9_user_docs_developer_api-reference_add-on",
    "title": "Add-On API Reference",
    "content": "\n\nPackages:\n\n\n\n\nextensions.kubeblocks.io/v1alpha1\n\n\n\nextensions.kubeblocks.io/v1alpha1\n\n\nResource Types:\n\n\nAddon\n\n\n\nAddon\n\n\n\n\nAddon is the Schema for the add-ons API.\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`extensions.kubeblocks.io/v1alpha1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`Addon`\n\n\n\n\n\n\n\n`metadata`\n\n\nKubernetes meta/v1.ObjectMeta\n\n\n\n\n\n\nRefer to the Kubernetes API documentation for the fields of the\n`metadata` field.\n\n\n\n\n\n\n\n\n`spec`\n\n\nAddonSpec\n\n\n\n\n\n\n\n\n\n\n\n\n\n`description`\n\nstring\n\n\n\n\n\n(Optional)\n\nSpecifies the description of the add-on.\n\n\n\n\n\n\n\n\n\n`type`\n\n\nAddonType\n\n\n\n\n\n\n\nDefines the type of the add-on. The only valid value is &lsquo;helm&rsquo;.\n\n\n\n\n\n\n\n\n\n`version`\n\nstring\n\n\n\n\n\n(Optional)\n\nIndicates the version of the add-on.\n\n\n\n\n\n\n\n\n\n`provider`\n\nstring\n\n\n\n\n\n(Optional)\n\nSpecifies the provider of the add-on.\n\n\n\n\n\n\n\n\n\n`helm`\n\n\nHelmTypeInstallSpec\n\n\n\n\n\n\n(Optional)\n\nRepresents the Helm installation specifications. This is only processed\nwhen the type is set to &lsquo;helm&rsquo;.\n\n\n\n\n\n\n\n\n\n`defaultInstallValues`\n\n\n[]AddonDefaultInstallSpecItem\n\n\n\n\n\n\n\nSpecifies the default installation parameters.\n\n\n\n\n\n\n\n\n\n`install`\n\n\nAddonInstallSpec\n\n\n\n\n\n\n(Optional)\n\nDefines the installation parameters.\n\n\n\n\n\n\n\n\n\n`installable`\n\n\nInstallableSpec\n\n\n\n\n\n\n(Optional)\n\nRepresents the installable specifications of the add-on. This includes\nthe selector and auto-install settings.\n\n\n\n\n\n\n\n\n\n`cliPlugins`\n\n\n[]CliPlugin\n\n\n\n\n\n\n(Optional)\n\nSpecifies the CLI plugin installation specifications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`status`\n\n\nAddonStatus\n\n\n\n\n\n\n\n\n\n\n\n\n\nAddonDefaultInstallSpecItem\n\n\n\n\n(Appears on:AddonSpec)\n\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`AddonInstallSpec`\n\n\nAddonInstallSpec\n\n\n\n\n\n\n\n\n(Members of `AddonInstallSpec` are embedded into this type.)\n\n\n\n\n\n\n\n\n\n\n`selectors`\n\n\n[]SelectorRequirement\n\n\n\n\n\n\n(Optional)\n\nIndicates the default selectors for add-on installations. If multiple selectors are provided,\nall selectors must evaluate to true.\n\n\n\n\n\n\n\n\nAddonInstallExtraItem\n\n\n\n\n(Appears on:AddonInstallSpec)\n\n\n\n\n\n\n\n\nFi",
    "path": "docs/release-0_9/user_docs/developer/api-reference/add-on",
    "description": "  Packages:     extensions.kubeblocks.io/v1alpha1    extensions.kubeblocks.io/v1alpha1   Resource Types:   Addon    Addon     Addon is the Schema for the add-ons API.       Field Description         `apiVersion` string    `extensions.kubeblocks.io/v1alpha1`         `kind` string    `Addon`        `m"
  },
  {
    "id": "docs_en_release-0_9_user_docs_developer_api-reference_backup",
    "title": "Backup API Reference",
    "content": "\n\nPackages:\n\n\n\n\ndataprotection.kubeblocks.io/v1alpha1\n\n\n\ndataprotection.kubeblocks.io/v1alpha1\n\n\nResource Types:\n\n\nActionSet\n\n\n\nBackup\n\n\n\nBackupPolicy\n\n\n\nBackupRepo\n\n\n\nBackupSchedule\n\n\n\nRestore\n\n\n\nStorageProvider\n\n\n\nActionSet\n\n\n\n\nActionSet is the Schema for the actionsets API\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`dataprotection.kubeblocks.io/v1alpha1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`ActionSet`\n\n\n\n\n\n\n\n`metadata`\n\n\nKubernetes meta/v1.ObjectMeta\n\n\n\n\n\n\nRefer to the Kubernetes API documentation for the fields of the\n`metadata` field.\n\n\n\n\n\n\n\n\n`spec`\n\n\nActionSetSpec\n\n\n\n\n\n\n\n\n\n\n\n\n\n`backupType`\n\n\nBackupType\n\n\n\n\n\n\n\nSpecifies the backup type. Supported values include:\n\n\n\n`Full` for a full backup.\n\n\n`Incremental` back up data that have changed since the last backup (either full or incremental).\n\n\n`Differential` back up data that has changed since the last full backup.\n\n\n`Continuous` back up transaction logs continuously, such as MySQL binlog, PostgreSQL WAL, etc.\n\n\n\nContinuous backup is essential for implementing Point-in-Time Recovery (PITR).\n\n\n\n\n\n\n\n\n\n`env`\n\n\n[]Kubernetes core/v1.EnvVar\n\n\n\n\n\n\n(Optional)\n\nSpecifies a list of environment variables to be set in the container.\n\n\n\n\n\n\n\n\n\n`envFrom`\n\n\n[]Kubernetes core/v1.EnvFromSource\n\n\n\n\n\n\n(Optional)\n\nSpecifies a list of sources to populate environment variables in the container.\nThe keys within a source must be a C_IDENTIFIER. Any invalid keys will be\nreported as an event when the container starts. If a key exists in multiple\nsources, the value from the last source will take precedence. Any values\ndefined by an Env with a duplicate key will take precedence.\n\n\nThis field cannot be updated.\n\n\n\n\n\n\n\n\n\n`backup`\n\n\nBackupActionSpec\n\n\n\n\n\n\n(Optional)\n\nSpecifies the backup action.\n\n\n\n\n\n\n\n\n\n`restore`\n\n\nRestoreActionSpec\n\n\n\n\n\n\n(Optional)\n\nSpecifies the restore action.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`status`\n\n\nActionSetStatus\n\n\n\n\n\n\n\n\n\n\n\n\n\nBackup\n\n\n\n\nBackup is the Schema for the backups API.\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`data",
    "path": "docs/release-0_9/user_docs/developer/api-reference/backup",
    "description": "  Packages:     dataprotection.kubeblocks.io/v1alpha1    dataprotection.kubeblocks.io/v1alpha1   Resource Types:   ActionSet    Backup    BackupPolicy    BackupRepo    BackupSchedule    Restore    StorageProvider    ActionSet     ActionSet is the Schema for the actionsets API       Field Description"
  },
  {
    "id": "docs_en_release-0_9_user_docs_developer_api-reference_cluster",
    "title": "Cluster API Reference",
    "content": "\n\nPackages:\n\n\n\n\napps.kubeblocks.io/v1alpha1\n\n\n\n\napps.kubeblocks.io/v1beta1\n\n\n\n\nworkloads.kubeblocks.io/v1alpha1\n\n\n\napps.kubeblocks.io/v1alpha1\n\n\nResource Types:\n\n\nCluster\n\n\n\nClusterDefinition\n\n\n\nClusterVersion\n\n\n\nComponent\n\n\n\nComponentClassDefinition\n\n\n\nComponentDefinition\n\n\n\nComponentResourceConstraint\n\n\n\nComponentVersion\n\n\n\nConfigConstraint\n\n\n\nConfiguration\n\n\n\nOpsDefinition\n\n\n\nOpsRequest\n\n\n\nServiceDescriptor\n\n\n\nCluster\n\n\n\n\nCluster offers a unified management interface for a wide variety of database and storage systems:\n\n\n\nRelational databases: MySQL, PostgreSQL, MariaDB\n\n\nNoSQL databases: Redis, MongoDB\n\n\nKV stores: ZooKeeper, etcd\n\n\nAnalytics systems: ElasticSearch, OpenSearch, ClickHouse, Doris, StarRocks, Solr\n\n\nMessage queues: Kafka, Pulsar\n\n\nDistributed SQL: TiDB, OceanBase\n\n\nVector databases: Qdrant, Milvus, Weaviate\n\n\nObject storage: Minio\n\n\n\nKubeBlocks utilizes an abstraction layer to encapsulate the characteristics of these diverse systems.\nA Cluster is composed of multiple Components, each defined by vendors or KubeBlocks Addon developers via ComponentDefinition,\narranged in Directed Acyclic Graph (DAG) topologies.\nThe topologies, defined in a ClusterDefinition, coordinate reconciliation across Cluster&rsquo;s lifecycle phases:\nCreating, Running, Updating, Stopping, Stopped, Deleting.\nLifecycle management ensures that each Component operates in harmony, executing appropriate actions at each lifecycle stage.\n\n\nFor sharded-nothing architecture, the Cluster supports managing multiple shards,\neach shard managed by a separate Component, supporting dynamic resharding.\n\n\nThe Cluster object is aimed to maintain the overall integrity and availability of a database cluster,\nserves as the central control point, abstracting the complexity of multiple-component management,\nand providing a unified interface for cluster-wide operations.\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`apps.kubeblocks.io/v1alpha1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`Cluster`\n\n\n\n\n\n\n\n`metada",
    "path": "docs/release-0_9/user_docs/developer/api-reference/cluster",
    "description": "  Packages:     apps.kubeblocks.io/v1alpha1     apps.kubeblocks.io/v1beta1     workloads.kubeblocks.io/v1alpha1    apps.kubeblocks.io/v1alpha1   Resource Types:   Cluster    ClusterDefinition    ClusterVersion    Component    ComponentClassDefinition    ComponentDefinition    ComponentResourceConstr"
  },
  {
    "id": "docs_en_release-0_9_user_docs_developer_external-component_reference-external-component",
    "title": "Reference external component",
    "content": "\n# Reference an external component\n\n:::note\n\nThe external component function is an alpha version and there might be major revisions and changes in the future.\n\n:::\n\n## What is referencing an external component?\n\nSome database clusters rely on metadata storage for distributed coordination and dynamic configuration. However, as the number of database clusters increases, the metadata storage itself can consume a significant amount of resources. Examples of such components include ZooKeeper in Pulsar. To reduce overhead, you can reference the same external component in multiple database clusters now.\n\nReferencing an external component in KubeBlocks means referencing an external or KubeBlocks-based component in a declarative manner in a KubeBlocks cluster.\n\nAs its definition indicates, referencing the external component can be divided into two types:\n\n* Referencing an external component\n\n  This external component can be Kubernetes-based or non-Kubernetes. When referencing this component, first create a ServiceDescriptor CR (custom resources) which defines both the service and resources for referencing.\n\n* Referencing a KubeBlocks-based component\n\n  This type of component is based on KubeBlocks clusters. When referencing this component, just fill in the referenced Cluster and no ServiceDescriptor is required.\n\n## Examples of referencing an external component\n\nThe following examples show how a Pulsar cluster created by the KubeBlocks add-on references ZooKeeper as an external component. The instructions below include two parts:\n\n1. [Create an external component reference declaration](#create-an-external-component-reference-declaration) when installing KubeBlocks or enabling an add-on.\n2. [Define the mapping relation of the external component](#define-the-mapping-relation-of-the-external-component) when creating a cluster.\n\nA KubeBlocks Pulsar cluster is composed of components including proxy, broker, bookies, and ZooKeeper and broker and bookies rely on ZooKeeper to provid",
    "path": "docs/release-0_9/user_docs/developer/external-component/reference-external-component",
    "description": " # Reference an external component  :::note  The external component function is an alpha version and there might be major revisions and changes in the future.  :::  ## What is referencing an external component?  Some database clusters rely on metadata storage for distributed coordination and dynamic"
  },
  {
    "id": "docs_en_release-0_9_user_docs_developer_integration_add-ons-of-kubeblocks",
    "title": "Overview",
    "content": "\n# Add-ons of KubeBlocks\n\nKubeBlocks is a control and management platform to manage a bunch of database engines and other add-ons.\n\nThis series provides basic knowledge of add-ons, so you can get a quick start and become a member of the KubeBlocks community.\n\nKubeBlocks features a rich add-on ecosystem with major databases, streaming and vector databases, including:\n\n- Relational Database: ApeCloud-MySQL (MySQL RaftGroup cluster), PostgreSQL (Replication cluster) \n- NoSQL Database: MongoDB, Redis\n- Graph Database: Nebula (from community contributors)\n- Time Series Database: TDengine, Greptime (from community contributors)\n- Vector Database: Milvus, Qdrant, Weaviate, etc.\n- Streaming: Kafka, Pulsar, ElasticSearch\n\nAdding an add-on to KubeBlocks is easy, you can just follow this guide to add the add-on to KubeBlocks as long as you know the followings: \n1. How to write a YAML file (e.g., You should know how many spaces to add when indenting with YAML).\n2. Knowledge about Helm (e.g. What is Helm and Helm chart).\n3. Have tried K8s (e.g., You should know what a pod is, or have installed an operator on K8s with Helm).\n4. Grasp basic concepts of KubeBlocks, such as ClusterDefinition, ClusterVersion and Cluster.\n\nIf you have any question, you can join our [slack channel](https://join.slack.com/t/kubeblocks/shared_invite/zt-22cx2f84x-BPZvnLRqBOGdZ_XSjELh4Q) to ask.",
    "path": "docs/release-0_9/user_docs/developer/integration/add-ons-of-kubeblocks",
    "description": " # Add-ons of KubeBlocks  KubeBlocks is a control and management platform to manage a bunch of database engines and other add-ons.  This series provides basic knowledge of add-ons, so you can get a quick start and become a member of the KubeBlocks community.  KubeBlocks features a rich add-on ecosys"
  },
  {
    "id": "docs_en_release-0_9_user_docs_developer_integration_backup-and-restore",
    "title": "Backup and restore",
    "content": "\n\n# Backup and restore\n\nThis tutorial takes Oracle MySQL as an example and introduces how to create backups and restore data in KubeBlocks. The full PR can be found at [Learn KubeBlocks Add-on](https://github.com/apecloud/learn-kubeblocks-addon/tree/main/tutorial-2-backup-restore/).\n\nDifferent classification results in different types of backups, including volume snapshot backup and file backup, data backup and log backup, full backup and incremental backup, as well as scheduled backup and on-demand backup, which differ in terms of their methods, contents, volumes, and timing.\n\nThis tutorial illustrates how to realize the most frequently used snapshot backup and file backup in KubeBlocks.\n\n- Snapshot backup relies on the volume snapshot capability of Kubernetes.\n- File backup relies on backup tools provided by database engines.\n\nNow take a quick look at the basic concepts of KubeBlocks in the table below, which also are elaborated in the following tutorial.\n\n:paperclip: Table 1. Terminology\n\n| Term | Description | Scope |\n| :--- | :---------- | :---- |\n| Backup | Backup object  It defines the entity to be backed up. | Namespace |\n| BackupPolicy | Backup policy  It defines the policy for each backup type, such as scheduling, retention time, and tools. | Namespace |\n| BackupTool | Backup tool  It is the carrier of backup tools in KubeBlocks and should realize the backup and restoration logic of corresponding tools. | Cluster |\n| BackupPolicyTemplate | Template of backup policy  It is the bridge between the backup and ClusterDefinition. When creating a cluster, KubeBlocks automatically generates a default backup policy for each cluster according to BackupPolicyTemplate. | Cluster |\n\n## Before you start\n\n- Finish the configuration in [Add an add-on to KubeBlocks](./how-to-add-an-add-on.md).\n- Grasp the basics of K8s concepts, such as Pod, PVC, PV, VolumeSnapshot, etc.\n\n## Step 1. Prepare environment\n\n1. Install CSI Driver.\n\n   Since volume snapshot is only available for",
    "path": "docs/release-0_9/user_docs/developer/integration/backup-and-restore",
    "description": "  # Backup and restore  This tutorial takes Oracle MySQL as an example and introduces how to create backups and restore data in KubeBlocks. The full PR can be found at [Learn KubeBlocks Add-on](https://github.com/apecloud/learn-kubeblocks-addon/tree/main/tutorial-2-backup-restore/).  Different class"
  },
  {
    "id": "docs_en_release-0_9_user_docs_developer_integration_environment-variables-and-placeholders",
    "title": "Environment variables and placeholders",
    "content": "\n# Environment variables and placeholders\n\n## Environment variables\n\n### Pod container environment variables\n\nThe following variables are injected by KubeBlocks into each pod.\n\n| Name | Description |\n| :--- | :---------- |\n| `KB_POD_NAME` | K8s Pod Name |\n| `KB_NAMESPACE` | K8s Pod Namespace |\n| `KB_SA_NAME` | KubeBlocks Service Account Name |\n| `KB_NODENAME` | K8s Node Name |\n| `KB_HOSTIP` | K8s Host IP address |\n| `KB_PODIP` | K8s Pod IP address |\n| `KB_PODIPS` | K8s Pod IP addresses |\n| `KB_POD_UID` | POD UID (`pod.metadata.uid`) |\n| `KB_CLUSTER_NAME` | KubeBlocks Cluster API object name |\n| `KB_COMP_NAME` | Running pod's KubeBlocks Cluster API object's `.spec.components.name` |\n| `KB_CLUSTER_COMP_NAME` | Running pod's KubeBlocks Cluster API object's `-` |\n| `KB_REPLICA_COUNT` | Running pod's component's replica |\n| `KB_CLUSTER_UID` | Running pods' KubeBlocks Cluster API object's `metadata.uid` |\n| `KB_CLUSTER_UID_POSTFIX_8` | Last eight digits of KB_CLUSTER_UID |\n| `KB__HOSTNAME` | Running pod's hostname, where `` is the ordinal of pod.  N/A if workloadType=Stateless. |\n| `KB_POD_FQDN` | Running pod's fully qualified domain name (FQDN).  N/A if workloadType=Stateless. |\n\n## Built-in Place-holders\n\n### ComponentValueFrom API\n\n| Name | Description |\n| :--- | :---------- |\n| `POD_ORDINAL` | Pod ordinal |\n| `POD_FQDN` | Pod FQDN (fully qualified domain name) |\n| `POD_NAME` | Pod Name |\n\n### ConnectionCredential API\n\n| Name | Description |\n| :--- | :---------- |\n| `UUID` | Generate a random UUID v4 string. |\n| `UUID_B64` | Generate a random UUID v4 BASE64 encoded string. |\n| `UUID_STR_B64` | Generate a random UUID v4 string then BASE64 encoded. |\n| `UUID_HEX` | Generate a random UUID v4 HEX representation. |\n| `HEADLESS_SVC_FQDN` | Headless service FQDN placeholder, value pattern - `$(CLUSTER_NAME)-$(1ST_COMP_NAME)-headless.$(NAMESPACE).svc`, where `1ST_COMP_NAME` is the 1st component that provide `ClusterDefinition.spec.componentDefs[].service` attribute. |\n| `SVC_F",
    "path": "docs/release-0_9/user_docs/developer/integration/environment-variables-and-placeholders",
    "description": " # Environment variables and placeholders  ## Environment variables  ### Pod container environment variables  The following variables are injected by KubeBlocks into each pod.  | Name | Description | | :--- | :---------- | | `KB_POD_NAME` | K8s Pod Name | | `KB_NAMESPACE` | K8s Pod Namespace | | `KB"
  },
  {
    "id": "docs_en_release-0_9_user_docs_developer_integration_how-to-add-an-add-on",
    "title": "Add an add-on",
    "content": "\n# Add an add-on to KubeBlocks\n\nThis tutorial explains how to integrate an add-on to KubeBlocks, and takes Oracle MySQL as an example. You can also find the [PR here](https://github.com/apecloud/learn-kubeblocks-addon).\n\nThere are altogether 3 steps to integrate an add-on:\n\n1. Design cluster blueprint.\n2. Prepare cluster templates.\n3. Add an `addon.yaml` file.\n\n## Step 1. Design a blueprint for cluster\n\nBefore getting started, make sure to design your cluster blueprint. Think about what you want your cluster to look like. For example:\n\n- What components it has\n- What format each component takes\n  - stateful/stateless\n  - Standalone/Replication/RaftGroup\n\nIn this tutorial you will learn how to deploy a cluster with one Stateful component which has only one node. The design configuration of the cluster is shown in the following table.\n\nCluster Format: Deploying a MySQL 8.0 Standalone.\n\n:paperclip: Table 1. Blueprint for Oracle MySQL Cluster\n\n| Term              | Settings                                                                                                     |\n|-------------------|--------------------------------------------------------------------------------------------------------------|\n| ClusterDefinition | Startup Scripts: Default  Configuration Files: Default Service Port: 3306 Number of Components: 1, i.e. MySQL |\n| ClusterVersion    | Image: docker.io/mysql:8.0.34                                                                                |\n| Cluster.yaml      | Specified by the user during creation                                                                        |\n\n## Step 2. Prepare cluster templates\n\n### 2.1 Create a Helm chart\n\nOpt 1.`helm create oracle-mysql`\n\nOpt 2. Directly create `mkdir oracle-mysql`\n\nIt should contain the following information:\n\n```bash\n> tree oracle-mysql\n.\n├── Chart.yaml        #  A YAML file containing information about the chart\n├── templates         # A directory of templates that, when combined with values,",
    "path": "docs/release-0_9/user_docs/developer/integration/how-to-add-an-add-on",
    "description": " # Add an add-on to KubeBlocks  This tutorial explains how to integrate an add-on to KubeBlocks, and takes Oracle MySQL as an example. You can also find the [PR here](https://github.com/apecloud/learn-kubeblocks-addon).  There are altogether 3 steps to integrate an add-on:  1. Design cluster bluepri"
  },
  {
    "id": "docs_en_release-0_9_user_docs_developer_integration_monitoring",
    "title": "Monitoring",
    "content": "\n# Configure monitoring\n\nThis tutorial takes Oracle MySQL as an example and explains how to configure monitoring in KubeBlocks. You can refer to [the full PR](https://github.com/apecloud/learn-kubeblocks-addon/tree/main/tutorial-4-monitor-cluster/oracle-mysql).\n\n## Before you start\n\n1. Knowledge about basic Kubernetes concepts, such as Pod and Sidecar.\n2. Finish [Tutorial 1](./how-to-add-an-add-on.md).\n3. Knowledge about basic monitoring system concepts, such as Prometheus and Grafana.\n\n## Introduction\n\nMonitoring is an essential part of Kubernetes observability. It helps developers check the system's operational status to quickly identify issues.\n\nKubeblocks currently integrates Prometheus and Grafana as add-ons. In this tutorial, you will learn how to integrate the Prometheus/Grafana solution.\n\n### Prometheus Overview\n\nPrometheus provides an open-source monitoring solution that integrates metric collection, metric storage, and alert capabilities.\n\nIt is widely used in cloud-native, containerized, and microservices architectures. With Prometheus, developers and operations teams can monitor the performance and health status of applications in real-time, so as to quickly identify and resolve issues to ensure application reliability and availability. Prometheus is usually used with Grafana to create powerful monitoring and observability solutions.\n\n### Grafana Overview\n\nGrafana is an open-source analytics and monitoring platform widely used for visualizing time series data. It allows users to create interactive and customizable dashboards to monitor and analyze data from various sources.\n\n:paperclip: Table 1. Terminology\n\n| Term | Description |\n| :--  | :---------- |\n| Prometheus Exporter | Prometheus Exporter is a component that collects monitoring data and provides data to external entities using the Prometheus monitoring specification.  For more details, refer to [Prometheus  Exporter List](https://prometheus.io/docs/instrumenting/exporters/). |\n| Prometheus Metric",
    "path": "docs/release-0_9/user_docs/developer/integration/monitoring",
    "description": " # Configure monitoring  This tutorial takes Oracle MySQL as an example and explains how to configure monitoring in KubeBlocks. You can refer to [the full PR](https://github.com/apecloud/learn-kubeblocks-addon/tree/main/tutorial-4-monitor-cluster/oracle-mysql).  ## Before you start  1. Knowledge abo"
  },
  {
    "id": "docs_en_release-0_9_user_docs_developer_integration_multi-component",
    "title": "Multi-component configuration",
    "content": "\n# Multi-component configuration\n\nSo far, you've learned the definition, backup, and configuration of single-component clusters (e.g., Oracle-MySQL).\n\nThis tutorial takes NebulaGraph as an example to demonstrate how to integrate a multi-component cluster and address several common issues in multi-component configurations. You can find more details in [this repository](https://github.com/apecloud/kubeblocks-addons/tree/main/addons/nebula).\n\n## Before you start\n\n- Finish [Tutorial 1](./how-to-add-an-add-on.md).\n- Knowledge about basic KubeBlocks concepts, such as ClusterDefinition, Cluster, ComponentRef, and Component.\n\n## NebulaGraph Architecture\n\nFirst, take a look at the overall architecture of NebulaGraph.\n\nNebulaGraph applies the separation of storage and computing architecture and consists of three services: the Graph Service, the Meta Service, and the Storage Service. The following figure shows the architecture of a typical NebulaGraph cluster.\n\n![NebulaGraph Architecture (source: https://github.com/vesoft-inc/nebula)](/img/docs/en/nebula-aichitecture.png)\n\n- Metad: It is a component based on the Raft protocol and is responsible for data management tasks such as Schema operations, cluster management, and user permission management.\n- Graphd: It is the compute component and is responsible for handling query requests, including query parsing, validation, and generating and executing query plans.\n- Storaged: It is the distributed storage component based on Multi Group Raft, responsible for storing data.\n\nIf the client is considered, the fourth component is:\n\n- Client: It is a stateless component used to connect to Graphd and send graph queries.\n\n## Configure cluster typology\n\nNow you've learned the four components of NebulaGraph, and how each component is started and configured.\n\nSimilar to a single-component cluster, you can quickly assemble the definition for a multi-component cluster.\n\n```yaml\napiVersion: apps.kubeblocks.io/v1alpha1\nkind: ClusterDefinition\nmeta",
    "path": "docs/release-0_9/user_docs/developer/integration/multi-component",
    "description": " # Multi-component configuration  So far, you've learned the definition, backup, and configuration of single-component clusters (e.g., Oracle-MySQL).  This tutorial takes NebulaGraph as an example to demonstrate how to integrate a multi-component cluster and address several common issues in multi-co"
  },
  {
    "id": "docs_en_release-0_9_user_docs_developer_integration_parameter-configuration",
    "title": "Parameter configuration",
    "content": "\n\n# Parameter configuration\n\nThis tutorial takes Oracle MySQL as an example and explains how to configure parameter templates and parameters in KubeBlocks. You can find [the full PR here](https://github.com/apecloud/learn-kubeblocks-addon/tree/main/tutorial-3-config-and-reconfig/).\n\n## Before you start\n\n1. Grasp basic concepts of Kubernetes, such as Pod and ConfigMap.\n2. Finish configurations in [Configure parameter template](./parameter-template.md).\n3. (Optional) Know something about Go Template.\n4. (Optional)Know something about CUE Lang.\n\n## Introduction\n\nKubeBlocks adds configurations by mounting the ConfigMap to the volume. With a Kubernetes-Native concept that `ConfigMap is the only source of truth`, it centralizes entry for parameter changes in the ConfigMap to prevent configuration drifting. Therefore, the order below illustrates how KubeBlocks performs parameter reconfiguration:\n\n1. Configure parameter values in the ConfigMap.\n2. Derive parameter configurations (add/delete/update) based on ConfigMap modifications.\n3. Apply the parameter configurations to the engine.\n\nDifferent parameters require different configuration methods:\n\n- Static parameters require a cluster restart (cold update).\n- Dynamic parameters require a parameter refresh (hot update).\n\nTable 1 lists four common hot update methods, including UNIX Signal, SQL, Auto, etc. Currently, engines in KubeBlocks can implement one or more of these methods. For example, to apply dynamic configuration in PostgreSQL, you can use:\n\n- UNIX Signal: Send a `SIGHUP` signal.\n- Tools: Call `pg_ctl` command.\n- SQL: Execute SQL statements to directly update parameters.\n\n:paperclip: Table 1. Summary of Parameter Hot Updates\n\n| Methods     | Descriptions | Applicability |\n| :---------- | :----------- | :------------ |\n| Unix Signal | For example, PostgreSQL.  If you need to reload the configuration file after parameter configuration, send a `SIGHUP` signal to PG. | Applicable to engines that support Unix Signal upda",
    "path": "docs/release-0_9/user_docs/developer/integration/parameter-configuration",
    "description": "  # Parameter configuration  This tutorial takes Oracle MySQL as an example and explains how to configure parameter templates and parameters in KubeBlocks. You can find [the full PR here](https://github.com/apecloud/learn-kubeblocks-addon/tree/main/tutorial-3-config-and-reconfig/).  ## Before you st"
  },
  {
    "id": "docs_en_release-0_9_user_docs_developer_integration_parameter-template",
    "title": "Parameter template",
    "content": "\n\n# Parameter template\n\nThis tutorial demonstrates how to configure parameter templates in KubeBlocks with Oracle MySQL as an example. You can find [the full PR here](https://github.com/apecloud/learn-kubeblocks-addon/tree/main/tutorial-3-config-and-reconfig/).\n\n## Before you start\n\n1. Grasp basic concepts of Kubernetes, such as Pod and ConfigMap.\n2. Finish [Tutorial 1](./how-to-add-an-add-on.md).\n3. Know something about Go Template (Optional).\n\n## Introduction\n\nWhen creating a cluster, developers typically configure parameters according to resource availability, performance needs, environment, etc. Cloud database providers like AWS and Alibaba Cloud have therefore offered various parameter templates (such as high-performance and asynchronous templates for RDS) to facilitate a quick startup for users.\n\nIn this tutorial, you will learn how to configure parameters in KubeBlocks, which includes adding parameter templates, configuring parameters, and configuring parameter validation.\n\nAlthough Kubernetes allows users to mount parameter files as ConfigMap on volumes of the Pod, it only manages ConfigMap updates and synchronizes them to the volume. Therefore, if the database engine (such as MySQL and Postgres) fails to support dynamic loading of configuration files, you can only log in to the database to perform update operations, which can easily lead to configuration drift.\n\nTo prevent that, KubeBlocks manages all parameters through ConfigMap with the principle that `ConfigMap is the only source-of-truth`. It means that all parameter configurations are first applied to ConfigMap, and then, depending on different ways the parameters take effect, applied to each Pod in the cluster. A comprehensive guide on how to configure parameters will be provided in the next tutorial.\n\n## ConfigTemplate\n\nKubeBlocks renders parameter templates with ***Go Template***. Apart from common functions, it also includes some frequently-used calculation functions such as `callBufferSizeByResour",
    "path": "docs/release-0_9/user_docs/developer/integration/parameter-template",
    "description": "  # Parameter template  This tutorial demonstrates how to configure parameter templates in KubeBlocks with Oracle MySQL as an example. You can find [the full PR here](https://github.com/apecloud/learn-kubeblocks-addon/tree/main/tutorial-3-config-and-reconfig/).  ## Before you start  1. Grasp basic c"
  },
  {
    "id": "docs_en_release-0_9_user_docs_installation_prepare-a-local-k8s-cluster_prepare-a-local-k8s-cluster",
    "title": "Create a local Kubernetes test cluster",
    "content": "\n\n# Create a local Kubernetes test cluster\n\nThis tutorial introduces how to create a local Kubernetes test cluster using Minikube, K3d, and Kind. These tools make it easy to try out KubeBlocks on your local host, offering a great solution for development, testing, and experimentation without the complexity of creating a full production-grade cluster.\n\n## Before you start\n\nMake sure you have the following tools installed on your local host:\n\n- Docker: All three tools rely on Docker to create containerized Kubernetes clusters.\n- kubectl: The Kubernetes command-line tool for interacting with clusters. Refer to the [kubectl installation guide](https://kubernetes.io/docs/tasks/tools/)\n\n\n\n## Create a Kubernetes cluster using Kind\n\nKind stands for Kubernetes IN Docker. It runs Kubernetes clusters within Docker containers, making it an ideal tool for local Kubernetes testing.\n\n1. Install Kind. For details, you can refer to [Kind Quickstart](https://kind.sigs.k8s.io/docs/user/quick-start/).\n\n\n\n   ```bash\n   brew install kind\n   ```\n\n   \n\n\n   ```bash\n   # For AMD64 / x86_64\n   [ $(uname -m) = x86_64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.24.0/kind-linux-amd64\n   # For ARM64\n   [ $(uname -m) = aarch64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.24.0/kind-linux-arm64\n   chmod +x ./kind\n   sudo cp ./kind /usr/local/bin/kind\n   rm -rf kind\n   ```\n\n   \n\n\n   You can use chocolatey to install Kind.\n\n   ```bash\n   choco install kind\n   ```\n\n   \n\n   \n\n2. Create a Kind cluster.\n\n   ```bash\n   kind create cluster --name mykindcluster\n   ```\n\n   This command creates a single-node Kubernetes cluster running in a Docker container.\n\n3. Check whether the cluster is started and running.\n\n   ```bash\n   kubectl get nodes\n   >\n   NAME                          STATUS   ROLES           AGE   VERSION\n   mykindcluster-control-plane   Ready    control-plane   25s   v1.31.0\n   ```\n\n   You can see a node named `mykindcluster-control-plane` from the output, which means the cluster i",
    "path": "docs/release-0_9/user_docs/installation/prepare-a-local-k8s-cluster/prepare-a-local-k8s-cluster",
    "description": "  # Create a local Kubernetes test cluster  This tutorial introduces how to create a local Kubernetes test cluster using Minikube, K3d, and Kind. These tools make it easy to try out KubeBlocks on your local host, offering a great solution for development, testing, and experimentation without the com"
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_backup-and-restore_introduction",
    "title": "Introduction",
    "content": "\n# Introduction\n\nKubeBlocks provides the backup and restore function to ensure the safety and reliability of your data. The backup and restore function of KubeBlocks relies on BackupRepo and before using the full backup and restore function, you need to [configure BackupRepo first](./backup/backup-repo.md).\n\nKubeBlocks adopts physical backup which takes the physical files in a database as the backup object. You can choose one backup option based on your demands to back up the cluster data on demand or by schedule.\n\n* [On-demand backup](./backup/on-demand-backup.md): Based on different backup options, on-demand backup can be further divided into backup tool and snapshot backup.\n  * Backup tool: You can use the backup tool of the corresponding data product, such as MySQL XtraBackup and PostgreSQL pg_basebackup. KubeBlocks supports configuring backup tools for different data products.\n  * Snapshot backup: If your data is stored in a cloud disk that supports snapshots, you can create a data backup by snapshots. Snapshot backup is usually faster than a backup tool, and thus is recommended.\n\n* [Scheduled backup](./backup/scheduled-backup.md): You can specify retention time, backup method, time, and other parameters to customize your backup schedule.\n\nAs for the restore function, KubeBlocks supports restoring data from the backup set.\n\n* Restore\n  * [Restore data from the backup set](./restore/restore-data-from-backup-set.md)\n\nFollow the steps below to back up and restore your cluster.\n\n1. [Configure BackupRepo](./backup/backup-repo.md).\n2. [Configure BackupPolicy](./backup/configure-backuppolicy.md).\n3. Backup your cluster [on demand](./backup/on-demand-backup.md) or [by schedule](./backup/scheduled-backup.md).\n4. Restore your data by [PITR](./restore/pitr.md) or from the [backup set](./restore/restore-data-from-backup-set.md).\n",
    "path": "docs/release-0_9/user_docs/maintenance/backup-and-restore/introduction",
    "description": " # Introduction  KubeBlocks provides the backup and restore function to ensure the safety and reliability of your data. The backup and restore function of KubeBlocks relies on BackupRepo and before using the full backup and restore function, you need to [configure BackupRepo first](./backup/backup-r"
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_cross-k8s-deployment_cross-k8s-deployment",
    "title": "Deploy a Cluster across Multiple Kubernetes Clusters by KubeBlocks",
    "content": "\n# Deploy a Cluster across Multiple Kubernetes Clusters by KubeBlocks\n\nKubeBlocks supports managing multiple Kubernetes clusters to provide new options for instance disaster recovery and K8s cluster management. KubeBlocks introduces the control plane and data plane to support cross-K8s management.\n\n* Control plane: An independent K8s cluster in which the KubeBlocks operator runs. Most of the objects defined by KubeBlocks, such as definition, cluster, backup, and ops, are stored in this cluster. Users interact with the API of this cluster to manage multiple cluster instances.\n* Data plane: A K8s cluster that is used to run the actual workloads. There can be one or more clusters in the data plane. These clusters host resources such as pods, persistent volume claims (PVC), services, service accounts (SA), config maps (CM), secrets, jobs, etc., related to the instances. But in KubeBlocks v0.9.0, the KubeBlocks operator does not run in the data plane.\n\nIn terms of actual physical deployment, the control plane can be deployed in a single availability zone (AZ) for simplicity and flexibility. It can also be deployed in multiple different AZs to provide higher availability guarantees. Alternatively, it can be deployed by reusing a data plane, which offers a lower-cost approach to running the control plane.\n\n## Prepare an environment\n\nCreate several K8s clusters and prepare the configuration information for deploying KubeBlocks. This tutorial takes three data plane K8s clusters as an example and their contexts are named as k8s-1, k8s-2, and k8s-3.\n\n* Create K8s clusters: one for the control plane and several for data plane. Make sure the API servers of these data plane K8s clusters can be reached from the control plane, which should include both network connectivity and access configuration.\n* Prepare the configuration information for the KubeBlocks operator to access the data plane K8s clusters. Store this information in the control plane cluster as a secret and this secrec",
    "path": "docs/release-0_9/user_docs/maintenance/cross-k8s-deployment/cross-k8s-deployment",
    "description": " # Deploy a Cluster across Multiple Kubernetes Clusters by KubeBlocks  KubeBlocks supports managing multiple Kubernetes clusters to provide new options for instance disaster recovery and K8s cluster management. KubeBlocks introduces the control plane and data plane to support cross-K8s management.  "
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_in-place-update_ignore-vertical-scale",
    "title": "Enable in-place update",
    "content": "\n# Enable in-place update\n\nIn Kubernetes versions below 1.27, we have seen support for in-place updates of Resources in many Kubernetes distributions. Different distributions may adopt different approaches to implement this feature.\n\nTo accommodate these Kubernetes distributions, KubeBlocks has introduced the `IgnorePodVerticalScaling` feature switch. When this feature is enabled, KubeBlocks ignores updates to CPU and Memory in Resources during instance updates, ensuring that the Resources configuration of the final rendered Pod remains consistent with the Resources configuration of the currently running Pod.\n",
    "path": "docs/release-0_9/user_docs/maintenance/in-place-update/ignore-vertical-scale",
    "description": " # Enable in-place update  In Kubernetes versions below 1.27, we have seen support for in-place updates of Resources in many Kubernetes distributions. Different distributions may adopt different approaches to implement this feature.  To accommodate these Kubernetes distributions, KubeBlocks has intr"
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_in-place-update_overview",
    "title": "Overview on in-place update",
    "content": "\n# Overview on in-place update\n\nIn its earlier versions, KubeBlocks ultimately generated Workloads as StatefulSets. For StatefulSets, any change in the segment of PodTemplate may result in the update of all pods, and the method of update is called `Recreate`, that deletes all current pods and creates a new one. This is obviously not the best practice for database management, which has a high requirement on system availability.\nTo address this issue, KubeBlocks introduced the instance in-place update feature starting from version 0.9, reducing the impact on system availability during instance updates.\n\n## Which fields of an instance support in-place updates?\n\nIn principle, KubeBlocks instance in-place updates leverage [the Kubernetes Pod API's in-place update capability](https://kubernetes.io/docs/concepts/workloads/pods/#pod-update-and-replacement). Therefore, the specific supported fields are as follows:\n\n* `annotations`\n* `labels`\n* `spec.activeDeadlineSeconds`\n* `spec.initContainers[*].image`\n* `spec.containers[*].image`\n* `spec.tolerations (only supports adding Toleration)`\n\nStarting from Kubernetes version 1.27, support for in-place updates of CPU and Memory can be further enabled through the `InPlacePodVerticalScaling` feature switch. KubeBlocks also supports the `InPlacePodVerticalScaling` feature switc which further supports the following capabilities:\n\nFor Kubernetes versions equal to or greater than 1.27 with InPlacePodVerticalScaling enabled, the following fields' in-place updates are supported:\n\n* `spec.containers[*].resources.requests[\"cpu\"]`\n* `spec.containers[*].resources.requests[\"memory\"]`\n* `spec.containers[*].resources.limits[\"cpu\"]`\n* `spec.containers[*].resources.limits[\"memory\"]`\n\nIt is important to note that after successful resource resizing, some applications may need to be restarted to recognize the new resource configuration. In such cases, further configuration of container `restartPolicy` is required in ClusterDefinition or ComponentDefi",
    "path": "docs/release-0_9/user_docs/maintenance/in-place-update/overview",
    "description": " # Overview on in-place update  In its earlier versions, KubeBlocks ultimately generated Workloads as StatefulSets. For StatefulSets, any change in the segment of PodTemplate may result in the update of all pods, and the method of update is called `Recreate`, that deletes all current pods and create"
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_instance-template_how-to-use-instance-template",
    "title": "Apply instance template",
    "content": "\n# Apply instance template\n\nInstance templates can be applied to many scenarios. In this section, we take a RisingWave cluster as an example.\n\nKubeBlocks supports the management of RisingWave clusters. The RisingWave addon is contributed by the RisingWave official team. For RisingWave to function optimally, it relies on an external storage solution, such as AWS S3 or Alibaba Cloud OSS, to serve as its state backend. When creating a RisingWave cluster, it is necessary to configure credentials and other information for the external storage to ensure normal operation, and this information may vary for each cluster.\n\nIn the official image of RisingWave, this information can be injected via environment variables. Therefore, in KubeBlocks 0.9, we can configure corresponding environment variables in the instance template and set the values of these environment variables each time a cluster is created, so as to inject credential information into the container of RisingWave.\n\n## An example\n\nIn the default template of RisingWave addon, [the environment variables](https://github.com/apecloud/kubeblocks-addons/blob/main/addons/risingwave/templates/cmpd-compute.yaml#L26) are configured as follows:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1alpha1\nkind: ComponentDefinition\nmetadata:\n  name: risingwave\n# ...\nspec:\n#...\n  runtime:\n    containers:\n      - name: compute\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        command:\n        - /risingwave/bin/risingwave\n        - compute-node\n        env:\n        - name: RUST_BACKTRACE\n          value: \"1\"\n        - name: RW_CONFIG_PATH\n          value: /risingwave/config/risingwave.toml\n        - name: RW_LISTEN_ADDR\n          value: 0.0.0.0:5688\n        - name: RW_ADVERTISE_ADDR\n          value: $(KB_POD_FQDN):5688\n        - name: RW_META_ADDR\n          value: load-balance+http://$(metaSvc)-headless:5690\n        - name: RW_METR",
    "path": "docs/release-0_9/user_docs/maintenance/instance-template/how-to-use-instance-template",
    "description": " # Apply instance template  Instance templates can be applied to many scenarios. In this section, we take a RisingWave cluster as an example.  KubeBlocks supports the management of RisingWave clusters. The RisingWave addon is contributed by the RisingWave official team. For RisingWave to function op"
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_instance-template_introduction",
    "title": "Introduction of Instance Template",
    "content": "\n# Introduction of instance template\n\n## What is an instance template\n\nAn *instance* serves as the fundamental unit in KubeBlocks, comprising a Pod along with several auxiliary objects. To simplify, you can initially think of it as a Pod, and henceforth, we'll consistently refer to it as an \"Instance.\"\n\nStarting from version 0.9, we're able to establish multiple instance templates for a particular component within a cluster. These instance templates include several fields such as Name, Replicas, Annotations, Labels, Env, Tolerations, NodeSelector, etc. These fields will ultimately override the corresponding ones in the default template (originating from ClusterDefinition and ComponentDefinition) to generate the final template for rendering the instance.\n\n## Why do we introduce the instance template\n\nIn KubeBlocks, a *Cluster* is composed of several *Components*, where each *Component* ultimately oversees multiple *Pods* and auxiliary objects.\n\nPrior to version 0.9, these pods were rendered from a shared PodTemplate, as defined in either ClusterDefinition or ComponentDefinition. However, this design can’t meet the following demands:\n\n - For Clusters rendered from the same addon, setting separate scheduling configurations such as *NodeName*, *NodeSelector*, or *Tolerations*.\n - For Components rendered from the same addon, adding custom *Annotations*, *Labels*, or ENV to the Pods they manage.\n - For Pods managed by the same Component, configuring different *CPU*, *Memory*, and other *Resource Requests* and *Limits*.\n\nWith various similar requirements emerging, the Cluster API introduced the Instance Template feature from version 0.9 onwards to cater to these needs.\n",
    "path": "docs/release-0_9/user_docs/maintenance/instance-template/introduction",
    "description": " # Introduction of instance template  ## What is an instance template  An *instance* serves as the fundamental unit in KubeBlocks, comprising a Pod along with several auxiliary objects. To simplify, you can initially think of it as a Pod, and henceforth, we'll consistently refer to it as an \"Instanc"
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_resource-scheduling_resource-scheduling",
    "title": "Configure pod affinity for database clusters",
    "content": "\n\n# Configure pod affinity for database clusters\n\nAffinity controls the selection logic of pod allocation on nodes. By a reasonable allocation of Kubernetes pods on different nodes, the business availability, resource usage rate, and stability are improved.\n\nAffinity and toleration can be set by `kbcli` or editing the spec of a cluster or a component. Note that `kbcli` only supports the cluster-level configuration. If you want to realize the cluster-level and component-level configurations, you can configure the CR YAML file.\n\n\n\nYou can configure pod affinity and toleration in either the spec of a cluster or the spec of a component.\n\nThe cluster-level configuration is used as the default configuration of all components; if the pod affinity configuration exists in a component, the component-level configuration will take effect and cover the default cluster-level configuration.\n\n```yaml\nspec:\n  affinity:\n    podAntiAffinity: Preferred\n    topologyKeys:\n    - kubernetes.io/hostname\n    nodeLabels:\n      topology.kubernetes.io/zone: us-east-1a\n  tolerations:\n  - key: EngineType\n    operator: Equal\n    value: mysql\n    effect: NoSchedule\n  componentSpecs:\n  - name: mysql\n    componentDefRef: mysql\n    affinity:\n      podAntiAffinity: Required\n      topologyKeys:\n        - kubernetes.io/hostname\n...\n```\n\n**Description of parameters in the YAML file**\n\n* Affinity\n  \n  Parameters related to pod affinity are under the object of `spec.affinity` in the Cluster CR YAML file.\n  The pod affinity configuration can be applied to the cluster or component and the component-level configuration covers the cluster-level configuration.\n\n* Toleration\n  \n  Parameters related to toleration are under the object of `spec.tolerations` in the Cluster CR YAML file and Kubernetes native semantics are used. For the toleration parameter configuration, refer to the Kubernetes official document [Taints and Tolerations](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/).\n  ",
    "path": "docs/release-0_9/user_docs/maintenance/resource-scheduling/resource-scheduling",
    "description": "  # Configure pod affinity for database clusters  Affinity controls the selection logic of pod allocation on nodes. By a reasonable allocation of Kubernetes pods on different nodes, the business availability, resource usage rate, and stability are improved.  Affinity and toleration can be set by `kb"
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_scale_horizontal-scale",
    "title": "Horizontal Scale",
    "content": "\n\n# Horizontal Scale\n\nFrom v0.9.0, the horizontal scale provided by KubeBlocks supports ScaleIn and ScaleOut operations and supports scaling both replicas and instances.\n\n- ScaleIn: It supports scaling in the specified replicas and offloading specified instances.\n- ScaleOut: It supports scaling out the specified replicas and makes the offline instances online again.\n\nYou can perform the horizontal scale by modifying the cluster in a declarative API style or creating an OpsRequest:\n\n- Modifying the Cluster in a declarative API style\n\n    With the declarative API style, users can directly modify the Cluster YAML file to specify the number of replicas for each component and instance template. If the new number of replicas is greater than the current number of Pods, it indicates a scale-out; conversely, if the new number of replicas is less than the current number of Pods, it indicates a scale-in.\n\n- Creating an OpsRequest\n\n    Another approach is to specify the replica count increment in the OpsRequest. The controller will calculate the desired number of replicas based on the current number of Pods in the Cluster's components and the increment value, and perform scaling accordingly.\n\n:::note\n\n- In cases of concurrent modifications, such as multiple controllers concurrently modifying the number of Pods, the calculated number of Pods might be inaccurate. You can ensure the order is on the client side or set KUBEBLOCKS_RECONCILE_WORKERS=1.\n- If there is an ongoing scaling operation using the declarative API, this operation will be terminated.\n- From v0.9.0, for MySQL and PostgreSQL, after horizontal scaling is performed, KubeBlocks automatically matches the appropriate configuration template based on the new specification. This is the KubeBlocks dynamic configuration feature, which simplifies the process of configuring parameters, saves time and effort and reduces performance issues caused by incorrect configuration. For detailed instructions, refer to [Configuration](./.",
    "path": "docs/release-0_9/user_docs/maintenance/scale/horizontal-scale",
    "description": "  # Horizontal Scale  From v0.9.0, the horizontal scale provided by KubeBlocks supports ScaleIn and ScaleOut operations and supports scaling both replicas and instances.  - ScaleIn: It supports scaling in the specified replicas and offloading specified instances. - ScaleOut: It supports scaling out "
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_scale_vertical-scale",
    "title": "Vertical Scale",
    "content": "\n\n# Vertical Scale\n\nYou can change the resource requirements and limits (CPU and storage) by performing the vertical scale. For example, you can change the resource class from 1C2G to 2C4G.\n\nThis tutorial takes MySQL as an example to illustrate how to vertically scale a cluster.\n\n:::note\n\nFrom v0.9.0, for MySQL and PostgreSQL, after vertical scaling is performed, KubeBlocks automatically matches the appropriate configuration template based on the new specification. This is the KubeBlocks dynamic configuration feature. This feature simplifies the process of configuring parameters, saves time and effort and reduces performance issues caused by incorrect configuration. For detailed instructions, refer to [Configuration](./../../kubeblocks-for-apecloud-mysql/configuration/configuration.md).\n\n:::\n\n## Before you start\n\nCheck whether the cluster status is `Running`. Otherwise, the following operations may fail.\n\n```bash\nkubectl get cluster mycluster\n>\nNAME        CLUSTER-DEFINITION   VERSION        TERMINATION-POLICY   STATUS    AGE\nmycluster   mysql                mysql-8.0.33   Delete               Running   4d18h\n```\n\n## Steps\n\nThere are two ways to apply vertical scaling.\n\n\n\n1. Apply an OpsRequest to the specified cluster. Configure the parameters according to your needs.\n\n   ```bash\n   kubectl apply -f - \n   NAMESPACE   NAME                   TYPE              CLUSTER     STATUS    PROGRESS   AGE\n   demo        ops-vertical-scaling   VerticalScaling   mycluster   Succeed   3/3        6m\n   ```\n\n   If an error occurs, you can troubleshoot with `kubectl describe ops -n demo` command to view the events of this operation.\n\n3. Check whether the corresponding resources change.\n\n   ```bash\n   kubectl describe cluster mycluster -n demo\n   ```\n\n\n  \n\n1. Change the configuration of `spec.componentSpecs.resources` in the YAML file. `spec.componentSpecs.resources` controls the requirement and limit of resources and changing them triggers a vertical scaling.\n\n   ```yaml\n   kubectl ",
    "path": "docs/release-0_9/user_docs/maintenance/scale/vertical-scale",
    "description": "  # Vertical Scale  You can change the resource requirements and limits (CPU and storage) by performing the vertical scale. For example, you can change the resource class from 1C2G to 2C4G.  This tutorial takes MySQL as an example to illustrate how to vertically scale a cluster.  :::note  From v0.9."
  },
  {
    "id": "docs_en_release-1_0_user_docs_concepts_backup-and-restore_introduction",
    "title": "Introduction",
    "content": "\n# Introduction\n\nKubeBlocks provides the backup and restore function to ensure the safety and reliability of your data. The backup and restore function of KubeBlocks relies on BackupRepo and before using the full backup and restore function, you need to [configure BackupRepo first](./backup/backup-repo.md).\n\nKubeBlocks adopts physical backup which takes the physical files in a database as the backup object. You can choose one backup option based on your demands to back up the cluster data on demand or by schedule.\n\n* [On-demand backup](./backup/on-demand-backup.md): Based on different backup options, on-demand backup can be further divided into backup tool and snapshot backup.\n  * Backup tool: You can use the backup tool of the corresponding data product, such as MySQL XtraBackup and PostgreSQL pg_basebackup. KubeBlocks supports configuring backup tools for different data products.\n  * Snapshot backup: If your data is stored in a cloud disk that supports snapshots, you can create a data backup by snapshots. Snapshot backup is usually faster than a backup tool, and thus is recommended.\n\n* [Scheduled backup](./backup/scheduled-backup.md): You can specify retention time, backup method, time, and other parameters to customize your backup schedule.\n\nAs for the restore function, KubeBlocks supports restoring data from the backup set.\n\n* Restore\n  * [Restore data from the backup set](./restore/restore-data-from-backup-set.md)\n\nFollow the steps below to back up and restore your cluster.\n\n1. [Configure BackupRepo](./backup/backup-repo.md).\n2. [Configure BackupPolicy](./backup/configure-backuppolicy.md).\n3. Backup your cluster [on demand](./backup/on-demand-backup.md) or [by schedule](./backup/scheduled-backup.md).\n4. Restore your data by [PITR](./restore/pitr.md) or from the [backup set](./restore/restore-data-from-backup-set.md).\n",
    "path": "docs/release-1_0/user_docs/concepts/backup-and-restore/introduction",
    "description": " # Introduction  KubeBlocks provides the backup and restore function to ensure the safety and reliability of your data. The backup and restore function of KubeBlocks relies on BackupRepo and before using the full backup and restore function, you need to [configure BackupRepo first](./backup/backup-r"
  },
  {
    "id": "docs_en_release-1_0_user_docs_concepts_in-place-update_ignore-vertical-scale",
    "title": "Enable in-place update",
    "content": "\n# Enable in-place update\n\nIn Kubernetes versions below 1.27, we have seen support for in-place updates of Resources in many Kubernetes distributions. Different distributions may adopt different approaches to implement this feature.\n\nTo accommodate these Kubernetes distributions, KubeBlocks has introduced the `IgnorePodVerticalScaling` feature switch. When this feature is enabled, KubeBlocks ignores updates to CPU and Memory in Resources during instance updates, ensuring that the Resources configuration of the final rendered Pod remains consistent with the Resources configuration of the currently running Pod.\n",
    "path": "docs/release-1_0/user_docs/concepts/in-place-update/ignore-vertical-scale",
    "description": " # Enable in-place update  In Kubernetes versions below 1.27, we have seen support for in-place updates of Resources in many Kubernetes distributions. Different distributions may adopt different approaches to implement this feature.  To accommodate these Kubernetes distributions, KubeBlocks has intr"
  },
  {
    "id": "docs_en_release-1_0_user_docs_concepts_in-place-update_overview",
    "title": "Introduction",
    "content": "\n# Overview\n\nIn its earlier versions, KubeBlocks ultimately generated Workloads as StatefulSets. For StatefulSets, any change in the segment of PodTemplate may result in the update of all pods, and the method of update is called `Recreate`, that deletes all current pods and creates a new one. This is obviously not the best practice for database management, which has a high requirement on system availability.\nTo address this issue, KubeBlocks introduced the instance in-place update feature starting from version 0.9, reducing the impact on system availability during instance updates.\n\n## Which fields of an instance support in-place updates?\n\nIn principle, KubeBlocks instance in-place updates leverage [the Kubernetes Pod API's in-place update capability](https://kubernetes.io/docs/concepts/workloads/pods/#pod-update-and-replacement). Therefore, the specific supported fields are as follows:\n\n* `annotations`\n* `labels`\n* `spec.activeDeadlineSeconds`\n* `spec.initContainers[*].image`\n* `spec.containers[*].image`\n* `spec.tolerations (only supports adding Toleration)`\n\nStarting from Kubernetes version 1.27, support for in-place updates of CPU and Memory can be further enabled through the `InPlacePodVerticalScaling` feature switch. KubeBlocks also supports the `InPlacePodVerticalScaling` feature switch which further supports the following capabilities:\n\nFor Kubernetes versions equal to or greater than 1.27 with InPlacePodVerticalScaling enabled, the following fields' in-place updates are supported:\n\n* `spec.containers[*].resources.requests[\"cpu\"]`\n* `spec.containers[*].resources.requests[\"memory\"]`\n* `spec.containers[*].resources.limits[\"cpu\"]`\n* `spec.containers[*].resources.limits[\"memory\"]`\n\nIt is important to note that after successful resource resizing, some applications may need to be restarted to recognize the new resource configuration. In such cases, further configuration of container `restartPolicy` is required in ClusterDefinition or ComponentDefinition.\n\nFor PVC, ",
    "path": "docs/release-1_0/user_docs/concepts/in-place-update/overview",
    "description": " # Overview  In its earlier versions, KubeBlocks ultimately generated Workloads as StatefulSets. For StatefulSets, any change in the segment of PodTemplate may result in the update of all pods, and the method of update is called `Recreate`, that deletes all current pods and creates a new one. This i"
  },
  {
    "id": "docs_en_release-1_0_user_docs_concepts_instance-template_how-to-use-instance-template",
    "title": "Apply instance template",
    "content": "\n# Apply instance template\n\nInstance templates can be applied to many scenarios. In this section, we take a RisingWave cluster as an example.\n\nKubeBlocks supports the management of RisingWave clusters. The RisingWave addon is contributed by the RisingWave official team. For RisingWave to function optimally, it relies on an external storage solution, such as AWS S3 or Alibaba Cloud OSS, to serve as its state backend. When creating a RisingWave cluster, it is necessary to configure credentials and other information for the external storage to ensure normal operation, and this information may vary for each cluster.\n\nIn the official image of RisingWave, this information can be injected via environment variables. Therefore, in KubeBlocks 0.9, we can configure corresponding environment variables in the instance template and set the values of these environment variables each time a cluster is created, so as to inject credential information into the container of RisingWave.\n\n## An example\n\nIn the default template of RisingWave addon, [the environment variables](https://github.com/apecloud/kubeblocks-addons/blob/main/addons/risingwave/templates/cmpd-compute.yaml#L26) are configured as follows:\n\n```yaml\napiVersion: apps.kubeblocks.io/v1alpha1\nkind: ComponentDefinition\nmetadata:\n  name: risingwave\n# ...\nspec:\n#...\n  runtime:\n    containers:\n      - name: compute\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        command:\n        - /risingwave/bin/risingwave\n        - compute-node\n        env:\n        - name: RUST_BACKTRACE\n          value: \"1\"\n        - name: RW_CONFIG_PATH\n          value: /risingwave/config/risingwave.toml\n        - name: RW_LISTEN_ADDR\n          value: 0.0.0.0:5688\n        - name: RW_ADVERTISE_ADDR\n          value: $(KB_POD_FQDN):5688\n        - name: RW_META_ADDR\n          value: load-balance+http://$(metaSvc)-headless:5690\n        - name: RW_METR",
    "path": "docs/release-1_0/user_docs/concepts/instance-template/how-to-use-instance-template",
    "description": " # Apply instance template  Instance templates can be applied to many scenarios. In this section, we take a RisingWave cluster as an example.  KubeBlocks supports the management of RisingWave clusters. The RisingWave addon is contributed by the RisingWave official team. For RisingWave to function op"
  },
  {
    "id": "docs_en_release-1_0_user_docs_concepts_instance-template_introduction",
    "title": "Introduction",
    "content": "\n# Introduction\n\n## What is an instance template\n\nAn *instance* serves as the fundamental unit in KubeBlocks, comprising a Pod along with several auxiliary objects. To simplify, you can initially think of it as a Pod, and henceforth, we'll consistently refer to it as an \"Instance.\"\n\nStarting from version 0.9, we're able to establish multiple instance templates for a particular component within a cluster. These instance templates include several fields such as Name, Replicas, Annotations, Labels, Env, Tolerations, NodeSelector, etc. These fields will ultimately override the corresponding ones in the default template (originating from ClusterDefinition and ComponentDefinition) to generate the final template for rendering the instance.\n\n## Why do we introduce the instance template\n\nIn KubeBlocks, a *Cluster* is composed of several *Components*, where each *Component* ultimately oversees multiple *Pods* and auxiliary objects.\n\nPrior to version 0.9, these pods were rendered from a shared PodTemplate, as defined in either ClusterDefinition or ComponentDefinition. However, this design can’t meet the following demands:\n\n - For Clusters rendered from the same addon, setting separate scheduling configurations such as *NodeName*, *NodeSelector*, or *Tolerations*.\n - For Components rendered from the same addon, adding custom *Annotations*, *Labels*, or ENV to the Pods they manage.\n - For Pods managed by the same Component, configuring different *CPU*, *Memory*, and other *Resource Requests* and *Limits*.\n\nWith various similar requirements emerging, the Cluster API introduced the Instance Template feature from version 0.9 onwards to cater to these needs.\n",
    "path": "docs/release-1_0/user_docs/concepts/instance-template/introduction",
    "description": " # Introduction  ## What is an instance template  An *instance* serves as the fundamental unit in KubeBlocks, comprising a Pod along with several auxiliary objects. To simplify, you can initially think of it as a Pod, and henceforth, we'll consistently refer to it as an \"Instance.\"  Starting from ve"
  },
  {
    "id": "docs_en_release-1_0_user_docs_references_api-reference_add-on",
    "title": "Add-On API Reference",
    "content": "\n\n\nPackages:\n\n\n\n\nextensions.kubeblocks.io/v1alpha1\n\n\n\nextensions.kubeblocks.io/v1alpha1\n\n\nResource Types:\n\n\nAddon\n\n\n\nAddon\n\n\n\n\n\nAddon is the Schema for the add-ons API.\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`extensions.kubeblocks.io/v1alpha1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`Addon`\n\n\n\n\n\n\n\n`metadata`\n\n\nKubernetes meta/v1.ObjectMeta\n\n\n\n\n\n\nRefer to the Kubernetes API documentation for the fields of the\n`metadata` field.\n\n\n\n\n\n\n\n\n`spec`\n\n\nAddonSpec\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`description`\n\nstring\n\n\n\n\n\n(Optional)\n\n\nSpecifies the description of the add-on.\n\n\n\n\n\n\n\n\n\n`type`\n\n\nAddonType\n\n\n\n\n\n\n\n\nDefines the type of the add-on. The only valid value is &lsquo;helm&rsquo;.\n\n\n\n\n\n\n\n\n\n`version`\n\nstring\n\n\n\n\n\n(Optional)\n\n\nIndicates the version of the add-on.\n\n\n\n\n\n\n\n\n\n`provider`\n\nstring\n\n\n\n\n\n(Optional)\n\n\nSpecifies the provider of the add-on.\n\n\n\n\n\n\n\n\n\n`helm`\n\n\nHelmTypeInstallSpec\n\n\n\n\n\n\n(Optional)\n\n\nRepresents the Helm installation specifications. This is only processed\nwhen the type is set to &lsquo;helm&rsquo;.\n\n\n\n\n\n\n\n\n\n`defaultInstallValues`\n\n\n[]AddonDefaultInstallSpecItem\n\n\n\n\n\n\n\n\nSpecifies the default installation parameters.\n\n\n\n\n\n\n\n\n\n`install`\n\n\nAddonInstallSpec\n\n\n\n\n\n\n(Optional)\n\n\nDefines the installation parameters.\n\n\n\n\n\n\n\n\n\n`installable`\n\n\nInstallableSpec\n\n\n\n\n\n\n(Optional)\n\n\nRepresents the installable specifications of the add-on. This includes\nthe selector and auto-install settings.\n\n\n\n\n\n\n\n\n\n`cliPlugins`\n\n\n[]CliPlugin\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies the CLI plugin installation specifications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`status`\n\n\nAddonStatus\n\n\n\n\n\n\n\n\n\n\n\n\n\nAddonDefaultInstallSpecItem\n\n\n\n\n\n(Appears on:AddonSpec)\n\n\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`AddonInstallSpec`\n\n\nAddonInstallSpec\n\n\n\n\n\n\n\n\n\n(Members of `AddonInstallSpec` are embedded into this type.)\n\n\n\n\n\n\n\n\n\n\n`selectors`\n\n\n[]SelectorRequirement\n\n\n\n\n\n\n(Optional)\n\n\nIndicates the default selectors for add-on installations. If multiple selectors are provided,\nall selectors must evaluate to true.\n\n\n\n\n\n\n\n\nAddonInstallExtraItem\n\n\n\n\n\n(Appears on:AddonInstal",
    "path": "docs/release-1_0/user_docs/references/api-reference/add-on",
    "description": "   Packages:     extensions.kubeblocks.io/v1alpha1    extensions.kubeblocks.io/v1alpha1   Resource Types:   Addon    Addon      Addon is the Schema for the add-ons API.       Field Description         `apiVersion` string    `extensions.kubeblocks.io/v1alpha1`         `kind` string    `Addon`        "
  },
  {
    "id": "docs_en_release-1_0_user_docs_references_api-reference_cluster",
    "title": "Cluster API Reference",
    "content": "\n\n\nPackages:\n\n\n\n\napps.kubeblocks.io/v1\n\n\n\n\napps.kubeblocks.io/v1alpha1\n\n\n\n\napps.kubeblocks.io/v1beta1\n\n\n\n\nworkloads.kubeblocks.io/v1\n\n\n\n\nworkloads.kubeblocks.io/v1alpha1\n\n\n\napps.kubeblocks.io/v1\n\n\nResource Types:\n\n\nCluster\n\n\n\nClusterDefinition\n\n\n\nComponent\n\n\n\nComponentDefinition\n\n\n\nComponentVersion\n\n\n\nServiceDescriptor\n\n\n\nShardingDefinition\n\n\n\nSidecarDefinition\n\n\n\nCluster\n\n\n\n\n\nCluster offers a unified management interface for a wide variety of database and storage systems:\n\n\n\nRelational databases: MySQL, PostgreSQL, MariaDB\n\n\nNoSQL databases: Redis, MongoDB\n\n\nKV stores: ZooKeeper, etcd\n\n\nAnalytics systems: ElasticSearch, OpenSearch, ClickHouse, Doris, StarRocks, Solr\n\n\nMessage queues: Kafka, Pulsar\n\n\nDistributed SQL: TiDB, OceanBase\n\n\nVector databases: Qdrant, Milvus, Weaviate\n\n\nObject storage: Minio\n\n\n\n\nKubeBlocks utilizes an abstraction layer to encapsulate the characteristics of these diverse systems.\nA Cluster is composed of multiple Components, each defined by vendors or KubeBlocks Addon developers via ComponentDefinition,\narranged in Directed Acyclic Graph (DAG) topologies.\nThe topologies, defined in a ClusterDefinition, coordinate reconciliation across Cluster&rsquo;s lifecycle phases:\nCreating, Running, Updating, Stopping, Stopped, Deleting.\nLifecycle management ensures that each Component operates in harmony, executing appropriate actions at each lifecycle stage.\n\n\n\nFor sharded-nothing architecture, the Cluster supports managing multiple shards,\neach shard managed by a separate Component, supporting dynamic resharding.\n\n\n\nThe Cluster object is aimed to maintain the overall integrity and availability of a database cluster,\nserves as the central control point, abstracting the complexity of multiple-component management,\nand providing a unified interface for cluster-wide operations.\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`apps.kubeblocks.io/v1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`Cluster`\n\n\n\n\n\n\n\n`metadata`\n\n\nKubernetes meta/v1.ObjectMeta\n\n\n\n\n\n\nRefer to t",
    "path": "docs/release-1_0/user_docs/references/api-reference/cluster",
    "description": "   Packages:     apps.kubeblocks.io/v1     apps.kubeblocks.io/v1alpha1     apps.kubeblocks.io/v1beta1     workloads.kubeblocks.io/v1     workloads.kubeblocks.io/v1alpha1    apps.kubeblocks.io/v1   Resource Types:   Cluster    ClusterDefinition    Component    ComponentDefinition    ComponentVersion "
  },
  {
    "id": "docs_en_release-1_0_user_docs_references_api-reference_dataprotection",
    "title": "Dataprotection API Reference",
    "content": "\n\n\nPackages:\n\n\n\n\ndataprotection.kubeblocks.io/v1alpha1\n\n\n\ndataprotection.kubeblocks.io/v1alpha1\n\n\nResource Types:\n\n\nActionSet\n\n\n\nBackup\n\n\n\nBackupPolicy\n\n\n\nBackupRepo\n\n\n\nBackupSchedule\n\n\n\nRestore\n\n\n\nStorageProvider\n\n\n\nActionSet\n\n\n\n\n\nActionSet is the Schema for the actionsets API\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`dataprotection.kubeblocks.io/v1alpha1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`ActionSet`\n\n\n\n\n\n\n\n`metadata`\n\n\nKubernetes meta/v1.ObjectMeta\n\n\n\n\n\n\nRefer to the Kubernetes API documentation for the fields of the\n`metadata` field.\n\n\n\n\n\n\n\n\n`spec`\n\n\nActionSetSpec\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`backupType`\n\n\nBackupType\n\n\n\n\n\n\n\n\nSpecifies the backup type. Supported values include:\n\n\n\n`Full` for a full backup.\n\n\n`Incremental` back up data that have changed since the last backup (either full or incremental).\n\n\n`Differential` back up data that has changed since the last full backup.\n\n\n`Continuous` back up transaction logs continuously, such as MySQL binlog, PostgreSQL WAL, etc.\n\n\n`Selective` back up data more precisely, use custom parameters, such as specific databases or tables.\n\n\n\n\nContinuous backup is essential for implementing Point-in-Time Recovery (PITR).\n\n\n\n\n\n\n\n\n\n`parametersSchema`\n\n\nActionSetParametersSchema\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies the schema of parameters in backups and restores before their usage.\n\n\n\n\n\n\n\n\n\n`env`\n\n\n[]Kubernetes core/v1.EnvVar\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies a list of environment variables to be set in the container.\n\n\n\n\n\n\n\n\n\n`envFrom`\n\n\n[]Kubernetes core/v1.EnvFromSource\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies a list of sources to populate environment variables in the container.\nThe keys within a source must be a C_IDENTIFIER. Any invalid keys will be\nreported as an event when the container starts. If a key exists in multiple\nsources, the value from the last source will take precedence. Any values\ndefined by an Env with a duplicate key will take precedence.\n\n\n\nThis field cannot be updated.\n\n\n\n\n\n\n\n\n\n`backup`\n\n\nBackupActionSpec\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies the ba",
    "path": "docs/release-1_0/user_docs/references/api-reference/dataprotection",
    "description": "   Packages:     dataprotection.kubeblocks.io/v1alpha1    dataprotection.kubeblocks.io/v1alpha1   Resource Types:   ActionSet    Backup    BackupPolicy    BackupRepo    BackupSchedule    Restore    StorageProvider    ActionSet      ActionSet is the Schema for the actionsets API       Field Descripti"
  },
  {
    "id": "docs_en_release-1_0_user_docs_references_api-reference_operations",
    "title": "Operations API Reference",
    "content": "\n\n\nPackages:\n\n\n\n\noperations.kubeblocks.io/v1alpha1\n\n\n\noperations.kubeblocks.io/v1alpha1\nResource Types:\n\n\nOpsDefinition\n\n\n\nOpsRequest\n\n\n\nOpsDefinition\n\n\n\n\n\nOpsDefinition is the Schema for the OpsDefinitions API.\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`operations.kubeblocks.io/v1alpha1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`OpsDefinition`\n\n\n\n\n\n\n\n`metadata`\n\n\nKubernetes meta/v1.ObjectMeta\n\n\n\n\n\n\nRefer to the Kubernetes API documentation for the fields of the\n`metadata` field.\n\n\n\n\n\n\n\n\n`spec`\n\n\nOpsDefinitionSpec\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`preConditions`\n\n\n[]PreCondition\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies the preconditions that must be met to run the actions for the operation.\nif set, it will check the condition before the Component runs this operation.\nExample:\n\n\n\n preConditions:\n - rule:\n     expression: '&#123;&#123; eq .component.status.phase &quot;Running&quot; &#125;&#125;'\n     message: Component is not in Running status.\n\n\n\n\n\n\n\n\n\n\n`podInfoExtractors`\n\n\n[]PodInfoExtractor\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies a list of PodInfoExtractor, each designed to select a specific Pod and extract selected runtime info\nfrom its PodSpec.\nThe extracted information, such as environment variables, volumes and tolerations, are then injected into\nJobs or Pods that execute the OpsActions defined in `actions`.\n\n\n\n\n\n\n\n\n\n`componentInfos`\n\n\n[]ComponentInfo\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies a list of ComponentDefinition for Components associated with this OpsDefinition.\nIt also includes connection credentials (address and account) for each Component.\n\n\n\n\n\n\n\n\n\n`parametersSchema`\n\n\nParametersSchema\n\n\n\n\n\n\n(Optional)\n\n\nSpecifies the schema for validating the data types and value ranges of parameters in OpsActions before their usage.\n\n\n\n\n\n\n\n\n\n`actions`\n\n\n[]OpsAction\n\n\n\n\n\n\n\n\nSpecifies a list of OpsAction where each customized action is executed sequentially.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`status`\n\n\nOpsDefinitionStatus\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpsRequest\n\n\n\n\n\nOpsRequest is the Schema for the opsrequests API\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersi",
    "path": "docs/release-1_0/user_docs/references/api-reference/operations",
    "description": "   Packages:     operations.kubeblocks.io/v1alpha1    operations.kubeblocks.io/v1alpha1 Resource Types:   OpsDefinition    OpsRequest    OpsDefinition      OpsDefinition is the Schema for the OpsDefinitions API.       Field Description         `apiVersion` string    `operations.kubeblocks.io/v1alpha"
  },
  {
    "id": "docs_en_release-1_0_user_docs_references_api-reference_parameters",
    "title": "Parameters API Reference",
    "content": "\n\n\nPackages:\n\n\n\n\nparameters.kubeblocks.io/v1alpha1\n\n\n\nparameters.kubeblocks.io/v1alpha1\nResource Types:\n\n\nComponentParameter\n\n\n\nParamConfigRenderer\n\n\n\nParameter\n\n\n\nParametersDefinition\n\n\n\nComponentParameter\n\n\n\n\n\nComponentParameter is the Schema for the componentparameters API\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`parameters.kubeblocks.io/v1alpha1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`ComponentParameter`\n\n\n\n\n\n\n\n`metadata`\n\n\nKubernetes meta/v1.ObjectMeta\n\n\n\n\n\n\nRefer to the Kubernetes API documentation for the fields of the\n`metadata` field.\n\n\n\n\n\n\n\n\n`spec`\n\n\nComponentParameterSpec\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`clusterName`\n\nstring\n\n\n\n\n\n(Optional)\n\n\nSpecifies the name of the Cluster that this configuration is associated with.\n\n\n\n\n\n\n\n\n\n`componentName`\n\nstring\n\n\n\n\n\n\n\nRepresents the name of the Component that this configuration pertains to.\n\n\n\n\n\n\n\n\n\n`configItemDetails`\n\n\n[]ConfigTemplateItemDetail\n\n\n\n\n\n\n(Optional)\n\n\nConfigItemDetails is an array of ConfigTemplateItemDetail objects.\n\n\n\nEach ConfigTemplateItemDetail corresponds to a configuration template,\nwhich is a ConfigMap that contains multiple configuration files.\nEach configuration file is stored as a key-value pair within the ConfigMap.\n\n\n\nThe ConfigTemplateItemDetail includes information such as:\n\n\n\nThe configuration template (a ConfigMap)\n\n\nThe corresponding ConfigConstraint (constraints and validation rules for the configuration)\n\n\nVolume mounts (for mounting the configuration files)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`status`\n\n\nComponentParameterStatus\n\n\n\n\n\n\n\n\n\n\n\n\n\nParamConfigRenderer\n\n\n\n\n\nParamConfigRenderer is the Schema for the paramconfigrenderers API\n\n\n\n\n\n\nField\nDescription\n\n\n\n\n\n\n\n\n`apiVersion`\nstring\n\n\n\n`parameters.kubeblocks.io/v1alpha1`\n\n\n\n\n\n\n\n\n`kind`\nstring\n\n\n\n`ParamConfigRenderer`\n\n\n\n\n\n\n\n`metadata`\n\n\nKubernetes meta/v1.ObjectMeta\n\n\n\n\n\n\nRefer to the Kubernetes API documentation for the fields of the\n`metadata` field.\n\n\n\n\n\n\n\n\n`spec`\n\n\nParamConfigRendererSpec\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n`componentDef`\n\nstring\n\n\n\n\n\n\n\nSpecifies the ComponentDefinit",
    "path": "docs/release-1_0/user_docs/references/api-reference/parameters",
    "description": "   Packages:     parameters.kubeblocks.io/v1alpha1    parameters.kubeblocks.io/v1alpha1 Resource Types:   ComponentParameter    ParamConfigRenderer    Parameter    ParametersDefinition    ComponentParameter      ComponentParameter is the Schema for the componentparameters API       Field Description"
  },
  {
    "id": "docs_en_preview_user_docs_concepts_backup-and-restore_backup_backup-repo",
    "title": "Configure BackupRepo",
    "content": "\n\n# Introduction\n\nBackupRepo is the storage repository for backup data. Currently, KubeBlocks supports configuring various object storage services as backup repositories, including OSS (Alibaba Cloud Object Storage Service), S3 (Amazon Simple Storage Service), COS (Tencent Cloud Object Storage), GCS (Google Cloud Storage), OBS (Huawei Cloud Object Storage), MinIO, and other S3-compatible services.\n\nYou can create multiple BackupRepos to suit different scenarios. For example, based on different businesses, the data of business A is stored in repository A, and the data of business B is stored in repository B. Or you can configure multiple repositories by region to realize geo-disaster recovery. But it is required to specify backup repositories when you create a backup. You can also create a default backup repository and KubeBlocks uses this default repository to store backup data if no specific repository is specified.\n\n## Before you start\n\nMake sure you have all the following prepared.\n\n* [Install kbcli](./../../../references/install-kbcli).\n* [Install kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl).\n* [Install Helm](https://helm.sh/docs/intro/install/).\n* [Install KubeBlocks](./../../../overview/install-kubeblocks).\n\n## Configure BackupRepo\n\nWith object storage services prepared, it's time to configure BackupRepo. KubeBlocks provides two ways for the configuration:\n\n* Automatic BackupRepo configuration during KubeBlocks installation;\n* Manual BackupRepo configuration for on-demand scenarios.\n\nIf you don't have any object storage at hand, please refer to [Install MinIO](#install-minio)\n\n### Access BackupRepo\n\nThere are two methods to access remote object storage:\n\n| Method | Description | Requirements | Security Consideration |\n|--------|-------------|--------------|------------------------|\n| Tool | Uses command-line tools to directly access remote storage | No additional driver required | Synchronizes credentials as secrets across namespaces |\n| Mount | M",
    "path": "docs/preview/user_docs/concepts/backup-and-restore/backup/backup-repo",
    "description": "  # Introduction  BackupRepo is the storage repository for backup data. Currently, KubeBlocks supports configuring various object storage services as backup repositories, including OSS (Alibaba Cloud Object Storage Service), S3 (Amazon Simple Storage Service), COS (Tencent Cloud Object Storage), GCS"
  },
  {
    "id": "docs_en_preview_user_docs_concepts_backup-and-restore_backup_configure-backuppolicy",
    "title": "Configure BackupPolicy",
    "content": "\n\n# Configure BackupPolicy\n\n## Configure encryption key\n\nTo ensure that the restored cluster can access the data properly, KubeBlocks encrypts the cluster's credentials during the backup process and securely stores it in the Annotation of the Backup object. Therefore, to protect your data security, it is strongly recommended to carefully assign Get/List permissions for backup objects and specify an encryption key during the installation or upgrade of KubeBlocks. These measures will help ensure the proper protection of your data.\n\nKubeBlocks has integrated data encryption functionality for datasafed since v0.9.0. Currently, the supported encryption algorithms include `AES-128-CFB`, `AES-192-CFB`, and `AES-256-CFB`. This function allows backup data to be encrypted before being written to storage. The encryption key then will be used to encrypt connection passwords and also to back up data. You can reference existing keys or create different secret keys for database clusters according to actual needs.\n\n### Reference an existing key\n\nIf the secret already exists, you can choose to directly reference it without setting the `dataProtection.encryptionKey`. KubeBlocks provides a quick way to reference an existing key for encryption.\n\nAssuming there is a pre-defined secret named `dp-encryption-key` and a key `encryptionKey` inside it. For example, a secret created by this command.\n\n```bash\nkubectl create secret generic dp-encryption-key \\\n    --from-literal=encryptionKey='S!B\\*d$zDsb='\n```\n\nAnd then you can reference it when installing or upgrading KubeBlocks.\n\n```bash\nkbcli kubeblocks install \\\n    --set dataProtection.encryptionKeySecretKeyRef.name=\"dp-encryption-key\" \\\n    --set dataProtection.encryptionKeySecretKeyRef.key=\"encryptionKey\"\n# The above command is equivalent to:\n# kbcli kubeblocks install --set dataProtection.encryptionKey='S!B\\*d$zDsb='\n```\n\n### Create a new key\n\nIf you do not need to enable backup encryption by default, or if you need to use a separate `en",
    "path": "docs/preview/user_docs/concepts/backup-and-restore/backup/configure-backuppolicy",
    "description": "  # Configure BackupPolicy  ## Configure encryption key  To ensure that the restored cluster can access the data properly, KubeBlocks encrypts the cluster's credentials during the backup process and securely stores it in the Annotation of the Backup object. Therefore, to protect your data security, "
  },
  {
    "id": "docs_en_preview_user_docs_concepts_backup-and-restore_backup_on-demand-backup",
    "title": "On-demand backup",
    "content": "\n\n# On-demand backup\n\nKubeBlocks supports on-demand backups. You can customize your backup method by specifying `--method`. The instructions below take using a backup tool and volume snapshot as examples.\n\n## Backup tool\n\nThe following command uses the `xtrabackup` backup method to create a backup named `mybackup`.\n\n\n\nTo Create a backup:\n```bash\nkubectl apply -f - \nNAME       POLICY                          METHOD       REPO     STATUS      TOTAL-SIZE   DURATION   DELETION-POLICY   CREATION-TIME          COMPLETION-TIME        EXPIRATION-TIME\nmybackup   mycluster-mysql-backup-policy   xtrabackup   kb-oss   Completed   1632402      10s        Delete            2025-05-26T10:14:33Z   2025-05-26T10:14:42Z\n```\n\n\n\nCreate a backup\n```bash\nkbcli cluster backup mycluster --name mybackup --method xtrabackup\n>\nBackup mybackup created successfully, you can view the progress:\n\tkbcli cluster list-backups --names=mybackup -n default\n```\n\nView the backup\n```bash\nkbcli cluster list-backups --names mybackup\n>\nNAME       NAMESPACE   SOURCE-CLUSTER   METHOD       STATUS      TOTAL-SIZE   DURATION   DELETION-POLICY   CREATE-TIME                  COMPLETION-TIME              EXPIRATION\nmybackup   default     mycluster        xtrabackup   Completed   1632402      10s        Delete            May 26,2025 18:14 UTC+0800   May 26,2025 18:14 UTC+0800\n```\n\n\n\n\n\n## Volume snapshot backup\n\n:::note\n**Prerequisites**\nVolume snapshot backups require:\n- StorageClass must support volume snapshots\n\nPlease check the list of CS Drivers and their features at: https://kubernetes-csi.github.io/docs/drivers.html\n\n:::\n\nTo create a backup using the snapshot, the `backupMethod` in the YAML configuration file or the `--method` field in the kbcli command should be set to `volume-snapshot`.\n\n\n\n```bash\n# Create a backup\nkubectl apply -f - \nNAME                 POLICY                           METHOD            REPO   STATUS    TOTAL-SIZE   DURATION   DELETION-POLICY   CREATION-TIME          COMPLETION-TIME   EXPIR",
    "path": "docs/preview/user_docs/concepts/backup-and-restore/backup/on-demand-backup",
    "description": "  # On-demand backup  KubeBlocks supports on-demand backups. You can customize your backup method by specifying `--method`. The instructions below take using a backup tool and volume snapshot as examples.  ## Backup tool  The following command uses the `xtrabackup` backup method to create a backup n"
  },
  {
    "id": "docs_en_preview_user_docs_concepts_backup-and-restore_backup_scheduled-backup",
    "title": "Scheduled backup",
    "content": "\n\n# Scheduled backup\n\nKubeBlocks supports configuring scheduled backups for clusters.\n\n\n\nModify the backup field with kubectl as follows.\n\n```bash\nkubectl edit cluster -n default mycluster\n```\n\nEdit the cluster YAML.\n\n```yaml\nspec:\n  ...\n  backup:\n    # Whether to enable automatic backups\n    enabled: true\n    # UTC timezone, the example below stands for 2 A.M. every Monday\n    cronExpression: 0 18 * * *\n    # Use xtrabackup for backups. If your storage supports snapshot, you can change it to volume-snapshot\n    method: xtrabackup\n    # Whether to enable PITR\n    pitrEnabled: false\n    # Retention period for a backup set\n    retentionPeriod: 7d\n    # BackupRepo\n    repoName: my-repo\n```\n\nIn the above YAML file, you can set whether to enable automatic backups and PITR as needed, and also specify backup methods, repo names, retention periods, etc.\n\n\n\n\n```bash\nkbcli cluster update mycluster --backup-enabled=true \\\n--backup-method=xtrabackup --backup-repo-name=my-repo \\\n--backup-retention-period=7d --backup-cron-expression=\"0 18 * * *\"\n```\n\n- `--backup-enabled` indicates whether to enable scheduled backups.\n- `--backup-method` specifies the backup method. You can use the `kbcli cluster describe-backup-policy mycluster` command to view the supported backup methods.\n- `--backup-repo-name` specifies the name of the BackupRepo.\n- `--backup-retention-period` specifies the retention period for backups, which is 7 days in the example.\n- `--backup-cron-expression` specifies the backup schedule using a cron expression in UTC timezone. Refer to [cron](https://en.wikipedia.org/wiki/Cron) for the expression format.\n\n\n\n\n\nAfter the scheduled backup is enabled, execute the following command to check if a CronJob object has been created:\n\n```bash\nkubectl get cronjob\n>\nNAME                                        SCHEDULE     SUSPEND   ACTIVE   LAST SCHEDULE   AGE\n96523399-mycluster-default-xtrabackup       0 18 * * *   False     0                  57m\n```\n\nYou can also execute the follo",
    "path": "docs/preview/user_docs/concepts/backup-and-restore/backup/scheduled-backup",
    "description": "  # Scheduled backup  KubeBlocks supports configuring scheduled backups for clusters.    Modify the backup field with kubectl as follows.  ```bash kubectl edit cluster -n default mycluster ```  Edit the cluster YAML.  ```yaml spec:   ...   backup:     # Whether to enable automatic backups     enable"
  },
  {
    "id": "docs_en_preview_user_docs_concepts_backup-and-restore_restore_pitr",
    "title": "PITR",
    "content": "\n\n# PITR\n\n## What is PITR (Point-in-Time Recovery)\n\nPITR (Point-in-Time Recovery) is a database backup and recovery technique commonly used in Relational Database Management Systems (RDBMS). It allows for the recovery of data changes to a specific point in time, restoring the database to a state prior to that point. In PITR, the database system regularly creates full backups and logs all transactions thereafter, including insert, update, and delete operations. During recovery, the system first restores the most recent full backup, and then applies the transaction logs recorded after the backup, bringing the database back to the desired state.\n\nKubeBlocks supports PITR for databases such as MySQL and PostgreSQL. This documentation takes PostgreSQL PITR as an example. Please refer to [PostgreSQL Backup and Restore](../../../../kubeblocks-for-postgresql/05-backup-restore/06-restore-with-pitr) for more details.\n\n## How to perform PITR?\n\n**Step 1. View the timestamps to which the cluster can be restored.**\n\n\n\n```bash\n# Get the backup time range for Continuous Backup\nkubectl get backup -l app.kubernetes.io/instance=pg-cluster -l dataprotection.kubeblocks.io/backup-type=Continuous -oyaml\n...\nstatus:\n  timeRange:\n  end: \"2024-05-07T10:47:14Z\"\n  start: \"2024-05-07T10:07:45Z\"\n```\n\nIt can be seen that the current backup time range is `2024-05-07T10:07:45Z ~2024-05-07T10:47:14Z`. Still, a full backup is required for data restoration, and this full backup must be completed within the time range of the log backups.\n\n\n\n\n```bash\nkbcli cluster describe pg-cluster\n>\n...\nData Protection:\nBACKUP-REPO   AUTO-BACKUP   BACKUP-SCHEDULE   BACKUP-METHOD   BACKUP-RETENTION   RECOVERABLE-TIME\nminio         Enabled       */5 * * * *       archive-wal     8d                 May 07,2024 15:29:46 UTC+0800 ~ May 07,2024 15:48:47 UTC+0800\n```\n\n`RECOVERABLE-TIME` represents the time range within which the cluster can be restored.\n\nIt can be seen that the current backup time range is `May 07,2024 15:2",
    "path": "docs/preview/user_docs/concepts/backup-and-restore/restore/pitr",
    "description": "  # PITR  ## What is PITR (Point-in-Time Recovery)  PITR (Point-in-Time Recovery) is a database backup and recovery technique commonly used in Relational Database Management Systems (RDBMS). It allows for the recovery of data changes to a specific point in time, restoring the database to a state pri"
  },
  {
    "id": "docs_en_preview_user_docs_concepts_backup-and-restore_restore_restore-data-from-backup-set",
    "title": "Restore data from backup set",
    "content": "\n\n# Restore data from backup set\n\nKubeBlocks supports restoring clusters from backups.  This documentation takes MySQL as an example. Please refer to [MySQL Backup and Restore](../../../../kubeblocks-for-mysql/05-backup-restore/05-restoring-from-full-backup) for more details.\n\n**Step 1. View backups.**\n\n\n\n```bash\nkubectl get backups\n```\n\n\n\n\nFor existing clusters, execute:\n\n```bash\nkbcli cluster list-backups mycluster\n```\n\nIf the cluster has been deleted, execute:\n\n```bash\nkbcli dataprotection list-backups\n```\n\n\n\n\n\n**Step 2. Restore clusters from a specific backup.**\n\n```bash\n# Restore new cluster\nkbcli cluster restore myrestore --backup mybackup\n>\nCluster myrestore created\n\n# View the status of the restored cluster\nkbcli cluster list myrestore\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     CREATED-TIME\nmyrestore   default     mysql                Delete               Running   May 26,2025 18:42 UTC+0800\n```",
    "path": "docs/preview/user_docs/concepts/backup-and-restore/restore/restore-data-from-backup-set",
    "description": "  # Restore data from backup set  KubeBlocks supports restoring clusters from backups.  This documentation takes MySQL as an example. Please refer to [MySQL Backup and Restore](../../../../kubeblocks-for-mysql/05-backup-restore/05-restoring-from-full-backup) for more details.  **Step 1. View backups"
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_backup-and-restore_backup_backup-repo",
    "title": "Configure BackupRepo",
    "content": "\n\n# Introduction\n\nBackupRepo is the storage repository for backup data. Currently, KubeBlocks supports configuring various object storage services as backup repositories, including OSS (Alibaba Cloud Object Storage Service), S3 (Amazon Simple Storage Service), COS (Tencent Cloud Object Storage), GCS (Google Cloud Storage), OBS (Huawei Cloud Object Storage), MinIO, and other S3-compatible services.\n\nYou can create multiple BackupRepos to suit different scenarios. For example, based on different businesses, the data of business A is stored in repository A, and the data of business B is stored in repository B. Or you can configure multiple repositories by region to realize geo-disaster recovery. But it is required to specify backup repositories when you create a backup. You can also create a default backup repository and KubeBlocks uses this default repository to store backup data if no specific repository is specified.\n\n## Before you start\n\nMake sure you have all the following prepared.\n\n* [Install kbcli](./../../../user_docs/installation/install-kbcli).\n* [Install kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl).\n* [Install Helm](https://helm.sh/docs/intro/install/).\n* [Install KubeBlocks](./../../../user_docs/installation/install-kubeblocks).\n\n## Install MinIO\n\nIf you don't have an object storage service from a cloud provider, you can deploy the open-source service MinIO in Kubernetes and use it to configure BackupRepo. If you are using an object storage service provided by a cloud provider, directly skip to [Configure BackupRepo](#configure-backuprepo).\n\n***Steps***\n\n1. Install MinIO in the `kb-system` namespace.\n\n   ```bash\n   helm install minio oci://registry-1.docker.io/bitnamicharts/minio --namespace kb-system --create-namespace --set \"extraEnvVars[0].name=MINIO_BROWSER_LOGIN_ANIMATION\" --set \"extraEnvVars[0].value=off\"\n   ```\n\n   Get the initial username and password:\n\n   ```bash\n   # Initial username\n   echo $(kubectl get secret --namespace kb-system",
    "path": "docs/release-0_9/user_docs/maintenance/backup-and-restore/backup/backup-repo",
    "description": "  # Introduction  BackupRepo is the storage repository for backup data. Currently, KubeBlocks supports configuring various object storage services as backup repositories, including OSS (Alibaba Cloud Object Storage Service), S3 (Amazon Simple Storage Service), COS (Tencent Cloud Object Storage), GCS"
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_backup-and-restore_backup_configure-backuppolicy",
    "title": "Configure BackupPolicy",
    "content": "\n\n# Configure BackupPolicy\n\n## Configure encryption key\n\nTo ensure that the restored cluster can access the data properly, KubeBlocks encrypts the cluster's credentials during the backup process and securely stores it in the Annotation of the Backup object. Therefore, to protect your data security, it is strongly recommended to carefully assign Get/List permissions for backup objects and specify an encryption key during the installation or upgrade of KubeBlocks. These measures will help ensure the proper protection of your data.\n\nKubeBlocks has integrated data encryption functionality for datasafed since v0.9.0. Currently, the supported encryption algorithms include `AES-128-CFB`, `AES-192-CFB`, and `AES-256-CFB`. This function allows backup data to be encrypted before being written to storage. The encryption key then will be used to encrypt connection passwords and also to back up data. You can reference existing keys or create different secret keys for database clusters according to actual needs.\n\n### Reference an existing key\n\nIf the secret already exists, you can choose to directly reference it without setting the `dataProtection.encryptionKey`. KubeBlocks provides a quick way to reference an existing key for encryption.\n\nAssuming there is a pre-defined secret named `dp-encryption-key` and a key `encryptionKey` inside it. For example, a secret created by this command.\n\n```bash\nkubectl create secret generic dp-encryption-key \\\n    --from-literal=encryptionKey='S!B\\*d$zDsb='\n```\n\nAnd then you can reference it when installing or upgrading KubeBlocks.\n\n```bash\nkbcli kubeblocks install \\\n    --set dataProtection.encryptionKeySecretKeyRef.name=\"dp-encryption-key\" \\\n    --set dataProtection.encryptionKeySecretKeyRef.key=\"encryptionKey\"\n# The above command is equivalent to:\n# kbcli kubeblocks install --set dataProtection.encryptionKey='S!B\\*d$zDsb='\n```\n\n### Create a new key\n\nIf you do not need to enable backup encryption by default, or if you need to use a separate `en",
    "path": "docs/release-0_9/user_docs/maintenance/backup-and-restore/backup/configure-backuppolicy",
    "description": "  # Configure BackupPolicy  ## Configure encryption key  To ensure that the restored cluster can access the data properly, KubeBlocks encrypts the cluster's credentials during the backup process and securely stores it in the Annotation of the Backup object. Therefore, to protect your data security, "
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_backup-and-restore_backup_on-demand-backup",
    "title": "On-demand backup",
    "content": "\n\n# On-demand backup\n\nKubeBlocks supports on-demand backups. You can customize your backup method by specifying `--method`. The instructions below take using a backup tool and volume snapshot as examples.\n\n## Backup tool\n\nThe following command uses the `xtrabackup` backup method to create a backup named `mybackup`.\n\n\n\n```bash\n# Create a backup\nkubectl apply -f - \nNAME       POLICY                              METHOD       REPO      STATUS      TOTAL-SIZE   DURATION   CREATION-TIME          COMPLETION-TIME        EXPIRATION-TIME\nmybackup   mysql-cluster-mysql-backup-policy   xtrabackup   my-repo   Completed   4426858      2m8s       2023-10-30T07:19:21Z   2023-10-30T07:21:28Z\n```\n\n:::note\n\nThe `dataprotection.kubeblocks.io/connection-password` in annotations uses the password of the original cluster.\n\n:::\n\n\n\n\n```bash\n# Create a backup\nkbcli cluster backup mysql-cluster --name mybackup --method xtrabackup\n>\nBackup mybackup created successfully, you can view the progress:\n        kbcli cluster list-backups --name=mybackup -n default\n        \n# View the backup\nkbcli cluster list-backups --name=mybackup -n default\n>\nNAME       NAMESPACE   SOURCE-CLUSTER   METHOD       STATUS      TOTAL-SIZE   DURATION   CREATE-TIME                  COMPLETION-TIME              EXPIRATION\nmybackup   default     mysql-cluster    xtrabackup   Completed   4426858      2m8s       Oct 30,2023 15:19 UTC+0800   Oct 30,2023 15:21 UTC+0800\n```\n\n\n\n\n\n## Volume snapshot backup\n\nTo create a backup using the snapshot, the `backupMethod` in the YAML configuration file or the `--method` field in the kbcli command should be set to `volume-snapshot`.\n\n\n\n```bash\n# Create a backup\nkubectl apply -f - \nNAME       POLICY                              METHOD            REPO      STATUS      TOTAL-SIZE   DURATION   CREATION-TIME          COMPLETION-TIME        EXPIRATION-TIME\nmybackup   mycluster-mysql-backup-policy       volume-snapshot   my-repo   Completed   4426858      2m8s       2023-10-30T07:19:21Z   2023-1",
    "path": "docs/release-0_9/user_docs/maintenance/backup-and-restore/backup/on-demand-backup",
    "description": "  # On-demand backup  KubeBlocks supports on-demand backups. You can customize your backup method by specifying `--method`. The instructions below take using a backup tool and volume snapshot as examples.  ## Backup tool  The following command uses the `xtrabackup` backup method to create a backup n"
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_backup-and-restore_backup_scheduled-backup",
    "title": "Scheduled backup",
    "content": "\n\n# Scheduled backup\n\nKubeBlocks supports configuring scheduled backups for clusters.\n\n\n\nModify the backup field with kubectl as follows.\n\n```bash\nkubectl edit cluster -n default mysql-cluster\n```\n\nEdit the cluster YAML.\n\n```yaml\nspec:\n  ...\n  backup:\n    # Whether to enable automatic backups\n    enabled: true\n    # UTC timezone, the example below stands for 2 A.M. every Monday\n    cronExpression: 0 18 * * *\n    # Use xtrabackup for backups. If your storage supports snapshot, you can change it to volume-snapshot\n    method: xtrabackup\n    # Whether to enable PITR\n    pitrEnabled: false\n    # Retention period for a backup set\n    retentionPeriod: 7d\n    # BackupRepo\n    repoName: my-repo\n```\n\nIn the above YAML file, you can set whether to enable automatic backups and PITR as needed, and also specify backup methods, repo names, retention periods, etc.\n\n\n\n\n```bash\nkbcli cluster update mysql-cluster --backup-enabled=true \\\n--backup-method=xtrabackup --backup-repo-name=my-repo \\\n--backup-retention-period=7d --backup-cron-expression=\"0 18 * * *\"\n```\n\n- `--backup-enabled` indicates whether to enable scheduled backups.\n- `--backup-method` specifies the backup method. You can use the `kbcli cluster describe-backup-policy mysql-cluster` command to view the supported backup methods.\n- `--backup-repo-name` specifies the name of the backupRepo.\n- `--backup-retention-period` specifies the retention period for backups, which is 7 days in the example.\n- `--backup-cron-expression` specifies the backup schedule using a cron expression in UTC timezone. Refer to [cron](https://en.wikipedia.org/wiki/Cron) for the expression format.\n\n\n\n\n\nAfter the scheduled backup is enabled, execute the following command to check if a CronJob object has been created:\n\n```bash\nkubectl get cronjob\n>\nNAME                                            SCHEDULE     SUSPEND   ACTIVE   LAST SCHEDULE   AGE\n96523399-mysql-cluster-default-xtrabackup       0 18 * * *   False     0                  57m\n```\n\nYou can al",
    "path": "docs/release-0_9/user_docs/maintenance/backup-and-restore/backup/scheduled-backup",
    "description": "  # Scheduled backup  KubeBlocks supports configuring scheduled backups for clusters.    Modify the backup field with kubectl as follows.  ```bash kubectl edit cluster -n default mysql-cluster ```  Edit the cluster YAML.  ```yaml spec:   ...   backup:     # Whether to enable automatic backups     en"
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_backup-and-restore_restore_pitr",
    "title": "PITR",
    "content": "\n\n# PITR\n\n## What is PITR (Point-in-Time Recovery)?\n\nPITR (Point-in-Time Recovery) is a database backup and recovery technique commonly used in Relational Database Management Systems (RDBMS). It allows for the recovery of data changes to a specific point in time, restoring the database to a state prior to that point. In PITR, the database system regularly creates full backups and logs all transactions thereafter, including insert, update, and delete operations. During recovery, the system first restores the most recent full backup, and then applies the transaction logs recorded after the backup, bringing the database back to the desired state.\n\nKubeBlocks supports PITR for databases such as MySQL and PostgreSQL. This documentation takes PostgreSQL PITR as an example.\n\n## How to perform PITR?\n\n1. View the timestamps to which the cluster can be restored.\n\n\n\n     ```bash\n     # 1. Get all backup objects for the current cluster\n     kubectl get backup -l app.kubernetes.io/instance=pg-cluster\n    \n     # 2. Get the backup time range for Continuous Backup\n     kubectl get backup -l app.kubernetes.io/instance=pg-cluster -l dataprotection.kubeblocks.io/backup-type=Continuous -oyaml\n     ...\n     status:\n         timeRange:\n         end: \"2024-05-07T10:47:14Z\"\n         start: \"2024-05-07T10:07:45Z\"\n     ```\n\n     It can be seen that the current backup time range is `2024-05-07T10:07:45Z ~2024-05-07T10:47:14Z`. Still, a full backup is required for data restoration, and this full backup must be completed within the time range of the log backups.\n\n     \n\n\n     ```bash\n     kbcli cluster describe pg-cluster\n     >\n     ...\n     Data Protection:\n     BACKUP-REPO   AUTO-BACKUP   BACKUP-SCHEDULE   BACKUP-METHOD   BACKUP-RETENTION   RECOVERABLE-TIME                                                \n     minio         Enabled       */5 * * * *       archive-wal     8d                 May 07,2024 15:29:46 UTC+0800 ~ May 07,2024 15:48:47 UTC+0800\n     ```\n\n     `RECOVERABLE-TIME` represe",
    "path": "docs/release-0_9/user_docs/maintenance/backup-and-restore/restore/pitr",
    "description": "  # PITR  ## What is PITR (Point-in-Time Recovery)?  PITR (Point-in-Time Recovery) is a database backup and recovery technique commonly used in Relational Database Management Systems (RDBMS). It allows for the recovery of data changes to a specific point in time, restoring the database to a state pr"
  },
  {
    "id": "docs_en_release-0_9_user_docs_maintenance_backup-and-restore_restore_restore-data-from-backup-set",
    "title": "Restore data from backup set",
    "content": "\n\n# Restore data from backup set\n\nKubeBlocks supports restoring clusters from backups with the following instructions.\n\n1. View backups.\n\n\n\n     ```bash\n     kubectl get backups\n     ```\n\n     \n\n\n     For existing clusters, execute:\n\n     ```bash\n     kbcli cluster list-backups mysql-cluster\n     ```\n\n     If the cluster has been deleted, execute:\n\n     ```bash\n     kbcli dataprotection list-backups\n     ```\n\n     \n\n     \n\n2. Restore clusters from a specific backup.\n\n\n\n     You can set the `connectionPassword.annotations` of the restored cluster as that of the original cluster. The password of the original cluster can be accessed by viewing the annotation of `dataprotection.kubeblocks.io/connection-password` in the backup YAML file.\n\n     ```bash\n     kubectl apply -f - \n\n\n     ```bash\n     # Restore new cluster\n     kbcli cluster restore myrestore --backup mybackup\n     >\n     Cluster myrestore created\n\n     # View the status of the restored cluster\n     kbcli cluster list myrestore\n     >\n     NAME        NAMESPACE   CLUSTER-DEFINITION   VERSION           TERMINATION-POLICY   STATUS    CREATED-TIME\n     myrestore   default     apecloud-mysql       ac-mysql-8.0.30   Delete               Running   Oct 30,2023 16:26 UTC+0800\n     ```\n\n     \n\n     \n\n3. Connect to the restored cluster for verification.  Once the cluster status is `Running`, [connect to the cluster](./../../../kubeblocks-for-apecloud-mysql/cluster-management/create-and-connect-an-apecloud-mysql-cluster.md#connect-to-a-cluster) for verification.\n",
    "path": "docs/release-0_9/user_docs/maintenance/backup-and-restore/restore/restore-data-from-backup-set",
    "description": "  # Restore data from backup set  KubeBlocks supports restoring clusters from backups with the following instructions.  1. View backups.         ```bash      kubectl get backups      ```               For existing clusters, execute:       ```bash      kbcli cluster list-backups mysql-cluster      ``"
  },
  {
    "id": "docs_en_release-1_0_user_docs_concepts_backup-and-restore_restore_pitr",
    "title": "PITR",
    "content": "\n\n# PITR\n\n## What is PITR (Point-in-Time Recovery)\n\nPITR (Point-in-Time Recovery) is a database backup and recovery technique commonly used in Relational Database Management Systems (RDBMS). It allows for the recovery of data changes to a specific point in time, restoring the database to a state prior to that point. In PITR, the database system regularly creates full backups and logs all transactions thereafter, including insert, update, and delete operations. During recovery, the system first restores the most recent full backup, and then applies the transaction logs recorded after the backup, bringing the database back to the desired state.\n\nKubeBlocks supports PITR for databases such as MySQL and PostgreSQL. This documentation takes PostgreSQL PITR as an example. Please refer to [PostgreSQL Backup and Restore](../../../../kubeblocks-for-postgresql/05-backup-restore/06-restore-with-pitr) for more details.\n\n## How to perform PITR?\n\n**Step 1. View the timestamps to which the cluster can be restored.**\n\n\n\n```bash\n# Get the backup time range for Continuous Backup\nkubectl get backup -l app.kubernetes.io/instance=pg-cluster -l dataprotection.kubeblocks.io/backup-type=Continuous -oyaml\n...\nstatus:\n  timeRange:\n  end: \"2024-05-07T10:47:14Z\"\n  start: \"2024-05-07T10:07:45Z\"\n```\n\nIt can be seen that the current backup time range is `2024-05-07T10:07:45Z ~2024-05-07T10:47:14Z`. Still, a full backup is required for data restoration, and this full backup must be completed within the time range of the log backups.\n\n\n\n\n```bash\nkbcli cluster describe pg-cluster\n>\n...\nData Protection:\nBACKUP-REPO   AUTO-BACKUP   BACKUP-SCHEDULE   BACKUP-METHOD   BACKUP-RETENTION   RECOVERABLE-TIME\nminio         Enabled       */5 * * * *       archive-wal     8d                 May 07,2024 15:29:46 UTC+0800 ~ May 07,2024 15:48:47 UTC+0800\n```\n\n`RECOVERABLE-TIME` represents the time range within which the cluster can be restored.\n\nIt can be seen that the current backup time range is `May 07,2024 15:2",
    "path": "docs/release-1_0/user_docs/concepts/backup-and-restore/restore/pitr",
    "description": "  # PITR  ## What is PITR (Point-in-Time Recovery)  PITR (Point-in-Time Recovery) is a database backup and recovery technique commonly used in Relational Database Management Systems (RDBMS). It allows for the recovery of data changes to a specific point in time, restoring the database to a state pri"
  },
  {
    "id": "docs_en_release-1_0_user_docs_concepts_backup-and-restore_restore_restore-data-from-backup-set",
    "title": "Restore data from backup set",
    "content": "\n\n# Restore data from backup set\n\nKubeBlocks supports restoring clusters from backups.  This documentation takes MySQL as an example. Please refer to [MySQL Backup and Restore](../../../../kubeblocks-for-mysql/05-backup-restore/05-restoring-from-full-backup) for more details.\n\n**Step 1. View backups.**\n\n\n\n```bash\nkubectl get backups\n```\n\n\n\n\nFor existing clusters, execute:\n\n```bash\nkbcli cluster list-backups mycluster\n```\n\nIf the cluster has been deleted, execute:\n\n```bash\nkbcli dataprotection list-backups\n```\n\n\n\n\n\n**Step 2. Restore clusters from a specific backup.**\n\n```bash\n# Restore new cluster\nkbcli cluster restore myrestore --backup mybackup\n>\nCluster myrestore created\n\n# View the status of the restored cluster\nkbcli cluster list myrestore\n>\nNAME        NAMESPACE   CLUSTER-DEFINITION   TERMINATION-POLICY   STATUS     CREATED-TIME\nmyrestore   default     mysql                Delete               Running   May 26,2025 18:42 UTC+0800\n```",
    "path": "docs/release-1_0/user_docs/concepts/backup-and-restore/restore/restore-data-from-backup-set",
    "description": "  # Restore data from backup set  KubeBlocks supports restoring clusters from backups.  This documentation takes MySQL as an example. Please refer to [MySQL Backup and Restore](../../../../kubeblocks-for-mysql/05-backup-restore/05-restoring-from-full-backup) for more details.  **Step 1. View backups"
  },
  {
    "id": "docs_en_release-1_0_user_docs_concepts_backup-and-restore_backup_backup-repo",
    "title": "Configure BackupRepo",
    "content": "\n\n# Introduction\n\nBackupRepo is the storage repository for backup data. Currently, KubeBlocks supports configuring various object storage services as backup repositories, including OSS (Alibaba Cloud Object Storage Service), S3 (Amazon Simple Storage Service), COS (Tencent Cloud Object Storage), GCS (Google Cloud Storage), OBS (Huawei Cloud Object Storage), MinIO, and other S3-compatible services.\n\nYou can create multiple BackupRepos to suit different scenarios. For example, based on different businesses, the data of business A is stored in repository A, and the data of business B is stored in repository B. Or you can configure multiple repositories by region to realize geo-disaster recovery. But it is required to specify backup repositories when you create a backup. You can also create a default backup repository and KubeBlocks uses this default repository to store backup data if no specific repository is specified.\n\n## Before you start\n\nMake sure you have all the following prepared.\n\n* [Install kbcli](./../../../references/install-kbcli).\n* [Install kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl).\n* [Install Helm](https://helm.sh/docs/intro/install/).\n* [Install KubeBlocks](./../../../overview/install-kubeblocks).\n\n## Configure BackupRepo\n\nWith object storage services prepared, it's time to configure BackupRepo. KubeBlocks provides two ways for the configuration:\n\n* Automatic BackupRepo configuration during KubeBlocks installation;\n* Manual BackupRepo configuration for on-demand scenarios.\n\nIf you don't have any object storage at hand, please refer to [Install MinIO](#install-minio)\n\n### Access BackupRepo\n\nThere are two methods to access remote object storage:\n\n| Method | Description | Requirements | Security Consideration |\n|--------|-------------|--------------|------------------------|\n| Tool | Uses command-line tools to directly access remote storage | No additional driver required | Synchronizes credentials as secrets across namespaces |\n| Mount | M",
    "path": "docs/release-1_0/user_docs/concepts/backup-and-restore/backup/backup-repo",
    "description": "  # Introduction  BackupRepo is the storage repository for backup data. Currently, KubeBlocks supports configuring various object storage services as backup repositories, including OSS (Alibaba Cloud Object Storage Service), S3 (Amazon Simple Storage Service), COS (Tencent Cloud Object Storage), GCS"
  },
  {
    "id": "docs_en_release-1_0_user_docs_concepts_backup-and-restore_backup_configure-backuppolicy",
    "title": "Configure BackupPolicy",
    "content": "\n\n# Configure BackupPolicy\n\n## Configure encryption key\n\nTo ensure that the restored cluster can access the data properly, KubeBlocks encrypts the cluster's credentials during the backup process and securely stores it in the Annotation of the Backup object. Therefore, to protect your data security, it is strongly recommended to carefully assign Get/List permissions for backup objects and specify an encryption key during the installation or upgrade of KubeBlocks. These measures will help ensure the proper protection of your data.\n\nKubeBlocks has integrated data encryption functionality for datasafed since v0.9.0. Currently, the supported encryption algorithms include `AES-128-CFB`, `AES-192-CFB`, and `AES-256-CFB`. This function allows backup data to be encrypted before being written to storage. The encryption key then will be used to encrypt connection passwords and also to back up data. You can reference existing keys or create different secret keys for database clusters according to actual needs.\n\n### Reference an existing key\n\nIf the secret already exists, you can choose to directly reference it without setting the `dataProtection.encryptionKey`. KubeBlocks provides a quick way to reference an existing key for encryption.\n\nAssuming there is a pre-defined secret named `dp-encryption-key` and a key `encryptionKey` inside it. For example, a secret created by this command.\n\n```bash\nkubectl create secret generic dp-encryption-key \\\n    --from-literal=encryptionKey='S!B\\*d$zDsb='\n```\n\nAnd then you can reference it when installing or upgrading KubeBlocks.\n\n```bash\nkbcli kubeblocks install \\\n    --set dataProtection.encryptionKeySecretKeyRef.name=\"dp-encryption-key\" \\\n    --set dataProtection.encryptionKeySecretKeyRef.key=\"encryptionKey\"\n# The above command is equivalent to:\n# kbcli kubeblocks install --set dataProtection.encryptionKey='S!B\\*d$zDsb='\n```\n\n### Create a new key\n\nIf you do not need to enable backup encryption by default, or if you need to use a separate `en",
    "path": "docs/release-1_0/user_docs/concepts/backup-and-restore/backup/configure-backuppolicy",
    "description": "  # Configure BackupPolicy  ## Configure encryption key  To ensure that the restored cluster can access the data properly, KubeBlocks encrypts the cluster's credentials during the backup process and securely stores it in the Annotation of the Backup object. Therefore, to protect your data security, "
  },
  {
    "id": "docs_en_release-1_0_user_docs_concepts_backup-and-restore_backup_on-demand-backup",
    "title": "On-demand backup",
    "content": "\n\n# On-demand backup\n\nKubeBlocks supports on-demand backups. You can customize your backup method by specifying `--method`. The instructions below take using a backup tool and volume snapshot as examples.\n\n## Backup tool\n\nThe following command uses the `xtrabackup` backup method to create a backup named `mybackup`.\n\n\n\nTo Create a backup:\n```bash\nkubectl apply -f - \nNAME       POLICY                          METHOD       REPO     STATUS      TOTAL-SIZE   DURATION   DELETION-POLICY   CREATION-TIME          COMPLETION-TIME        EXPIRATION-TIME\nmybackup   mycluster-mysql-backup-policy   xtrabackup   kb-oss   Completed   1632402      10s        Delete            2025-05-26T10:14:33Z   2025-05-26T10:14:42Z\n```\n\n\n\nCreate a backup\n```bash\nkbcli cluster backup mycluster --name mybackup --method xtrabackup\n>\nBackup mybackup created successfully, you can view the progress:\n\tkbcli cluster list-backups --names=mybackup -n default\n```\n\nView the backup\n```bash\nkbcli cluster list-backups --names mybackup\n>\nNAME       NAMESPACE   SOURCE-CLUSTER   METHOD       STATUS      TOTAL-SIZE   DURATION   DELETION-POLICY   CREATE-TIME                  COMPLETION-TIME              EXPIRATION\nmybackup   default     mycluster        xtrabackup   Completed   1632402      10s        Delete            May 26,2025 18:14 UTC+0800   May 26,2025 18:14 UTC+0800\n```\n\n\n\n\n\n## Volume snapshot backup\n\n:::note\n**Prerequisites**\nVolume snapshot backups require:\n- StorageClass must support volume snapshots\n\nPlease check the list of CS Drivers and their features at: https://kubernetes-csi.github.io/docs/drivers.html\n\n:::\n\nTo create a backup using the snapshot, the `backupMethod` in the YAML configuration file or the `--method` field in the kbcli command should be set to `volume-snapshot`.\n\n\n\n```bash\n# Create a backup\nkubectl apply -f - \nNAME                 POLICY                           METHOD            REPO   STATUS    TOTAL-SIZE   DURATION   DELETION-POLICY   CREATION-TIME          COMPLETION-TIME   EXPIR",
    "path": "docs/release-1_0/user_docs/concepts/backup-and-restore/backup/on-demand-backup",
    "description": "  # On-demand backup  KubeBlocks supports on-demand backups. You can customize your backup method by specifying `--method`. The instructions below take using a backup tool and volume snapshot as examples.  ## Backup tool  The following command uses the `xtrabackup` backup method to create a backup n"
  },
  {
    "id": "docs_en_release-1_0_user_docs_concepts_backup-and-restore_backup_scheduled-backup",
    "title": "Scheduled backup",
    "content": "\n\n# Scheduled backup\n\nKubeBlocks supports configuring scheduled backups for clusters.\n\n\n\nModify the backup field with kubectl as follows.\n\n```bash\nkubectl edit cluster -n default mycluster\n```\n\nEdit the cluster YAML.\n\n```yaml\nspec:\n  ...\n  backup:\n    # Whether to enable automatic backups\n    enabled: true\n    # UTC timezone, the example below stands for 2 A.M. every Monday\n    cronExpression: 0 18 * * *\n    # Use xtrabackup for backups. If your storage supports snapshot, you can change it to volume-snapshot\n    method: xtrabackup\n    # Whether to enable PITR\n    pitrEnabled: false\n    # Retention period for a backup set\n    retentionPeriod: 7d\n    # BackupRepo\n    repoName: my-repo\n```\n\nIn the above YAML file, you can set whether to enable automatic backups and PITR as needed, and also specify backup methods, repo names, retention periods, etc.\n\n\n\n\n```bash\nkbcli cluster update mycluster --backup-enabled=true \\\n--backup-method=xtrabackup --backup-repo-name=my-repo \\\n--backup-retention-period=7d --backup-cron-expression=\"0 18 * * *\"\n```\n\n- `--backup-enabled` indicates whether to enable scheduled backups.\n- `--backup-method` specifies the backup method. You can use the `kbcli cluster describe-backup-policy mycluster` command to view the supported backup methods.\n- `--backup-repo-name` specifies the name of the BackupRepo.\n- `--backup-retention-period` specifies the retention period for backups, which is 7 days in the example.\n- `--backup-cron-expression` specifies the backup schedule using a cron expression in UTC timezone. Refer to [cron](https://en.wikipedia.org/wiki/Cron) for the expression format.\n\n\n\n\n\nAfter the scheduled backup is enabled, execute the following command to check if a CronJob object has been created:\n\n```bash\nkubectl get cronjob\n>\nNAME                                        SCHEDULE     SUSPEND   ACTIVE   LAST SCHEDULE   AGE\n96523399-mycluster-default-xtrabackup       0 18 * * *   False     0                  57m\n```\n\nYou can also execute the follo",
    "path": "docs/release-1_0/user_docs/concepts/backup-and-restore/backup/scheduled-backup",
    "description": "  # Scheduled backup  KubeBlocks supports configuring scheduled backups for clusters.    Modify the backup field with kubectl as follows.  ```bash kubectl edit cluster -n default mycluster ```  Edit the cluster YAML.  ```yaml spec:   ...   backup:     # Whether to enable automatic backups     enable"
  },
  {
    "id": "blogs_en_a-testing-report-for-optimizing-PG-performance-on-kubeblocks",
    "title": "A testing report for optimizing PG performance on Kubernetes",
    "content": "\n# A testing report for optimizing PG performance on Kubernetes\n\n## Introduction\n\nNowadays, an increasing number of applications are being implemented within containers on Kubernetes. This trend has become so prominent that some have likened Kubernetes to the Linux of the cloud in terms of its ubiquity and influence. However, while this growth is evident at the application layer, containerization hasn't gained as much momentum in the realm of data management. Databases, as a stateful workload, are said to be the last thing you would run on Kubernetes. This is not surprising, given that containerized workloads inherently need to be robust in the face of restarts, scaling, virtualization, and various other constraints. \n\nHowever, attention is shifting to the data layer, as developers look to treat data infrastructure like application stacks. They aim to use the same tools for databases and applications, seeking the same benefits, such as rapid deployment and consistency across environments. In this blog, we run testing for a self-hosted PostgreSQL solution (we use KubeBlocks PG ) and a self-hosts PostgreSQL on ECS (ECS hereafter referred to as ECS PG), and explore how to optimize the database performance so the performance and stability in production environment can be same as or better than that of fully-managed database.\n\n## Environment preparation\n| | Version | CPU | Memory | Disk | Network | Instance Class Type | Replication Protocol |\n|--|-------|--------|-----|------|--------|-----|------|\n| ECS PG|12.14|16C|64G|ESSD PL1 500G|SLB|Dedicated|Asynchronous replication |\n|KubeBlocks PG|12.14|16C|64G|ESSD PL1 300G|SLB|Dedicated|Asynchronous replication |\n\n1. Purchase K8s clusters on ACK and deploy KubeBlocks (refer to [this tutorial](https://kubeblocks.io/docs/preview/user_docs/installation/install-with-kbcli/install-kbcli).) Apply Terway network mode, where Pod IP is VPC IP. This ensures network connectivity within a VPC, simplifies network management, and reduces th",
    "path": "blogs/a-testing-report-for-optimizing-PG-performance-on-kubeblocks",
    "description": " # A testing report for optimizing PG performance on Kubernetes  ## Introduction  Nowadays, an increasing number of applications are being implemented within containers on Kubernetes. This trend has become so prominent that some have likened Kubernetes to the Linux of the cloud in terms of its ubiqu"
  },
  {
    "id": "blogs_en_announcing-kubeblocks-v0-7-0",
    "title": "Announcing KubeBlocks v0.7.0",
    "content": "\n# Announcing KubeBlocks v0.7.0\n\nWe are excited to announce the official release of KubeBlocks v0.7.0!\n\nIn this release, KubeBlocks has already supported 31 open-source database engines, including new add-ons such as MariaDB, Elasticsearch, Pulsar, and Pika. This offers Kubernetes users a wider range of choices while maintaining the same user experience.\n\n## Highlights\n\n### External components\n\nSome database clusters rely on metadata storage for distributed coordination and dynamic configuration. However, as the number of database clusters increases, the metadata storage itself can consume a significant amount of resources. Examples of such components include Zookeeper in Pulsar. To reduce overhead, users can reference the same external component in multiple database clusters now.\n\n### Backup API\n\nSome lifecycle management functions of the database cluster rely on the backup and restore functionality, which depends on object storage. However, if object storage is missing, certain lifecycle management functions of KubeBlocks may not work properly. For example, creating new replicas or recovering data to another node may be affected.\n\nTo address this issue, we plan to decouple the cluster's lifecycle management functions from the backup and restore functionality. The first step is to separate the API. With the new backup API, backup and restore actions are abstracted, allowing users to define their own backup methods. Additionally, the API supports GCS, OBS, and COS object storage now.\n\n### Pika v3.5\n\nPika, an open-source NoSQL database developed by Qihoo, supports the Redis protocol and offers a cost advantage for handling data volumes exceeding 100 GB. The transition from Redis to Pika is smooth, as Pika preserves the same operational and usage patterns, ensuring minimal disruption to existing workflows. As of now, KubeBlocks supports the deployment of the Pika v3.5 sharding cluster.\n\n## Overview of Integrated Engines\n\nThe table below provides an overview of the int",
    "path": "blogs/announcing-kubeblocks-v0-7-0",
    "description": " # Announcing KubeBlocks v0.7.0  We are excited to announce the official release of KubeBlocks v0.7.0!  In this release, KubeBlocks has already supported 31 open-source database engines, including new add-ons such as MariaDB, Elasticsearch, Pulsar, and Pika. This offers Kubernetes users a wider rang"
  },
  {
    "id": "blogs_en_announcing-kubeblocks-v0-8-0",
    "title": "Announcing KubeBlocks v0.8.0",
    "content": "\n# Announcing KubeBlocks v0.8.0\n\nExciting news! KubeBlocks v0.8.0 is officially released! 🚀 🎉 🎈\n\nKubeBlocks v0.8.0 takes a big step forward in user experience by introducing component-level APIs, which makes standardized building blocks even smaller and more convenient to be reused.\n\nFor example, popular meta-database components like ETCD and ZK, which are widely used in various database clusters, now can be directly referenced after component modularization.\n\nWe also made Vitess Proxy a standard component, so that developers do not need to repeat their work (heavy lifting) when defining the read-write separation topology of MySQL or PostgreSQL engines in various distributions.\n\nFurther, the Add-on mechanism has also been significantly improved. The helm chart of the database engine has been split from the KubeBlocks repo. From then on, changes in the database engine or version have been unbound from the KubeBlocks release.\n\n## Highlights\n\n### Independent Component API\n\nWhen integrating the new database engine, we found deficiencies in the abstract design of KubeBlocks. The v0.8.0 splits Component from Cluster definition to better support database types with multiple components. It supports variable references between Components, including ConfigMap, Secret, Service, ServiceReference and other variable reference types, which can better connect the relationships between components and lay the foundation for building clusters with different topologies.\n\n### Removes the addon helm chart from KubeBlocks repo\n\nIn previous versions, Helm charts for database engines were located in the \"deploy\" directory, tightly coupled with the KubeBlocks Operator. However, this caused two problems: first, upgrading KubeBlocks would trigger the upgrade of the database engine; second, upgrading the database engine would overwrite existing CD/CV, restarting all the clusters.\n\nTo address these problems, KubeBlocks v0.8.0 has placed the database engines into a standalone repository called",
    "path": "blogs/announcing-kubeblocks-v0-8-0",
    "description": " # Announcing KubeBlocks v0.8.0  Exciting news! KubeBlocks v0.8.0 is officially released! 🚀 🎉 🎈  KubeBlocks v0.8.0 takes a big step forward in user experience by introducing component-level APIs, which makes standardized building blocks even smaller and more convenient to be reused.  For example,"
  },
  {
    "id": "blogs_en_announcing-kubeblocks-v0-9-0",
    "title": "Announcing KubeBlocks v0.9.0",
    "content": "\n# Announcing KubeBlocks v0.9.0\n\nWe are thrilled to announce the release of KubeBlocks v0.9.0, which brings us one step closer to the highly anticipated v1.0 release. This version introduces several significant improvements and new features that enhance the overall functionality and user experience of the KubeBlocks platform.\n\n## API Highlights\n\n- In KubeBlocks v0.9, with the introduction of topology support in KubeBlocks, the cluster building experience has become much more flexible and intuitive, akin to assembling a cluster using building blocks. The ClusterDefinition API has added the `topologies` field, allowing developers to provide various deployment patterns with different topologies. Database users can choose a topology when creating a Cluster through the `topology` field. For instance, the Redis Addon offers three topologies: Standalone, Replication, and Proxy. The Standalone topology only includes one Component - RedisServer, the Replication topology includes both RedisServer and Sentinel Components, and the Proxy topology adds a third Component, such as Twemproxy.\n- KubeBlocks now supports managing horizontal scaling (Reshard) of distributed databases. You can represent a horizontal shard with a Component, and scale up or down this horizontal shard by adding or removing Components. This scaling capability will also be used in the distributed deployment of Redis and Pika.\n- KubeBlocks now uses InstanceSet instead of StatefulSet to manage Pods. InstanceSet supports taking a specified Pod offline and Pod in-place update, and also the primary and secondary databases can adopt different Pod specs in a database Replication architecture (StatefulSet doesn't support these features).\n- Developers can add more custom event handlers for Components! The ComponentDefinition API, introduced in v0.8, includes the `lifeCycleActions` field, allowing you to define various custom event handlers. Building on this, KubeBlocks v0.9 provides more handlers for custom addon impl",
    "path": "blogs/announcing-kubeblocks-v0-9-0",
    "description": " # Announcing KubeBlocks v0.9.0  We are thrilled to announce the release of KubeBlocks v0.9.0, which brings us one step closer to the highly anticipated v1.0 release. This version introduces several significant improvements and new features that enhance the overall functionality and user experience "
  },
  {
    "id": "blogs_en_announcing-kubeblocks-v0-9-1",
    "title": "Announcing KubeBlocks v0.9.1",
    "content": "\n# Announcing KubeBlocks v0.9.1\n\nWe're thrilled to announce that KubeBlocks v0.9.1 now is released!\n\nIn this release, KubeBlocks further optimizes its APIs and Addons to bring you new features and better user experience. This release includes new features like stopping/starting a cluster by Cluster API, instance rebuild capability in OpsRequest, PITR and key-based recovery for Redis. We also fixed some bugs and introduced several improvements to enhance the overall functionality.\n\nRead the full release note and [upgrade to KubeBlocks v0.9.1](https://kubeblocks.io/docs/preview/user_docs/upgrade/upgrade-with-kbcli/upgrade-kubeblocks-to-0.9.1) to explore more features!\n\n## Highlights\n\n### KubeBlocks\n\n- Supports cluster stop & start operations via Cluster API\n\n   This feature provides a new option to meet different needs in various scenarios.\n\n- Enhanced instance rebuild capability in OpsRequest\n\n   Combined with KubeBlocks’ [InstanceSet](https://kubeblocks.io/blog/instanceset-introduction), this feature greatly improves the system’s recovery capability in failure scenarios.\n\n### Addons\n\n- Redis\n  \n   Supports PITR (Point-in-Time Recovery) and key-based recovery.\n\n- ZooKeeper\n\n   Supports backup.\n\n- New versions\n\n   MySQL and PostgreSQL Addons support more versions. For the latest versions of Addons, refer to the [Addon List](https://github.com/apecloud/kubeblocks-addons?tab=readme-ov-file#supported-add-ons).\n\n## What's Changed\n\n### New Features\n\n#### KubeBlocks\n\n- OpsDefinition and BackupPolicyTemplate support cmpdName prefix and regex matching [#8174](https://github.com/apecloud/kubeblocks/pull/8174)\n\n   OpsDefinition and BackupPolicyTemplate now support component name prefixes and regular expression matching, offering greater flexibility.\n\n- High Availability (HA) records [#8089](https://github.com/apecloud/kubeblocks/pull/8089)\n\n   KubeBlocks supports HA records, enhancing fault tolerance and system reliability.\n\n- Supports cluster start and stop operations via Clus",
    "path": "blogs/announcing-kubeblocks-v0-9-1",
    "description": " # Announcing KubeBlocks v0.9.1  We're thrilled to announce that KubeBlocks v0.9.1 now is released!  In this release, KubeBlocks further optimizes its APIs and Addons to bring you new features and better user experience. This release includes new features like stopping/starting a cluster by Cluster "
  },
  {
    "id": "blogs_en_announcing-kubeblocks-v0-9-2",
    "title": "Announcing KubeBlocks v0.9.2",
    "content": "\n# Announcing KubeBlocks v0.9.2\n\nWe are delighted to announce the release of KubeBlocks v0.9.2. This release includes several new features, bug fixes, and various improvements.\n\nRead the full release note and [upgrade to KubeBlocks v0.9.2](https://kubeblocks.io/docs/release-0.9/user_docs/upgrade/upgrade-with-kbcli/upgrade-kubeblocks-to-0.9.1) to explore more features! The upgrade process for v0.9.2 is identical to that of v0.9.1. Simply follow the v0.9.1 tutorial, updating the version number as needed to complete the upgrade to v0.9.2.\n\n## KubeBlocks\n\n- Added support for rolling updates of container images, enabling seamless updates with minimal downtime. ([#8389](https://github.com/apecloud/kubeblocks/pull/8389))  \n- Introduced component-level stop/start capabilities, allowing fine-grained control over cluster components. ([#8480](https://github.com/apecloud/kubeblocks/pull/8480))  \n- Enhanced  Host Network support for shardings. ([#8517](https://github.com/apecloud/kubeblocks/pull/8517), [#8502](https://github.com/apecloud/kubeblocks/pull/8502))  \n- Improved horizontal scaling OpeRequest for shardings. ([#8530](https://github.com/apecloud/kubeblocks/pull/8530))  \n- Added support for recreate pod update policies to enhance update strategies. ([#8466](https://github.com/apecloud/kubeblocks/pull/8466))  \n- Kubeblocks Installation improvements: Support for defining extra annotations and environment variables. ([#8454](https://github.com/apecloud/kubeblocks/pull/8454))\n\n## Addons\n\n### MySQL\n\n- Added Jemalloc support for improved memory management. ([#1158](https://github.com/apecloud/kubeblocks-addons/pull/1158))\n\n### Redis\n\n- Added NodePort announce mode support for Redis Sentinel. ([#1227](https://github.com/apecloud/kubeblocks-addons/pull/1227))\n- Introduced support for fixed pod IPs, custom master names, and full FQDN domains.([#1222](https://github.com/apecloud/kubeblocks-addons/pull/1222))  \n- Updated user ACL backup frequency for PITR backups. ([#1180](https://g",
    "path": "blogs/announcing-kubeblocks-v0-9-2",
    "description": " # Announcing KubeBlocks v0.9.2  We are delighted to announce the release of KubeBlocks v0.9.2. This release includes several new features, bug fixes, and various improvements.  Read the full release note and [upgrade to KubeBlocks v0.9.2](https://kubeblocks.io/docs/release-0.9/user_docs/upgrade/upg"
  },
  {
    "id": "blogs_en_community-monthly-report-for-april-2024",
    "title": "Community Monthly Report for April 2024",
    "content": "\n# Community Monthly Report for April 2024\n\n## Overview\n\nIn April, KubeBlocks continued the development of v0.9.0. The new version is expected to be released in May, so stay tuned 🌟.\n\nOver the past month, the community has participated in six offline events, merged 157 PRs, and resolved 133 issues, with contributions from 27 developers. What's more, KubeBlocks and WeScale have announced their participation in OSPP 2024 (Open Source Promotion Plan) and the projects are now open for student registration. For more details, refer to:\n\n- [KubeBlocks Community Homepage](https://summer-ospp.ac.cn/org/orgdetail/833ca537-91a2-44a8-9965-5eee8f34aceb?lang=en)\n- [WeScale Community Homepage](https://summer-ospp.ac.cn/org/orgdetail/5d8efb0a-7f0d-4705-b253-00bb162ef507?lang=en)\n\nSo far, KubeBlocks has grabbed 1.7k stars. Thanks for all the support.\n\n## Highlights\n\n- Supported saving backup CRs to a backup repo, allowing users to manually restore the cluster using the saved files (#7002).\n- Supported ServiceRef to reference new API objects, decoupling it from ConnCredential (#7006).\n- RSM supported in-place updates of Pods by adding the ability to retrieve Kubernetes versions, updating the Pod template revision generation algorithm, and supporting the IgnorePodVerticalScaling switch (#7000).\n- Supported cluster RuntimeClassName configuration (#7001).\n- Supported specific instance scale-down, useful for scenarios such as node failures, data corruption, or instance unavailability (#6958).\n- Supported overridable component services in the cluster API (#6934).\n\n## Bug Fixes\n\n- Fixed the issue where the cluster was always in the \"Deleting\" state after upgrading from v0.8.2 to v0.9 (#6985).\n- Fixed the issue where the cluster was always in the \"Updating\" state after vscale (#6971).\n- Fixed the issue where stop/hscale OpsRequests were always running in v0.9 (#6972).\n\n## New Contributors\n\n👏 Let's welcome\n\n💙 @Chiwency 💙!\n\nNice to have you here in the KubeBlocks family. Chiwency managed ",
    "path": "blogs/community-monthly-report-for-april-2024",
    "description": " # Community Monthly Report for April 2024  ## Overview  In April, KubeBlocks continued the development of v0.9.0. The new version is expected to be released in May, so stay tuned 🌟.  Over the past month, the community has participated in six offline events, merged 157 PRs, and resolved 133 issues,"
  },
  {
    "id": "blogs_en_community-monthly-report-for-february-2024",
    "title": "Community Monthly Report for February 2024",
    "content": "\n# Community Monthly Report for February 2024\n\n## Overview\n\nIn February, KubeBlocks continued the development of v0.8.2 and v0.9.0 and mainly focused on sharding API, which will be released in v0.8.2.\n\nIn the past month, the community has merged 76 PRs and resolved 57 issues. A total of 21 community members participated in these contributions, including 1 new contributor.\n\n## Highlights\n\n- Supported ShardingSpec API to define database clusters with sharding topology (to be released in v0.8.2).\n- Supported creating and deleting official Redis Clusters based on KubeBlocks Sharding API (to be released in v0.8.2).\n- Supported Camellia Redis Proxy for one-click deployment, configuration management, horizontal scaling, restarts, and other operations (to be released in v0.8.2).\n- Supported declaring host network capabilities (#6705).\n- Supported specifying namespaces that operators should manage (#6641).\n- MongoDB supported connections from outside of K8s using host network (#6689).\n- Lorry supported action commands(#6474).\n- Compatible with CronJob v1beta1 to resolve backup failures when using K8s v1.20 (#6687).\n- 【External Contribution】Added version validation for Addon spec. Addon CRD supported provider and version fields, and the controller could add provider information to labels for quick querying (#6603).\n\n## Bug Fixes\n\n- Fixed the issue of exposing SVC with an empty componentName (#6712).\n- Adjusted the OceanBase switchover feature to accommodate Oracle tenant mode (#6710).\n- Resolved backup and restore failures when using the host network (#6715).\n- Fixed the issue of parameter reconfiguration failures(#6664 #6665).\n- Fixed the issue of inoperative dynamic parameters. In Patroni PostgreSQL, if dynamic parameters and static parameters were modified simultaneously, the dynamic parameters did not take effect (#6648).\n- Removed duplicate ports generated when exposing SVC (#6631).\n- Fixed bugs related to ServiceAccount. If RestoreCR specifies a ServiceAccount, the back",
    "path": "blogs/community-monthly-report-for-february-2024",
    "description": " # Community Monthly Report for February 2024  ## Overview  In February, KubeBlocks continued the development of v0.8.2 and v0.9.0 and mainly focused on sharding API, which will be released in v0.8.2.  In the past month, the community has merged 76 PRs and resolved 57 issues. A total of 21 community"
  },
  {
    "id": "blogs_en_community-monthly-report-for-january-2024",
    "title": "Community Monthly Report for January 2024",
    "content": "\n# Community Monthly Report for January 2024\n\n\n## Overview\n\n- **Release of KubeBlocks v0.8.0**\n  \n  KubeBlocks [v0.8.0](./announcing-kubeblocks-v0.8.0.md) was officially released this month, with the current latest stable version being v0.8.1.\n\n- **Highlights of KubeBlocks v0.8.0**\n  \n  KubeBlocks v0.8.0 introduced the Component API, which simplified the integration of database engines. Addons have also been separated from the KubeBlocks repository and now exist independently, making it more convenient to use.\n\n- **Community Contributions**\n  \n  In the past month, the community has merged 114 PRs and resolved 100 issues. A total of 24 community members participated in these contributions, including 5 new contributors.\n\n## Highlights\n\n- KubeBlocks has supported more addons, such as TiDB, Xinference, openGauss, InfluxDB, OceanBase (primary/standby), and Flink. Check the [official documentation](https://kubeblocks.io/docs/release-0.8/user_docs/overview/supported-addons) for the supported capabilities of each engine.\n- KubeBlocks Client SDK is now available (check the [repo](https://github.com/apecloud/kubeblocks-client)) and currently supports Java, Python, and Rust.\n- Switchover is now supported for MySQL Replication clusters, and it allows `kbcli promote` commands.\n\n## New Contributors\n\nIn January, KubeBlocks has seen 5 new contributors who merged 5 pull requests. \n\nThey are 💙 @earayu @driverby @kissycn @LiuG-lynx @lispking 💙. \n\nThank you for your contributions for helping to build KubeBlocks as the next-generation database management platform.\n\nDevelopers who want to contribute to KubeBlocks can start by tackling \"good first issues\" and leave a comment to claim them. Once our maintainers assign the issues, you can just start right away!\n\nAdditionally, since addons are now separated and the Client SDK is released, you can also take an active part in these fields. \n\nRelated links:\n- Good first issue: https://github.com/apecloud/kubeblocks/contribute\n- Contribution g",
    "path": "blogs/community-monthly-report-for-january-2024",
    "description": " # Community Monthly Report for January 2024   ## Overview  - **Release of KubeBlocks v0.8.0**      KubeBlocks [v0.8.0](./announcing-kubeblocks-v0.8.0.md) was officially released this month, with the current latest stable version being v0.8.1.  - **Highlights of KubeBlocks v0.8.0**      KubeBlocks v"
  },
  {
    "id": "blogs_en_community-monthly-report-for-march-2024",
    "title": "Community Monthly Report for March 2024",
    "content": "\n# Community Monthly Report for March 2024\n\n## Overview\n\nIn March, KubeBlocks officially released v0.8.2. This update notably included the support for Redis Cluster, camellia-redis-proxy, and compatibility with Pulsar v3.0.2. Furthermore, the development team has been actively working on the upcoming v0.9.0.\n\nOver the past month, the community has merged 91 PRs and resolved 92 issues. A total of 19 community members participated in these contributions, including 6 new contributors.\n\nSo far, a total of 1607 GitHub users have starred KubeBlocks. We do appreciate your support and look forward to more interactions🌟.\n\n## Highlights\n\n- Supported Redis Cluster.\n- Supported Sharding topology for Cluster APIs, so that users could define shard numbers and component specs with ShardingSpec.\n- Supported camellia-redis-proxy. It is currently running on K8s v1.14, and allows configuring external Redis instances, including those outside the K8s cluster.\n- Supported high-availability access from an external K8s cluster for MongoDB Replicaset addresses.\n- Supported Pulsar v3.0.2.\n- Supported exposing nodeport addresses for each Pulsar broker.\n- Supported IPv4/IPv6 dual stack for StarRocks. Both private and public addresses could use IPv4 and IPv6. However, IPv6 support in the StarRocks kernel was still required.\n- Lorry supported customizing the roleProbe command for probing. The command could be declared through YAML scripts.\n\n## Bug Fixes\n\n- Resolved the issue of SVC recovery failure (#6768).\n- Fixed the issue where updating KubeBlocks would restart PG (#6771).\n- Resolved the problem of ConfigMap not found during cluster deletion and code optimization (#6793).\n- Fixed the issue of ineffective custom endpoints when creating OSS backup repositories (#6819).\n- Resolved the issue of etcd role probe failure during the upgrade process (#6839).\n- Fixed the issue of invalid reconcile worker nodes (#6805).\n\n## New Contributors\n\n👏 Shout out to\n\n💙 @Aayush Sharma, rustover, luoyuLianga, di",
    "path": "blogs/community-monthly-report-for-march-2024",
    "description": " # Community Monthly Report for March 2024  ## Overview  In March, KubeBlocks officially released v0.8.2. This update notably included the support for Redis Cluster, camellia-redis-proxy, and compatibility with Pulsar v3.0.2. Furthermore, the development team has been actively working on the upcomin"
  },
  {
    "id": "blogs_en_community-monthly-report-for-may-2024",
    "title": "Community Monthly Report for May 2024",
    "content": "\n# Community Monthly Report for May 2024\n\n## Overview\n\nIn May, KubeBlocks released v0.8.3 and continued the development of v0.9.0. The major updates are supporting cross-cluster referencing of configuration objects and NodeCountCluster. The community also fixed related defects.\n\nOver the past month, the community merged 170 PRs, and resolved 133 issues, with contributions from 24 developers.\n\n## Highlights\n\n- [#7153](https://github.com/apecloud/kubeblocks/pull/7153) Supported to reference config across clusters.\n- [#7258](https://github.com/apecloud/kubeblocks/pull/7258) Supported NodeCountScaler.\n\n## Bug Fixes\n\n- [#7475](https://github.com/apecloud/kubeblocks/pull/7475) Fixed the crash issue after executing a switchover for MongoDB.\n- [#7447](https://github.com/apecloud/kubeblocks/pull/7447) Adapted rolecheck to accommodate cluster initialization for some database engines.\n- [#7365](https://github.com/apecloud/kubeblocks/pull/7365) Supported PVC to create idempotent.\n- [#7352](https://github.com/apecloud/kubeblocks/pull/7352) Added labels to distinguish KubeBlocks from dataprotection.\n- [#7323](https://github.com/apecloud/kubeblocks/pull/7323) Fixed the issue that the configmap 'xxx-rsm-env' was not found.\n- [#7299](https://github.com/apecloud/kubeblocks/pull/7299) Added lorry ports to the host network.\n- [#7256](https://github.com/apecloud/kubeblocks/pull/7256) Removed the validation for the cluster component spec in webhook.\n- [#7266](https://github.com/apecloud/kubeblocks/pull/7267) Fixed the issue that there were duplicate volume mounts in the config-manager container.\n\n## New Contributors\n\n👏 Let's welcome\n\n💙 @[d976045024](https://github.com/d976045024), [duiniwukenaihe](https://github.com/duiniwukenaihe), [starnop](https://github.com/starnop) 💙!\n\nNice to have you here in the KubeBlocks community. All of them merged their first PR to KubeBlocks last month. Thanks for the contributions!\n\n## Good First Issues\n\nWe call for more developers to participate in the ",
    "path": "blogs/community-monthly-report-for-may-2024",
    "description": " # Community Monthly Report for May 2024  ## Overview  In May, KubeBlocks released v0.8.3 and continued the development of v0.9.0. The major updates are supporting cross-cluster referencing of configuration objects and NodeCountCluster. The community also fixed related defects.  Over the past month,"
  },
  {
    "id": "blogs_en_deploy-harbor-on-kubeblocks",
    "title": "Deploy a High Availability Harbor Cluster on Kubernetes with KubeBlocks (Operator for PostgreSQL and Redis ) in 5 Minutes",
    "content": "\n# Deploy a High Availability Harbor Cluster on Kubernetes with KubeBlocks (Operator for PostgreSQL and Redis ) in 5 Minutes\n\nWhen it comes to building a self-hosted Docker image repository, Harbor is often a highly recommended choice. However, **Harbor does not come with a built-in HA (high availability) integration**, which makes its services relatively unreliable. **To create a HA Harbor cluster, developers typically need to set up their own HA Redis and PostgreSQL clusters**, which can be quite cumbersome.\n\n![Harbor architecture](/img/blogs/blog-harbor-1.png)\n\nFigure 1. [Architecture of Harbor](https://goharbor.io/docs/2.1.0/install-config/harbor-ha-helm/#architecture)\n\nFortunately, you can now use KubeBlocks to set up a high-availability Harbor cluster with just a few steps.\n\n## Why KubeBlocks\n\nKubeBlocks is an open-source control plane software that runs and manages databases, message queues and other data infrastructure on K8s, and it could manage various types of engines, including RDBMSs (MySQL, PostgreSQL), Caches (Redis), NoSQLs (MongoDB), MQs (Kafka, Pulsar), etc.\n\nIn this blog, we will explain how to use KubeBlocks to build a HA Harbor cluster in just 5 minutes.\n\n## Environment preparation\n\nBefore you start, make sure your environment meets the requirements of [KubeBlocks](https://kubeblocks.io/docs/preview/user_docs/installation/install-with-kbcli/install-kubeblocks-with-kbcli#environment-preparation) and [Harbor](https://goharbor.io/docs/2.11.0/install-config/installation-prereqs/).\n\n## Install kbcli and KubeBlocks\n\n1. Install kbcli.\n\n   ```bash\n   curl -fsSL https://kubeblocks.io/installer/install_cli.sh | bash\n   ```\n\n2. Install KubeBlocks.\n\n   ```bash\n   kbcli kubeblocks install\n   ```\n\n3. Check whether KubeBlocks is installed successfully.\n\n   ```bash\n   kbcli kubeblocks status\n   ```\n\n4. Enable the `postgresql` and `redis` addons. By default, the two addons are enabled. You can check the status of the addons by running the following command. If t",
    "path": "blogs/deploy-harbor-on-kubeblocks",
    "description": " # Deploy a High Availability Harbor Cluster on Kubernetes with KubeBlocks (Operator for PostgreSQL and Redis ) in 5 Minutes  When it comes to building a self-hosted Docker image repository, Harbor is often a highly recommended choice. However, **Harbor does not come with a built-in HA (high availab"
  },
  {
    "id": "blogs_en_deploy-wordpress-on-kubeblocks",
    "title": "Deploy a High-Availability WordPress Site on Kubernetes with KubeBlocks MySQL Operator",
    "content": "\n# Deploy a High-Availability WordPress Site on Kubernetes with KubeBlocks MySQL Operator\n\n## Introduction\n\n### WordPress\n\nWordPress is the world's most popular content management system (CMS). Since its release in 2003, it has become the go-to tool for building websites. Its extensive ecosystem of plugins and themes allows users to easily expand functionality and enhance the appearance of their sites. The vibrant WordPress community also provides abundant resources and support, further reducing the difficulty of development and maintenance.\n\nAs a result, WordPress has become the choice of millions of users worldwide, holding a dominant position in the field of website building.\n\n### What is KubeBlocks?\n\nKubeBlocks is an open-source Kubernetes operator that manages a variety of databases and stateful middleware. It supports over 30 database systems, including MySQL, PostgreSQL, Redis, MongoDB, Kafka, ClickHouse, and Elasticsearch. The core concept behind KubeBlocks is its use of a common set of abstract APIs (CRDs) to describe the shared attributes across these diverse database engines. This allows database vendors and developers to leverage addons to account for the differences between engines.\n\n### Why use KubeBlocks?\n\nWhen deploying WordPress using the Bitnami image, the built-in MariaDB database provides an out-of-the-box solution. However, this approach has several drawbacks:\n\n- **High availability limitations**: The MariaDB instance in the Bitnami image is typically deployed on a single node. If an issue occurs to this node, it may lead to website service interruptions. Additionally, the built-in MariaDB lacks an automatic failover mechanism.\n- **Resource competition**: Hosting both the MariaDB database and the WordPress service within the same Pod can result in resource contention, complicating resource allocation.\n- **Poor scalability**: While MariaDB supports scaling, horizontally scaling the database (improving performance and capacity by adding instances)",
    "path": "blogs/deploy-wordpress-on-kubeblocks",
    "description": " # Deploy a High-Availability WordPress Site on Kubernetes with KubeBlocks MySQL Operator  ## Introduction  ### WordPress  WordPress is the world's most popular content management system (CMS). Since its release in 2003, it has become the go-to tool for building websites. Its extensive ecosystem of "
  },
  {
    "id": "blogs_en_dify-on-kb",
    "title": "Deploy Production-Ready AIGC Applications on Kubernetes using KubeBlocks (K8s Operator for PostgreSQL, Redis and Qdrant) and Dify",
    "content": "\n# Deploy Production-Ready AIGC Applications on Kubernetes using KubeBlocks (K8s Operator for PostgreSQL, Redis and Qdrant) and Dify\n\n## Introduction\n\nAI-generated content (AIGC) technology now is transforming our world at an unprecedented speed. AIGC not only provides powerful tools for content creators but also brings unprecedented business opportunities for enterprises. Through AIGC, applications can automatically generate text, images, audio, and even video, significantly enhancing the efficiency and quality of content production. More importantly, AIGC can generate content tailored to users' specific needs in real-time, greatly improving the user experience.\n\nHowever, fully leveraging the potential of AIGC comes with a series of challenges for developers, such as high technical barriers, complex model integration, and difficult operations management. It is against this backdrop that Dify was born. Dify is an open-source large language model (LLM) application development platform that ingeniously integrates the concepts of Backend as Service and LLMOps. It aims to provide a fast track from ideas to products for developers. Dify provides a range of LLMs, intuitive prompt design tools, a powerful agent framework, and flexible process orchestration capabilities, all within a user-friendly interface and API. This greatly reduces technical barriers, enabling those without a technical background to participate in building AI applications.\n\nAlthough Dify greatly simplifies the development of AI applications, how to efficiently manage the infrastructure on which these applications reply remains a significant issue during deployment and operations. AIGC applications typically use various databases, such as relational databases like PostgreSQL for application metadata, in-memory databases like Redis for conversation history, and vector databases like Qdrant for RAG recall. Ensuring the stable operation of these critical components, data consistency and security, and meeti",
    "path": "blogs/dify-on-kb",
    "description": " # Deploy Production-Ready AIGC Applications on Kubernetes using KubeBlocks (K8s Operator for PostgreSQL, Redis and Qdrant) and Dify  ## Introduction  AI-generated content (AIGC) technology now is transforming our world at an unprecedented speed. AIGC not only provides powerful tools for content cre"
  },
  {
    "id": "blogs_en_does-containerization-affect-the-performance-of-databases",
    "title": "Does containerization affect the performance of databases?",
    "content": "\n# Does containerization affect the performance of databases?\n\nThe wave of database containerization is on the rise, as clearly shown in Fig.1. Databases and analytics are now a major part of the tech scene. Yet, a common dilemma persists: Does containerization impact database performance? If yes, what factors come into play? How can we tackle performance and stability issues brought about by containerization?\n\n[4]\"  width='80%' style=} />\n\nFig. 1. Usage of containerized workloads by category[4]\n\n## Advantages and technical principles of containerization\n\nContainerization is a smart way to bundle an app and all its necessary parts into a self-contained, portable and immutable runtime environment. Think of it as a tech wizardry that simplifies the process of packaging, deploying, and managing applications. This magic is made possible by container runtime engines such as Docker or Containerd. These engines are responsible for creating, deploying, and supervising containers.\n\nKubernetes (K8s), is a game-changer in the world of container orchestration. This open-source platform functions as a central hub for managing containers, offering a scalable infrastructure that automates a wide range of operations. As a dominant container orchestration tool, it handles everything from seamless deployment to efficient scaling, comprehensive management, and smart scheduling.\n\n### Advantages of containerization\n\n1. Flexibility and portability.\n  \n   The deployment and migration of databases are simpler and more reliable. With containerization, the runtime environment and version of databases can be claimed through declarative API in the form of IaC (Infrastructure as Code).\n\n2. Resource isolation and scalability.\n\n   By leveraging container runtime engines, containerization ensures that each database instance operates in its own isolated environment with dedicated resources. This separation minimizes interference between workloads, allowing for more efficient use of computing power ",
    "path": "blogs/does-containerization-affect-the-performance-of-databases",
    "description": " # Does containerization affect the performance of databases?  The wave of database containerization is on the rise, as clearly shown in Fig.1. Databases and analytics are now a major part of the tech scene. Yet, a common dilemma persists: Does containerization impact database performance? If yes, w"
  },
  {
    "id": "blogs_en_does-running-mysql-on-kubernetes-lead-to-significant-performance-degradation",
    "title": "Does running MySQL on Kubernetes lead to significant performance degradation?",
    "content": "\n# Does running MySQL on Kubernetes lead to significant performance degradation?\n \nDoes running MySQL on Kubernetes lead to significant performance degradation? Although concerns about this have been raised recently, few tests have been conducted, and results have rarely been shared publicly.\nTo answer this question, we used a popular benchmarking tool to evaluate MySQL throughput and latency in the following typical scenarios and tried to give a report and our insights:\n- MySQL and the benchmarking tool are deployed in the same K8s cluster to simulate the application and database running in the same K8s cluster.\n- MySQL and the benchmarking tool are deployed in two K8s clusters to simulate the application and database running in two K8s clusters.\n\nFurthermore, we obtained Amazon RDS MySQL performance data using the same testing method. By comparing the performance data of Amazon RDS MySQL, users can gain a more comprehensive understanding of whether MySQL's performance on K8s can meet their production requirements.\n\n\n## Methodology\n\n### The Application Workloads\nAs part of the LAMP stack, MySQL is often used to build websites and web applications. Users generally do not run long transactions or complex queries in MySQL, so we used OLTP workloads that contain relatively simple transactions and queries to test performance. Here are several representative workloads:\n- Read-intensive workload: 80% of operations are reads, and 20% are writes\n- Read-write balanced workload: 50% of operations are reads, and 50% are writes\n- Write-intensive workload: 20% of operations are reads, and 80% are writes\n\nWe have chosen [sysbench](https://github.com/akopytov/sysbench) as our benchmarking tool. It is a widely used tool that can be scripted and run on multiple threads. Sysbench can simulate the application workloads mentioned above and output throughput in terms of queries per second (QPS) and latency in terms of the 99th percentile (ms).\n\n### The Open Source MySQL Operators\n\nFor K",
    "path": "blogs/does-running-mysql-on-kubernetes-lead-to-significant-performance-degradation",
    "description": " # Does running MySQL on Kubernetes lead to significant performance degradation?   Does running MySQL on Kubernetes lead to significant performance degradation? Although concerns about this have been raised recently, few tests have been conducted, and results have rarely been shared publicly. To ans"
  },
  {
    "id": "blogs_en_how-to-fix-pod-stuck-in-terminating-status",
    "title": "How to Fix Pods Stuck in Terminating Status in Kubernetes?",
    "content": "\n# How to Fix Pods Stuck in Terminating Status in Kubernetes?\n\nRunning into a Pod stuck in \"Terminating\" is one of those occasional, but nearly unavoidable, issues that every Kubernetes user experiences.\n\nRecently, while assisting a community user test the upgrade from KubeBlocks v0.8 to v0.9, we hit a situation where a Cluster just wouldn’t delete. After a day of back-and-forth troubleshooting, we figured out the causes of a few strange problems, but one issue persisted — a Pod stuck in Terminating. Coincidentally, I was also discussing Pod lifecycle management with the KubeBlocks SIG members, so I decided it was a good chance to delve deeper and figure out what was really going on.\n\n## Problem Reproduction\n\nAfter some recollection and attempts, the steps to reproduce the issue are as follows:\n\n1. Run `kbcli playground init` in KubeBlocks v0.8.\n2. Run `kbcli cluster create xxxx`.\n3. Upgrade kbcli to v0.9.\n4. Run `kbcli kubeblocks upgrade` in KubeBlocks v0.9 and the `helm upgrade job` times out and fails.\n5. Run `helm uninstall kubeblocks`.\n6. Run `Delete cluster xxxx`.\n7. Run `helm install kubeblocks`.\n8. The `Cluster xxxx` remained in the Deleting status.\n\n## Troubleshooting\n\n### Pinpoint the Issue\n\nBased on past experience, when a Cluster fails to delete, it’s usually because some subordinate resources can’t be removed. Our community user also observed that both the Pod and the PVC were stuck in the Terminating status. Since the PVC cannot be deleted until the Pod it’s attached to is gone (due to the finalizer protection), the focus naturally shifted to why the Pod couldn't be deleted.\n\nFirst, let’s examine the YAML file of the Pod object. According to the official K8s documentation on the Pod termination process, we filtered out the fields related to Pod termination:\nThe relevant fields are still quite a bit to sift through, but we need to analyze each field systematically. Fortunately, this analysis goes quickly:\n\n1. `deletionTimestamp` is set.\n2. `finalizer` i",
    "path": "blogs/how-to-fix-pod-stuck-in-terminating-status",
    "description": " # How to Fix Pods Stuck in Terminating Status in Kubernetes?  Running into a Pod stuck in \"Terminating\" is one of those occasional, but nearly unavoidable, issues that every Kubernetes user experiences.  Recently, while assisting a community user test the upgrade from KubeBlocks v0.8 to v0.9, we hi"
  },
  {
    "id": "blogs_en_how-to-manage-database-clusters-without-a-dedicated-operator",
    "title": "How to Manage Database Clusters Without a Dedicated Operator?",
    "content": "\n# How to Manage Database Clusters Without a Dedicated Operator?\n\nAs cloud computing and database technologies evolve, finding an efficient and cost-effective way to manage database clusters has become crucial. I hope this presentation provides you with valuable insights and practical solutions.\n\nBefore we dive into the details, let me introduce myself and my co-speaker. I am Shanshan from KubeBlocks, and ApeCloud is the startup company behind KubeBlocks. Before joining ApeCloud, I worked at Alibaba Cloud Database Group for years as a database developer for SQL optimization and SQL execution, focusing more on databases rather than cloud-native tech. My co-speaker Shun Ding is a system architect from China Mobile Cloud, and he is also a KubeBlocks contributor. He made this first commit earlier this year, and months later he integrated their in-house database to KubeBlocks as an addon.\n\nToday, I will introduce KubeBlocks and the design concept of it, and Shun will share why they chose KubeBlocks, how KubeBlocks has helped reduce their work and what their next step with KubeBlocks are. We won't dive into detailed APIs or specific database technology in this talk.\n\n## Why and how KubeBlocks is designed\n\n### How to manage various databases on K8s?\n\nThe motivation we started the KubeBlocks project was quite simple: how to manage databases more effectively.\nOur team is a group of people with strong database background and K8s background, including database developers, database administors, and SREs. We deal with hundreds of cloud database issues daily, from database crashes, slow queries, high availability to data migration, resource scheduling, etc.\n\nThe interesting part about our team is that we work on different database types, for instance, MySQL, PostgreSQL, Redis, MongoDB, which are widely used open source databases. When we exchange our ideas on how to manage a specific database, we find these databases actually share many things in common. If we can improve the man",
    "path": "blogs/how-to-manage-database-clusters-without-a-dedicated-operator",
    "description": " # How to Manage Database Clusters Without a Dedicated Operator?  As cloud computing and database technologies evolve, finding an efficient and cost-effective way to manage database clusters has become crucial. I hope this presentation provides you with valuable insights and practical solutions.  Be"
  },
  {
    "id": "blogs_en_in-place-updates",
    "title": "How to Realize Pod In-Place Update in K8s?",
    "content": "\n# How to Realize Pod In-Place Update in K8s?\n\n## Why is in-place update needed?\n\nIn earlier versions, as described in \"[Taking Specified Instance Offline](https://kubeblocks.io/blog/take-specified-instances-offline),\" KubeBlocks used StatefulSet as the final workload type, inheriting its limitations.\n\nOne key drawback of StatefulSet is its update mechanism: changes to the PodTemplate trigger updates to all Pods, using the Recreate strategy. This process deletes and recreates Pods, which is far from ideal for systems like databases that require high availability.\n\nTo address this issue, starting from v0.9, KubeBlocks replaced StatefulSet with InstanceSet and introduced the in-place update feature. This feature allows InstanceSet to update instances by in-place Pod update or expand PVC volumes when certain fields in instance templates updates, significantly reducing the impact on system availability.\n\n## Which fields support in-place updates?\n\nKubeBlocks leverages Kubernetes' native Pod API for in-place updates. The supported fields include:\n\n- `annotations`\n- `labels`\n- `spec.activeDeadlineSeconds`\n- `spec.initContainers[*].image`\n- `spec.containers[*].image`\n- `spec.tolerations` (only supports adding new Toleration)\n\nStarting from Kubernetes v1.27, the InPlacePodVerticalScaling feature enables in-place updates for CPU and memory resources. KubeBlocks also integrates this feature to further support the following capabilities:\n\nFor Kubernetes v1.27 or later, with InPlacePodVerticalScaling enabled, supported fields are as follow:\n\n- `spec.containers[*].resources.requests[\"cpu\"]`\n- `spec.containers[*].resources.requests[\"memory\"]`\n- `spec.containers[*].resources.limits[\"cpu\"]`\n- `spec.containers[*].resources.limits[\"memory\"]`\n  \nNote that after resizing resources, some applications may require a restart to detect the updated resource configurations. You can manage this behavior using the container  `restartPolicy` in ClusterDefinition or ComponentDefinition.\n\nFor PVCs,",
    "path": "blogs/in-place-updates",
    "description": " # How to Realize Pod In-Place Update in K8s?  ## Why is in-place update needed?  In earlier versions, as described in \"[Taking Specified Instance Offline](https://kubeblocks.io/blog/take-specified-instances-offline),\" KubeBlocks used StatefulSet as the final workload type, inheriting its limitation"
  },
  {
    "id": "blogs_en_instance-template",
    "title": "How to Configure Instance Templates in KubeBlocks",
    "content": "\n# How to Configure Instance Templates in KubeBlocks\n\n## Why introduce the instance template?\n\nIn KubeBlocks, a **Cluster** is composed of several **Components**, each of which manages multiple Pods and auxiliary objects.\n\nBefore v0.9, these Pods were rendered from a shared **PodTemplate**, as defined in either **ClusterDefinition** or **ComponentDefinition**. However, this design can’t fully address several key use cases:\n\n1. **Clusters** rendered from the same **Addon** needed the ability to configure separate scheduling settings, such as **NodeName**, **NodeSelector**, or **Tolerations**.\n2. **Components** rendered from the same **Addon** required the flexibility to apply custom **Annotations**, **Labels**, or **Environment Variables** (Env) to the Pods they managed.\n3. **Pods** managed by the same **Component** required the ability to define different **CPU**, **Memory**, and other **Resource Request**s and **Limits**.\n\nAs these requirements became more common, the **Instance Template** feature was introduced into the Cluster API in KubeBlocks v0.9. This feature provides finer control over Pod configuration, helping address the above use cases.\n\n## What is an instance template?\n\nAn **Instance** serves as the fundamental unit in KubeBlocks, comprising a Pod along with several auxiliary objects. For simplicity, you can think of an **Instance** as a **Pod**, and and for the sake of consistency, we'll continue referring to it as an \"**Instance**\" from here on.\n\nStarting from v0.9, KubeBlocks introduces the ability to define multiple **instance templates** for a given **Componen**t within a **Cluster**. These instance templates include several fields, such as **Name**, **Replicas**, **Annotations**, **Labels**, **Env**, **Tolerations**, **NodeSelector**. The values defined in these templates will override the corresponding values in the default template (i.e., **PodTemplate** defined in **ClusterDefinition** and **ComponentDefinition**) to generate the final configur",
    "path": "blogs/instance-template",
    "description": " # How to Configure Instance Templates in KubeBlocks  ## Why introduce the instance template?  In KubeBlocks, a **Cluster** is composed of several **Components**, each of which manages multiple Pods and auxiliary objects.  Before v0.9, these Pods were rendered from a shared **PodTemplate**, as defin"
  },
  {
    "id": "blogs_en_instanceset-introduction",
    "title": "What is InstanceSet?",
    "content": "\n# InstanceSet\n\nIn KubeBlocks, an instance is the fundamental unit, composed of a Pod and other auxiliary objects. To make it simple, you can think of it as just a Pod, but we'll refer to it as an \"instance\" throughout the text.\n\n**InstanceSet is a general workload API responsible for managing a group of instances. All workloads in KubeBlocks are ultimately managed through InstanceSet**.\n\nCompared to K8s native workload APIs like StatefulSet and Deployment, **InstanceSet incorporates additional features and design considerations specifically related to databases, such as roles and high availability**. **This makes it much more capable of supporting complex stateful workloads like databases**.\n\n## How to use InstanceSet?\n\nInstanceSet generates a fixed name for each instance it manages and creates a Headless Service, giving each instance a stable network identifier. With this identifier, instances within the same InstanceSet can find each other, and other systems within the same Kubernetes cluster can also find each instance under this InstanceSet.\n\nInstanceSet uses VolumeClaimTemplates to generate storage volumes with fixed identifiers for each instance. Other instances or systems can locate an instance through its fixed identifier and access the data stored in its volume.\n\nWhen it comes to updates, **InstanceSet supports RollingUpdate for all instances in a deterministic order**, and you can configure various behaviors for the rolling updates. Similarly, during horizontal scaling, InstanceSet adds or removes instances in a deterministic order.\n\nBuilding on these basic features, to meet the demands of supporting high availability in databases, InstanceSet further supports more features like **in-place update**, **instance template**, **taking specified instance offline**, **role-based services**, and **role-based update strategies**.\n\nLet's delve into these features in more detail below.\n\n## How to generate an instance name?\n\nInstanceSet renders instance objects by u",
    "path": "blogs/instanceset-introduction",
    "description": " # InstanceSet  In KubeBlocks, an instance is the fundamental unit, composed of a Pod and other auxiliary objects. To make it simple, you can think of it as just a Pod, but we'll refer to it as an \"instance\" throughout the text.  **InstanceSet is a general workload API responsible for managing a gro"
  },
  {
    "id": "blogs_en_is-k8s-a-database",
    "title": "Is Kubernetes a Database?",
    "content": "\n# Is Kubernetes a Database?\n\n> This blog was originally published by Lei Zhang in 2020, but even four years later, this blog is still inspiring. The question, \"Is Kubernetes a database?\", is worth pondering. Using this question as a starting point, Lei's blog unveils the core concepts behind K8s, such as declarative application management and Infrastructure as Data (IaD). By drawing an analogy between K8s and databases, this blog guides us in reinterpreting K8s from a database perspective. Through this blog, you'll gain a fresh understanding of both K8s and the question, \"Is Kubernetes a database?\"\n>\n> Hope you enjoy reading!\n\n*By Lei Zhang, CNCF TOC Member (2021-2023), Kubernetes maintainer*\n\nRecently, a statement about \"Kubernetes is the new database\" has attracted a lot of attention in the Kubernetes community. To be more accurate, this idea means Kubernetes operates similarly to a database, rather than suggesting you should use Kubernetes as a database.\n\n![](/img/blogs/k8s-as-database-twitter.png)\n\nAt first glance, comparing Kubernetes with a database is far-fetched. After all, the way Kubernetes typically works, like the controller pattern, declarative APIs, and so on, seems not to have direct connection with a database. However, behind this statement, there is a fundamental concept which can be traced back to one of the core theories adopted by Kubernetes.\n\n## Fundamentals of declarative application management in Kubernetes\n\nWhen talking about Kubernetes, the concept of declarative application management often comes up. In fact, this design is what sets Kubernetes apart from all other infrastructure projects since it's a unique capability of Kubernetes. But have you ever thought about what declarative application management is in Kubernetes?\n\n### 1. Declarative application management is more than declarative APIs\n\nIf we examine the core principle of Kubernetes, it's clear that most features in Kubernetes – whether it's kubelet running containers, kube-proxy a",
    "path": "blogs/is-k8s-a-database",
    "description": " # Is Kubernetes a Database?  > This blog was originally published by Lei Zhang in 2020, but even four years later, this blog is still inspiring. The question, \"Is Kubernetes a database?\", is worth pondering. Using this question as a starting point, Lei's blog unveils the core concepts behind K8s, s"
  },
  {
    "id": "blogs_en_kubeblocks-on-kata",
    "title": "Securing Your Workloads with Kata Containers - Running KubeBlocks on Kata",
    "content": "\n# Securing Your Workloads with Kata Containers: Running KubeBlocks on Kata\n\nTraditional containers run on the same operating system kernel, which may have some security vulnerabilities, such as privilege escalation and kernel vulnerabilities. If you are concerned about container security, run KubeBlocks on Kata Containers might be a solution. We have built a Kata Containers environment and performed some basic functional verification.\nKata Containers (Kata for short) is an open-source project that provides a secure and high-performance container runtime environment. The goal of Kata Containers is to combine virtualization technology with container technology, providing a user experience similar to lightweight containers while offering higher isolation and security.\n\n## The key features of Kata Containers\n\n1. Secure Isolation: Each container runs in its own virtual machine, providing hardware-level isolation to deliver higher security and isolation. This makes Kata Containers more suitable for multi-tenant environments and security-sensitive workloads.\n2. Performance and Resource Efficiency: Despite running in virtual machines, Kata Containers can still provide performance and resource efficiency close to that of lightweight containers. Kata Containers leverage the benefits of hardware virtualization and utilize technologies like hardware acceleration to achieve fast startup and high performance.\n3. Ecosystem Compatibility: Kata Containers are compatible with the container ecosystem, supporting Docker and Kubernetes, and can seamlessly integrate with existing container tools and platforms.\n4. Flexibility and Scalability: Kata Containers can run on various virtualization platforms, including those based on KVM, Firecracker, and others. This allows users to choose the appropriate virtualization solution based on their specific requirements.\n\n## The preparation for the virtualization environment\n1. Virtualization Requirements\nBefore installing Kata Containers, you need",
    "path": "blogs/kubeblocks-on-kata",
    "description": " # Securing Your Workloads with Kata Containers: Running KubeBlocks on Kata  Traditional containers run on the same operating system kernel, which may have some security vulnerabilities, such as privilege escalation and kernel vulnerabilities. If you are concerned about container security, run KubeB"
  },
  {
    "id": "blogs_en_mangage-6k-db-instance-with-kubeblocks",
    "title": "Managing Over 6,000 Self-Hosted Databases Without a DBA - How a Single Engineer Leveraged KubeBlocks to Make It Possible",
    "content": "\n# Managing Over 6,000 Self-Hosted Databases Without a DBA: How a Single Engineer Leveraged KubeBlocks to Make It Possible\n\n> About Sealos\n>\n> [Sealos](https://sealos.io/) is a startup offering Kubernetes-based PaaS (Platform-as-a-Service) solutions tailored for Chinese application developers. Sealos empowers developers to rapidly build and deploy applications by providing ready-to-use, pay-as-you-go Kubernetes services. In addition, Sealos offers a wide range of services, including function computing, gateways, DBaaS(Database-as-a-Service), MinIO storage, and an App store. \n\n> Author: Jinhu Xie, Sealos Engineer\n\nLet’s start with a question: Can a single engineer, without being a professional DBA, manage over 6,000 self-hosted database clusters?\n\nIn today’s era of cloud computing and large-scale distributed systems, databases remain the cornerstone of many applications. As their scale grows, so does the complexity of managing databases. Therefore, when faced with managing thousands of database instances alone, most would say, \"That's impossible!\"\n\nHowever, what seems impossible has already been made a reality inside Sealos. Sealos provides its developer users with Kubernetes-based PaaS services, along with essential DBaaS offerings, including **MySQL**, **PostgreSQL**, **Redis**, **MongoDB**, **Kafka**, and **Milvus**, managing **over 6,000 database instances across four regions**.\n\nIn this blog, I'll share how I, a K8s engineer without DBA background, successfully manage such a massive number of databases using KubeBlocks in Sealos, transforming what seemed impossible into an everyday operation.\n\n## Challenges of managing databases at large scale\n\nManaging such an extensive database cluster traditionally requires a large operations team and a wide array of  sophisticated tools. Even the smallest mistake can result in  service disruptions, performance degradation,  or, in the worst-case scenario, data loss. Tasks such as resource management, scaling, backup, monitor",
    "path": "blogs/mangage-6k-db-instance-with-kubeblocks",
    "description": " # Managing Over 6,000 Self-Hosted Databases Without a DBA: How a Single Engineer Leveraged KubeBlocks to Make It Possible  > About Sealos > > [Sealos](https://sealos.io/) is a startup offering Kubernetes-based PaaS (Platform-as-a-Service) solutions tailored for Chinese application developers. Sealo"
  },
  {
    "id": "blogs_en_migrate-redis-in-kuaishou-from-bare-metal-to-k8s",
    "title": "Resource Utilization Boost - Large-Scale Redis Migration from Bare Metal to Kubernetes",
    "content": "\n# Resource Utilization Boost: Large-Scale Redis Migration from Bare Metal to Kubernetes\n\nI'm Xueqiang Wu, and I'm thrilled to be here at KubeCon to share some insights with you. Today, I’ll be co-presenting with Yuxing Liu from Kuaishou, and we'll discuss the migration of large-scale Redis instances from bare metal to Kubernetes, focusing on how this move can significantly improve resource utilization.\n\nLet’s start with today’s agenda. First, I'll give you a quick introduction to the KubeBlocks project. Then, from the perspective of a single Redis cluster, I’ll highlight three key challenges KubeBlocks addresses to better run databases like Redis on Kubernetes. After that, Yuxing will share his experiences of managing multiple Redis clusters across multiple Kubernetes clusters, from a large-scale deployment perspective.\n\n## What is KubeBlocks?\n\nTo begin, let me give you an overview of KubeBlocks. KubeBlocks was created by ApeCloud, a startup founded by a team with extensive experience in database development and operations, particularly from our time at Alibaba Cloud. About two years ago, we decided to start our own company, focusing on building products that simplify running databases like Redis on Kubernetes. Last year, we open-sourced the KubeBlocks project.\n\nIn simple terms, KubeBlocks is a Kubernetes Operator, but it stands out from other database operators because it’s designed to support a wide variety of databases. In fact, KubeBlocks already supports over 35 different databases.\n\nTo achieve this, we structured the KubeBlocks API, or Custom Resource Definitions (CRDs), into two main categories. The first category is the **\"Add-on Provider API\"**. These APIs allow database providers to easily integrate their databases into the KubeBlocks ecosystem. The second category is the **\"Developer & DevOps API\"**, which gives users and operators the ability to manage databases — whether it’s creating a new cluster, scaling, performing backups and restores, and more. A",
    "path": "blogs/migrate-redis-in-kuaishou-from-bare-metal-to-k8s",
    "description": " # Resource Utilization Boost: Large-Scale Redis Migration from Bare Metal to Kubernetes  I'm Xueqiang Wu, and I'm thrilled to be here at KubeCon to share some insights with you. Today, I’ll be co-presenting with Yuxing Liu from Kuaishou, and we'll discuss the migration of large-scale Redis instance"
  },
  {
    "id": "blogs_en_moodle-in-kubeblocks-windows",
    "title": "Deploy Moddle on K8s with KubeBlocks MySQL Operator",
    "content": "\n# Deploy Moddle on K8s with KubeBlocks MySQL Operator (Windows)\n\nMoodle is a free online learning management system that allows educators to create their private websites, populate them with dynamic course content, and thereby enable people to learn anytime, anywhere.\n\n## Introduction\n\nThis document will guide you to quickly get started with installing and using Moodle on Windows, leveraging the MySQL database provided by KubeBlocks.\n\n## Necessary Preparations\n\n- [Docker](https://docs.docker.com/get-docker/)：v20.10.5 (runC ≥ v1.0.0-rc93) or higher version;\n- [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl): For interacting with the Kubernetes cluster;\n- [kbcli](https://cn.kubeblocks.io/docs/preview/user-docs/installation/install-with-kbcli/install-kbcli): For interaction between Playground and KubeBlocks.\n\n## Prepare a Database\n\n### Steps\n\n### 1. Ensure the ApeCloud MySQL addon is enabled\n\n```shell\nkbcli addon list\n>\nNAME                           TYPE   STATUS     EXTRAS         AUTO-INSTALL   INSTALLABLE-SELECTOR\n...\napecloud-mysql                 Helm   Enabled                   true\n...\n```\n\n### 2. Create a MySQL cluster\n\nThis is a Standalone demonstration. For a RaftGroup Cluster deployment, please refer to: [Create and connect to a MySQL Cluster | KubeBlocks](https://kubeblocks.io/docs/release-0.8/user_docs/kubeblocks-for-mysql/cluster-management/create-and-connect-a-mysql-cluster).\n\n```shell\nkbcli cluster create mysql \n```\n\n### 3. Obtain cluster basic information\n\nExecute the following commands to retrieve the network information of the target database, particularly note the password that will be needed later.\n\n```shell\nkbcli cluster connect --show-example --show-password $\n```\n\n![Figure 1](/img/blogs/obtain-cluster-basic-information.png)\n\n### 4. Service forwarding\n\nPerform a `port-forward` to provide the MySQL service to the host.\n\n:::note\n\nNote that since you will need to start the MySQL service on the local machine later, you need to modify the ",
    "path": "blogs/moodle-in-kubeblocks-windows",
    "description": " # Deploy Moddle on K8s with KubeBlocks MySQL Operator (Windows)  Moodle is a free online learning management system that allows educators to create their private websites, populate them with dynamic course content, and thereby enable people to learn anytime, anywhere.  ## Introduction  This documen"
  },
  {
    "id": "blogs_en_redis-containerization",
    "title": "Redis Containerization - Ready Yet?",
    "content": "\n# Redis Containerization - Ready Yet?\n\nIn the era of Kubernetes dominance, database containerization presents an extremely attractive yet often daunting challenge for cloud-native teams.\n\nOpen-source databases like MySQL and PostgreSQL were born in the PC server era and are often used to store critical business data. Migrating them to Kubernetes might require more effort and courage. However, when it comes to Redis, a database born in the same era as container technology and mostly used as a cache, does containerizing it become easier? **Many cloud-native teams indeed think so, but practice reveals that Redis isn’t as easy to handle as it seems.**\n\n## Too easy, right? Well...\n\nGetting started with Redis containerization is indeed a breeze. You can pull up a Redis service in just a few seconds using the official Redis Docker image. Deploying applications and Redis in the same Kubernetes cluster significantly simplifies the onboarding process, but there are two \"minor\" problems:\n\n- Redis service is not highly available\n  As soon as the Redis container is rescheduled, the IP of the Redis container changes, causing the connection pool configuration on the application side to fail. To adapt to the volatility of the container environment, the IP of the Redis container cannot be directly exposed to the application. Instead, a VIP (Virtual IP) or DNS domain name needs to be added to provide a fixed connection address.\n\n- Redis service is not highly reliable\n  If the host machine running the Redis container crashes, the persistent volume of the Redis container may be damaged, leading to data loss for the application. Although many developers use Redis as a volatile in-memory database, there are also quite a few who use it for persistent key-value storage. Therefore, Redis containerization inevitably involves solutions for distributed block storage or local disk synchronization to ensure data persistence.\n\n## If nothing goes wrong, something will\n\nAmbitious cloud-native team",
    "path": "blogs/redis-containerization",
    "description": " # Redis Containerization - Ready Yet?  In the era of Kubernetes dominance, database containerization presents an extremely attractive yet often daunting challenge for cloud-native teams.  Open-source databases like MySQL and PostgreSQL were born in the PC server era and are often used to store crit"
  },
  {
    "id": "blogs_en_redis-on-kb",
    "title": "Streamlining Redis Cluster for Kubernetes with KubeBlocks and Solving Network Compatibility Problems",
    "content": "\n# Streamlining Redis Cluster for Kubernetes with KubeBlocks and Solving Network Compatibility Problems\n\nRedis Cluster is the distributed solution for the Redis database, used to distribute data across multiple nodes to provide high availability and scalability. It allows storing a large amount of data sharded across multiple nodes and automatically handles data sharding and migration.\nRedis Cluster uses the hash slots to manage data distribution. Data is divided into a fixed number of hash slots, and each slot can be assigned to a different node. Each node is responsible for handling a portion of the data within the assigned hash slots. Clients can directly connect to any node without the need for an intermediate proxy.\nIn application deployment, the overall architecture is generally composed of the backend Redis Cluster and the application-side smart client.\nRedis Cluster provides the following features:\n\n1. Automatic sharding and data migration: When nodes join or leave the cluster, Redis Cluster automatically migrates data to the correct nodes to maintain a balanced data distribution.\n2. High availability: Redis Cluster uses a master-slave replication mechanism, where each master node has multiple slave nodes. When a master node fails, a slave node can automatically take over, providing high availability.\n3. Load balancing: Redis Cluster implements automatic load balancing between clients and nodes. Clients can directly connect to any node, and the nodes will automatically forward requests, achieving load balancing.\nBy distributing data across multiple nodes and providing automatic failover and load balancing mechanisms, Redis Cluster enables applications to handle large-scale data sets and high-concurrency access requirements. It is a powerful distributed solution commonly used in scenarios that require high performance and scalability, such as caching, session storage, and real-time counting.\nMany of Kubeblocks' clients have a strong demand for Redis Cluster, ",
    "path": "blogs/redis-on-kb",
    "description": " # Streamlining Redis Cluster for Kubernetes with KubeBlocks and Solving Network Compatibility Problems  Redis Cluster is the distributed solution for the Redis database, used to distribute data across multiple nodes to provide high availability and scalability. It allows storing a large amount of d"
  },
  {
    "id": "blogs_en_run-redis-on-k8s-kuaishou-solution-with-kubeblocks",
    "title": "Managing Large-Scale Redis Clusters on Kubernetes with an Operator - Kuaishou's Approach",
    "content": "\n# Managing Large-Scale Redis Clusters on Kubernetes with an Operator: Kuaishou's Approach\n\n> **About Kuaishou**\n>\n> Kuaishou is a leading content community and social platform in China and globally, committed to becoming the most customer-obsessed company in the world. Kuaishou uses its technological backbone, powered by cutting-edge AI technology, to continuously drive innovation and product enhancements that enrich its service offerings and application scenarios, creating exceptional customer value. Through short videos and live streams on Kuaishou's platform, users can share their lives, discover goods and services they need and showcase their talent. By partnering closely with content creators and businesses, Kuaishou provides technologies, products, and services that cater to diverse user needs across a broad spectrum of entertainment, online marketing services, e-commerce, local services, gaming, and much more.\n\n> **About Author**\n>\n> Yuxing Liu is the senior software engineer from Kuaishou. Yuxing has worked in the cloud-native teams of Alibaba Cloud and Kuaishou, focusing on the cloud-native field and gaining experience in open source, commercialization, and scaling of cloud-native technologies. Yuxing is also one of the maintainers of the CNCF/Dragonfly project and also one of the maintainers of the CNCF/Sealer project. Currently, he focuses on driving the cloud-native transformation of stateful business in Kuaishou.\n\nAs a popular short-form video application, Kuaishou relies heavily on Redis to deliver low-latency responses to its users. Operating on private cloud infrastructure, automating the management of large-scale Redis clusters with minimal human intervention presents a significant challenge. A promising solution emerged: running Redis on Kubernetes using an Operator.\n\nWhile containerizing stateless services like applications and Nginx is now standard, running stateful services like databases and Redis on Kubernetes remains debated. Based on Kuaish",
    "path": "blogs/run-redis-on-k8s-kuaishou-solution-with-kubeblocks",
    "description": " # Managing Large-Scale Redis Clusters on Kubernetes with an Operator: Kuaishou's Approach  > **About Kuaishou** > > Kuaishou is a leading content community and social platform in China and globally, committed to becoming the most customer-obsessed company in the world. Kuaishou uses its technologic"
  },
  {
    "id": "blogs_en_take-specified-instances-offline",
    "title": "How to Take Specified Instances Offline?",
    "content": "\n# How to Take Specified Instances Offline?\n\nIn [the previous blog](https://kubeblocks.io/blog/instanceset-introduction), we introduced InstanceSet, along with a series of features derived from it to address high availability and other database needs. In this blog, we will introduce one of these features -- taking specified instances offline.\n\n## Why is this feature needed?\n\nBefore v0.9.0, KubeBlocks generated workloads as ***StatefulSets***, which was a double-edged sword. While KubeBlocks could leverage the advantages of ***StatefulSets*** to manage stateful applications like databases, it inherited its limitations.\n\nOne of these limitations is evident in horizontal scaling scenarios, where ***StatefulSets*** offload Pods sequentially based on the *Ordinal* order, potentially impacting the availability of databases running within.\n\nFor example, managing a PostgreSQL database with one primary and two secondary replicas using a ***StatefulSet*** named `foo-bar`. And Pod `foo-bar-2` is selected as the primary node. Now, if we decide to scale in this database cluster due to low read load, according to ***StatefulSet*** rules, we can only offload Pod `foo-bar-2`, which is currently the primary node. In this way, we can either directly offload `foo-bar-2`, triggering a failover mechanism to elect a new primary pod from `foo-bar-0` and `foo-bar-1`, or use a switchover mechanism to convert `foo-bar-2` into a secondary pod before offloading it. Either way, there will be a period where write is not applicable.\n\nAnother issue arises in the same scenario: if the node hosting `foo-bar-1` experiences a hardware failure, causing disk damage and rendering data read-write inaccessible, according to best operation practices, we need to offload `foo-bar-1` and rebuild replicas on healthy nodes. However, performing such operation tasks based on ***StatefulSets*** isn't easy.\n\nTo solve the limitations mentioned above, starting from v0.9, KubeBlocks replaces ***StatefulSets*** with ***",
    "path": "blogs/take-specified-instances-offline",
    "description": " # How to Take Specified Instances Offline?  In [the previous blog](https://kubeblocks.io/blog/instanceset-introduction), we introduced InstanceSet, along with a series of features derived from it to address high availability and other database needs. In this blog, we will introduce one of these fea"
  },
  {
    "id": "blogs_en_use-kubeblocks-to-build-your-aigc-infra-on-amazon-eks",
    "title": "Use KubeBlocks to build your AIGC infrastructure on Amazon EKS",
    "content": "\n# Use KubeBlocks to build your AIGC infrastructure on Amazon EKS\n## Foreword\nGenerative AI has sparked a widespread interest and catapulted the vector database market into the spotlight. Numerous vector databases have begun to emerge and catch the public's attention.\n\nAccording to IDC's prediction, by 2025, more than 80% of business data will be unstructured, stored in text, images, audio, video or other formats. However, handling large-scale unstructured data storage and querying still faces great challenges.\n\nIn Generative AI and Deep Learning, the common practice is to convert unstructured data into vectors for storage, and use vector similarity search technology to perform semantic correlation retrieval. The fast storage, indexing and search of embedding are the core functions of vector databases.\n\nSo, what is embedding? Put simply, embedding is a vector representation composed of floating point numbers. The distance between two vectors represents their correlation. The closer the distance, the higher the correlation; the farther the distance, the lower the correlation. If two embedding vectors are similar, it means that the original data they represent is also similar, which is different from traditional keyword searches.\n\nHowever, a vector database is complex to manage because in nature it is a stateful workload. When used in the production environment, it faces the same problems as those of traditional OLTP and OLAP databases, such as data security, high availability, vertical/horizontal scalability, monitoring and alerts, backup and restore, etc. As vector databases are relatively new, most users lack necessary knowledge, which brings great challenges to implementing LLMs + vector databases stack. **Users pay more attention to the value that LLMs and vector databases bring to the business, rather than investing excessive effort into their management.**\n\nTo address these problems, KubeBlocks, leveraging the declarative API of K8s, abstracts various databases",
    "path": "blogs/use-kubeblocks-to-build-your-aigc-infra-on-amazon-eks",
    "description": " # Use KubeBlocks to build your AIGC infrastructure on Amazon EKS ## Foreword Generative AI has sparked a widespread interest and catapulted the vector database market into the spotlight. Numerous vector databases have begun to emerge and catch the public's attention.  According to IDC's prediction,"
  }
]