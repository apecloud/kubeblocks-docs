---
title: FAQs
description: FAQs of Redis
keywords: [KubeBlocks, Redis, Kubernetes Operator]
sidebar_position: 9
sidebar_label: FAQs
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Redis FAQs

## 1. How do I configure scheduling policies for a Redis Cluster?

You can use the `schedulingPolicy` field to control how Redis Pods are distributed across your Kubernetes nodes. This is useful for improving high availability, optimizing resource usage, or meeting specific placement requirements.

Below are common scheduling scenarios with example configurations.

### Distribute Pods within the same shard across different nodes

To ensure high availability within a shard, use `podAntiAffinity` to prevent Pods of the same component/shard from running on the same node.
Since each shard name are generated by KubeBlocks, there is no way set the value beforehand. You can use the placeholder `""` and KubeBlocks will automatically replace it with the actual shard name.

```yaml
schedulingPolicy:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              apps.kubeblocks.io/component-name: ""  # Leave it blank, automatically replaced by KubeBlocks
          topologyKey: kubernetes.io/hostname
```

### Distribute shards across different availability zones

To improve fault tolerance at the zone level, use `podAntiAffinity` to spread all shards within a sharding group across different availability zones.

```yaml
schedulingPolicy:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                apps.kubeblocks.io/sharding-name: my-sharding  # Replace with your sharding name
            topologyKey: topology.kubernetes.io/zone
```

### Co-locate Pods with a specific cluster

To reduce network latency or optimize data locality, use `podAffinity` to schedule Pods on the same nodes as an existing cluster.

```yaml
schedulingPolicy:
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 50
          podAffinityTerm:
            labelSelector:
              matchLabels:
                app.kubernetes.io/instance: my-cluster  # Replace with the target cluster name
            topologyKey: kubernetes.io/hostname
```

### Label selector reference

Use the following labels to target different scopes in your scheduling policies:

| Scope | Label Key | Description |
|-------|-----------|-------------|
| Component/Shard | `apps.kubeblocks.io/component-name` | Targets Pods within a specific shard or component |
| Sharding group | `apps.kubeblocks.io/sharding-name` | Targets all shards within a sharding group |
| Cluster | `app.kubernetes.io/instance` | Targets all Pods within an entire cluster |